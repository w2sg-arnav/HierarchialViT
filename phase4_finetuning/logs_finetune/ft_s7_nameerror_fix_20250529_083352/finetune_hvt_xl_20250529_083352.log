2025-05-29 08:33:52 - root - INFO - [setup_logging:75] - Logging configured. Log file: /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_s7_nameerror_fix_20250529_083352/finetune_hvt_xl_20250529_083352.log. Logger 'root' Effective Level: DEBUG
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:186] - ======== Starting Phase 4 Fine-tuning (Run: ft_s7_nameerror_fix_20250529_083352) ========
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:187] - Full effective configuration for this run: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase4_finetuning', 'run_name_suffix': 'ft_s7_nameerror_fix', 'log_dir': 'logs_finetune', 'log_file_finetune_base': 'finetune_hvt_xl', 'best_model_filename_base': 'best_finetuned_hvt', 'final_model_filename_base': 'final_finetuned_hvt', 'checkpoint_save_dir_name': 'checkpoints', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'ssl_pretrained_backbone_path': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_best_probe.pth', 'load_pretrained_backbone_from_ssl': True, 'resume_finetune_checkpoint_path': None, 'load_optimizer_scheduler_on_resume': True, 'ssl_pretrain_img_size_fallback': (448, 448), 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': None, 'img_size': (448, 448), 'num_classes': 7, 'train_split_ratio': 0.8, 'normalize_data': True, 'use_weighted_sampler': True, 'num_workers': 4, 'prefetch_factor': 2, 'model_architecture_name': 'DiseaseAwareHVT_SSL_Finetuned_S7', 'hvt_params_for_model_init': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.15, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.15, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'epochs': 60, 'batch_size': 16, 'accumulation_steps': 2, 'amp_enabled': True, 'clip_grad_norm': 1.0, 'log_interval': 10, 'optimizer': 'AdamW', 'weight_decay': 0.05, 'optimizer_params': {'betas': (0.9, 0.999), 'eps': 1e-08}, 'freeze_backbone_epochs': 10, 'lr_head_frozen_phase': 5e-05, 'lr_backbone_unfrozen_phase': 5e-06, 'lr_head_unfrozen_phase': 1e-05, 'scheduler': 'WarmupCosine', 'warmup_epochs': 5, 'eta_min_lr': 1e-07, 'loss_label_smoothing': 0.1, 'augmentations_enabled': True, 'evaluate_every_n_epochs': 1, 'early_stopping_patience': 20, 'metric_to_monitor_early_stopping': 'f1_macro'}
2025-05-29 08:33:52 - __main__ - INFO - [set_global_seed:71] - Global random seed set to: 42
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:190] - Device: cuda. GPU: Tesla T4
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:191] - cudnn.benchmark = True
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:193] - matmul_precision = 'high'
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:53] - [DATASET INIT - train] Root: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection, ImgSize: (448, 448), Normalize: True
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:74] - [DATASET INIT - train] Scanning: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-29 08:33:52 - phase4_finetuning.dataset - DEBUG - [__init__:67] - [DATASET INIT - train] Dataset name 'None' is None/empty, skipping.
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:105] - [DATASET INIT - train] Total valid image paths collected: 2137 from ~2137 items considered.
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:117] - [DATASET INIT - train] Dataset split size: 1709 samples.
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:128] - [DATASET INIT - train] Base RGB Transforms: Compose(
      ToImage()
      ToDtype(scale=True)
      Resize(size=[448, 448], interpolation=InterpolationMode.BICUBIC, antialias=True)
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)
)
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:53] - [DATASET INIT - val] Root: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection, ImgSize: (448, 448), Normalize: True
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:74] - [DATASET INIT - val] Scanning: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-29 08:33:52 - phase4_finetuning.dataset - DEBUG - [__init__:67] - [DATASET INIT - val] Dataset name 'None' is None/empty, skipping.
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:105] - [DATASET INIT - val] Total valid image paths collected: 2137 from ~2137 items considered.
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:117] - [DATASET INIT - val] Dataset split size: 428 samples.
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [__init__:128] - [DATASET INIT - val] Base RGB Transforms: Compose(
      ToImage()
      ToDtype(scale=True)
      Resize(size=[448, 448], interpolation=InterpolationMode.BICUBIC, antialias=True)
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)
)
2025-05-29 08:33:52 - phase4_finetuning.dataset - INFO - [get_class_weights:185] - Computed class weights (inv_freq for loss) for split 'train': [1.191 0.684 1.179 1.115 1.387 0.533 2.806]
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:205] - Using WeightedRandomSampler.
2025-05-29 08:33:52 - __main__ - INFO - [main_execution_logic:210] - Dataloaders: Train batches=106, Val batches=27
2025-05-29 08:33:52 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-29 08:33:55 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-29 08:33:58 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-29 08:33:58 - __main__ - INFO - [main_execution_logic:214] - Base HVT model instance created (num_classes=7).
2025-05-29 08:33:58 - __main__ - INFO - [main_execution_logic:230] - Optimizer created: AdamW. Initial group LRs set.
2025-05-29 08:33:58 - __main__.load_initial_ssl_weights - INFO - [load_initial_ssl_weights:109] - Loading initial SSL backbone weights from: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_best_probe.pth
2025-05-29 08:34:01 - __main__.load_initial_ssl_weights - INFO - [load_initial_ssl_weights:129] - SSL Backbone loaded: 450 direct, 0 PE interp, 2 head skip, 0 mismatch.
2025-05-29 08:34:01 - __main__.load_initial_ssl_weights - WARNING - [load_initial_ssl_weights:130] - Missing keys in SSL load: ['classifier_head.weight', 'classifier_head.bias']
2025-05-29 08:34:01 - __main__.load_initial_ssl_weights - INFO - [load_initial_ssl_weights:134] - Re-initialized HVT classifier_head for 7 FT classes (in_features=1536).
2025-05-29 08:34:02 - __main__ - INFO - [main_execution_logic:243] - Model ready on cuda. Total params: 273,615,751, Initial Trainable: 273,615,751
2025-05-29 08:34:02 - __main__ - INFO - [main_execution_logic:257] - Scheduler: WarmupCosine. WU Steps: 265, Total Steps: 3180, Resumed step: 0
2025-05-29 08:34:02 - phase4_finetuning.utils.augmentations - INFO - [__init__:23] - FinetuneAugmentation initialized with enhanced transforms.
2025-05-29 08:34:02 - phase4_finetuning.utils.augmentations - DEBUG - [__init__:24] - Augmentation pipeline: Compose(
      RandomHorizontalFlip(p=0.5)
      RandomVerticalFlip(p=0.3)
      RandomRotation(degrees=[-30.0, 30.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)
      ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.7, 1.3), hue=(-0.2, 0.2))
      RandomAffine(degrees=[-15.0, 15.0], translate=(0.2, 0.2), scale=(0.8, 1.2), shear=[-5.0, 5.0], interpolation=InterpolationMode.NEAREST, fill=0)
      RandomApply(    GaussianBlur(kernel_size=(3, 7), sigma=[0.1, 2.0]))
      RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)
      RandomSolarize(p=0.1, threshold=0.5)
)
2025-05-29 08:34:02 - phase4_finetuning.finetune.trainer - INFO - [__init__:44] - Finetuner initialized: device=cuda, accum_steps=2, lr_sched_on_batch=True, AMP=True
2025-05-29 08:34:02 - phase4_finetuning.finetune.trainer - INFO - [__init__:46] - Using training augmentations: FinetuneAugmentation
2025-05-29 08:34:02 - __main__ - INFO - [main_execution_logic:272] - Checkpoints will be saved in: /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_s7_nameerror_fix_20250529_083352/checkpoints
2025-05-29 08:34:02 - __main__ - INFO - [main_execution_logic:273] - Starting fine-tuning from epoch 1 for 60 total epochs. Monitoring 'f1_macro'.
2025-05-29 08:34:02 - __main__ - INFO - [main_execution_logic:300] - E1: Optim group 'head' base LR set to 5.00e-05
2025-05-29 08:34:09 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B6: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:09 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B7: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:09 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B8: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:09 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B9: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:09 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B10: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:10 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B11: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:10 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B12: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:10 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B13: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:10 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B14: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:10 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B15: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:10 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B16: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B17: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B18: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B19: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B20: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B21: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B22: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:11 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B23: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:12 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B24: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:12 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B25: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:12 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B26: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:12 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B27: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:12 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B28: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:12 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B29: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:13 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B30: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:13 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B31: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:13 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B32: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:13 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B33: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:13 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B34: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:13 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B35: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:14 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B36: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:14 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B37: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:14 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B38: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:14 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B39: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:14 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B40: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:14 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B41: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:15 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B42: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:15 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B43: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:15 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B44: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:15 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B45: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:15 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B46: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:15 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B47: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:16 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B48: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:16 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B49: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:16 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B50: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:16 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B51: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:16 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B52: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:16 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B53: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:17 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B54: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:17 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B55: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:17 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B56: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:17 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B57: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:17 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B58: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:17 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B59: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:18 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B60: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:18 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B61: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:18 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B62: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:18 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B63: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:18 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B64: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:18 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B65: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B66: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B67: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B68: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B69: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B70: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B71: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:19 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B72: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:20 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B73: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:20 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B74: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:20 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B75: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:20 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B76: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:20 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B77: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:21 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B78: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:21 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B79: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:21 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B80: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:21 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B81: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:21 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B82: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:21 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B83: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:22 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B84: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:22 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B85: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:22 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B86: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:22 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B87: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:22 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B88: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:22 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B89: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:23 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B90: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:23 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B91: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:23 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B92: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:23 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B93: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:23 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B94: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:23 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B95: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B96: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B97: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B98: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B99: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B100: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B101: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:24 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B102: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:25 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B103: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:25 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B104: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:25 - phase4_finetuning.finetune.trainer - ERROR - [train_one_epoch:87] - E1 B105: Non-finite loss (nan). Skipping grad.
2025-05-29 08:34:25 - phase4_finetuning.finetune.trainer - INFO - [train_one_epoch:114] - Epoch 1 training finished. Avg Loss: 2.1678, Final LR for epoch: 5.66e-07, Opt Steps: 3
2025-05-29 08:34:26 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:26 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:26 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:26 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:26 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:27 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:28 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:28 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:28 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:28 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:28 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:29 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:30 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:30 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:30 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:157] - Non-finite validation loss detected (nan). Skipping this batch for loss avg.
2025-05-29 08:34:30 - phase4_finetuning.finetune.trainer - WARNING - [validate_one_epoch:169] - Validation yielded no outputs. Returning zero metrics beyond val_loss.
2025-05-29 08:34:30 - phase4_finetuning.finetune.trainer - INFO - [validate_one_epoch:182] - Validation finished. Avg Loss: 0.0000, accuracy: 0.0000, f1_macro: 0.0000, f1_weighted: 0.0000, precision_macro: 0.0000, recall_macro: 0.0000
2025-05-29 08:34:30 - __main__ - INFO - [main_execution_logic:321] - E1: Val f1_macro (0.0000) not better than 0.0000. Patience: 1/20
2025-05-29 08:34:30 - __main__ - CRITICAL - [main_execution_logic:328] - FT error at E2: name 'epoch_start_time' is not defined
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/cvpr25/phase4_finetuning/main.py", line 324, in main_execution_logic
    logger.info(f"Epoch {epoch_1_based} completed in {(time.time() - epoch_start_time):.2f}s.")
NameError: name 'epoch_start_time' is not defined
2025-05-29 08:34:30 - __main__ - INFO - [main_execution_logic:332] - FT ended. Absolute epochs completed: 1.
2025-05-29 08:34:32 - phase4_finetuning.finetune.trainer - INFO - [save_model_checkpoint:189] - Model checkpoint saved to /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_s7_nameerror_fix_20250529_083352/checkpoints/final_finetuned_hvt_ft_s7_nameerror_fix_20250529_083352.pth
2025-05-29 08:34:32 - __main__ - INFO - [main_execution_logic:336] - FT summary: Best validation 'f1_macro': 0.0000
2025-05-29 08:34:32 - __main__ - INFO - [<module>:344] - Application exited with code 1.
