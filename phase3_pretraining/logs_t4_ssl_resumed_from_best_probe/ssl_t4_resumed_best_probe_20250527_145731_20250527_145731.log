2025-05-27 14:57:33 - root - INFO - [setup_logging:57] - Logging configured. File: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/logs_t4_ssl_resumed_from_best_probe/ssl_t4_resumed_best_probe_20250527_145731_20250527_145731.log (Level: DEBUG), Console Level: INFO
2025-05-27 14:57:35 - phase2_model.models - INFO - [<module>:17] - Models (InceptionV3Baseline, DFCA, DiseaseAwareHVT, factory) imported successfully into phase2_model.models package.
2025-05-27 14:57:35 - phase2_model - INFO - [<module>:25] - Models re-exported successfully by phase2_model/__init__.py.
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:58] - ======== Starting Phase 3: HVT Self-Supervised Pre-training (Run ID: 20250527_145731) ========
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:59] - Full run configuration snapshot: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining', 'log_dir_name': 'logs_t4_ssl_resumed_from_best_probe', 'log_file_pretrain': 'ssl_t4_resumed_best_probe.log', 'checkpoint_dir_name': 'pretrain_checkpoints_hvt_xl', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'resume_checkpoint_path': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth', 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': 'Augmented Dataset', 'train_split_ratio': 0.95, 'num_classes': 7, 'num_workers': 4, 'prefetch_factor': 2, 'hvt_params_for_backbone': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'pretrain_img_size': (448, 448), 'pretrain_epochs': 80, 'pretrain_batch_size': 32, 'accumulation_steps': 2, 'pretrain_lr': 0.0005, 'pretrain_optimizer': 'AdamW', 'pretrain_scheduler': 'WarmupCosine', 'warmup_epochs': 10, 'eta_min_lr': 1e-06, 'pretrain_weight_decay': 0.05, 'temperature': 0.1, 'projection_dim': 256, 'projection_hidden_dim': 4096, 'simclr_s': 1.0, 'simclr_p_grayscale': 0.2, 'simclr_p_gaussian_blur': 0.5, 'simclr_rrc_scale_min': 0.08, 'evaluate_every_n_epochs': 10, 'linear_probe_epochs': 10, 'linear_probe_lr': 0.1, 'probe_optimizer': 'SGD', 'probe_momentum': 0.9, 'probe_weight_decay': 0.0, 'probe_batch_size': 64, 'save_every_n_epochs': 20, 'model_arch_name_for_ckpt': 'hvt_xl_simclr_t4_resumed', 'clip_grad_norm': 1.0}
2025-05-27 14:57:35 - __main__ - INFO - [apply_pytorch_optimizations:49] - torch.backends.cudnn.benchmark = True
2025-05-27 14:57:35 - __main__ - INFO - [apply_pytorch_optimizations:52] - torch.set_float32_matmul_precision('high')
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:64] - Global random seed: 42. Device: cuda.
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:65] - CUDA Device: Tesla T4
2025-05-27 14:57:35 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-27 14:57:35 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 14:57:35 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 14:57:36 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 14:57:36 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-27 14:57:36 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-27 14:57:36 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 14:57:37 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 14:57:38 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 14:57:38 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-27 14:57:38 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='val', img_size=(448, 448), use_spectral=False
2025-05-27 14:57:38 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 14:57:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 14:57:40 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 14:57:40 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'val' size: 457 samples.
2025-05-27 14:57:40 - __main__ - INFO - [main_pretrain_script:80] - Dataset sizes: Pretrain=8680, ProbeTrain=8680, ProbeVal=457
2025-05-27 14:57:40 - __main__ - INFO - [main_pretrain_script:91] - Pretrain DataLoader: 271 batches, BS=32, Workers=4
2025-05-27 14:57:40 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:36] - Initializing HVTForPretraining wrapper for img_size: (448, 448)
2025-05-27 14:57:40 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:44] - Instantiating HVTBackbone using parameters defined in Phase 3 config (hvt_params_for_backbone).
2025-05-27 14:57:40 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-27 14:57:44 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-27 14:57:48 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-27 14:57:48 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:70] - Projection head input dimension set to: 1536
2025-05-27 14:57:49 - phase3_pretraining.models.projection_head - INFO - [__init__:29] - ProjectionHead initialized: In=1536, Hidden=4096, Out=256, BatchNorm=True
2025-05-27 14:57:49 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:79] - HVTForPretraining wrapper initialized successfully.
2025-05-27 14:57:49 - phase3_pretraining.utils.augmentations - INFO - [__init__:28] - SimCLRAugmentation: img_size=(448, 448), s=1.0, p_gray=0.2, p_blur=0.5, rrc_min_scale=0.08
2025-05-27 14:57:49 - phase3_pretraining.utils.losses - INFO - [__init__:13] - InfoNCELoss initialized with temperature: 0.1
2025-05-27 14:57:49 - phase3_pretraining.pretrain.trainer - INFO - [__init__:42] - Initializing Pretrainer...
2025-05-27 14:57:49 - phase3_pretraining.pretrain.trainer - INFO - [__init__:59] - Optimizer: AdamW, BaseLR: 0.0005, WD: 0.05
2025-05-27 14:57:49 - phase3_pretraining.pretrain.trainer - INFO - [__init__:70] - Pretrainer init complete. AMP: True, Accum: 2, ClipGrad: 1.0
2025-05-27 14:57:49 - __main__ - INFO - [main_pretrain_script:103] - Attempting to resume training from checkpoint: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:110] - Loaded backbone and projection head state dicts.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:115] - Optimizer state loaded.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:117] - GradScaler state loaded.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:134] - Scheduler state_dict found. Pretrainer will use 'numerically_last_completed_epoch' for re-initialization.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:135] - Resuming from completed SSL epoch 30. Will start training at epoch 31. Best probe: 41.79%
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:152] - Starting/Resuming SimCLR pre-training from epoch 31 up to 80 total epochs.
2025-05-27 14:58:46 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:85] - Resuming: Scheduler will be initialized with last_step/epoch equivalent to completion of epoch 30 (0-indexed value: 8129).
2025-05-27 14:58:46 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:109] - Scheduler: WarmupCosine (WarmupSteps=2710, TotalTrainSteps=21680). Initialized with last_epoch (step): 8130
2025-05-27 15:06:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E31: Applying leftover grads from accum (1 steps).
2025-05-27 15:06:09 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 31 Training Summary: AvgLoss=1.7080, OptSteps=136, FinalLR=4.01e-04
2025-05-27 15:06:09 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 31/80 | Duration: 442.74s | Eff. Samples/sec: 39.17 | Avg Loss: 1.7080 | LR: 4.01e-04
2025-05-27 15:06:09 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E31 END: Alloc 4325.1MB, MaxAlloc 9430.9MB
2025-05-27 15:13:28 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E32: Applying leftover grads from accum (1 steps).
2025-05-27 15:13:29 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 32 Training Summary: AvgLoss=1.6430, OptSteps=136, FinalLR=3.97e-04
2025-05-27 15:13:29 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 32/80 | Duration: 439.97s | Eff. Samples/sec: 39.42 | Avg Loss: 1.6430 | LR: 3.97e-04
2025-05-27 15:13:29 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E32 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:20:46 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E33: Applying leftover grads from accum (1 steps).
2025-05-27 15:20:46 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 33 Training Summary: AvgLoss=1.6986, OptSteps=136, FinalLR=3.92e-04
2025-05-27 15:20:46 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 33/80 | Duration: 437.20s | Eff. Samples/sec: 39.67 | Avg Loss: 1.6986 | LR: 3.92e-04
2025-05-27 15:20:46 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E33 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:28:04 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E34: Applying leftover grads from accum (1 steps).
2025-05-27 15:28:05 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 34 Training Summary: AvgLoss=1.6131, OptSteps=136, FinalLR=3.88e-04
2025-05-27 15:28:05 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 34/80 | Duration: 438.54s | Eff. Samples/sec: 39.55 | Avg Loss: 1.6131 | LR: 3.88e-04
2025-05-27 15:28:05 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E34 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:35:20 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E35: Applying leftover grads from accum (1 steps).
2025-05-27 15:35:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 35 Training Summary: AvgLoss=1.5219, OptSteps=136, FinalLR=3.83e-04
2025-05-27 15:35:20 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 35/80 | Duration: 435.53s | Eff. Samples/sec: 39.82 | Avg Loss: 1.5219 | LR: 3.83e-04
2025-05-27 15:35:20 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E35 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:42:35 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E36: Applying leftover grads from accum (1 steps).
2025-05-27 15:42:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 36 Training Summary: AvgLoss=1.6070, OptSteps=136, FinalLR=3.78e-04
2025-05-27 15:42:35 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 36/80 | Duration: 435.25s | Eff. Samples/sec: 39.85 | Avg Loss: 1.6070 | LR: 3.78e-04
2025-05-27 15:42:35 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E36 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:49:51 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E37: Applying leftover grads from accum (1 steps).
2025-05-27 15:49:51 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 37 Training Summary: AvgLoss=1.6002, OptSteps=136, FinalLR=3.73e-04
2025-05-27 15:49:51 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 37/80 | Duration: 435.76s | Eff. Samples/sec: 39.80 | Avg Loss: 1.6002 | LR: 3.73e-04
2025-05-27 15:49:51 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E37 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:57:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E38: Applying leftover grads from accum (1 steps).
2025-05-27 15:57:08 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 38 Training Summary: AvgLoss=1.6459, OptSteps=136, FinalLR=3.68e-04
2025-05-27 15:57:08 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 38/80 | Duration: 437.12s | Eff. Samples/sec: 39.68 | Avg Loss: 1.6459 | LR: 3.68e-04
2025-05-27 15:57:08 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E38 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 16:04:26 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E39: Applying leftover grads from accum (1 steps).
2025-05-27 16:04:26 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 39 Training Summary: AvgLoss=1.5386, OptSteps=136, FinalLR=3.63e-04
2025-05-27 16:04:26 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 39/80 | Duration: 437.82s | Eff. Samples/sec: 39.61 | Avg Loss: 1.5386 | LR: 3.63e-04
2025-05-27 16:04:26 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E39 END: Alloc 4325.1MB, MaxAlloc 9436.6MB
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E40: Applying leftover grads from accum (1 steps).
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 40 Training Summary: AvgLoss=1.4906, OptSteps=136, FinalLR=3.58e-04
2025-05-27 16:11:42 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 40/80 | Duration: 436.34s | Eff. Samples/sec: 39.75 | Avg Loss: 1.4906 | LR: 3.58e-04
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 40) ---
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E40: Feature dim for linear classifier: 1536
2025-05-27 16:32:06 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:269] - Linear Probe (SSL E40) Validation Accuracy: 31.07% (142/457)
2025-05-27 16:32:06 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:193] - Saving checkpoint (reflecting completion of SSL epoch 40) to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_40.pth...
2025-05-27 16:32:11 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:207] - Checkpoint for SSL epoch 40 saved to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_40.pth
2025-05-27 16:32:11 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E40 END: Alloc 4325.1MB, MaxAlloc 9436.6MB
2025-05-27 16:39:29 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E41: Applying leftover grads from accum (1 steps).
2025-05-27 16:39:29 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 41 Training Summary: AvgLoss=1.5083, OptSteps=136, FinalLR=3.53e-04
2025-05-27 16:39:29 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 41/80 | Duration: 438.10s | Eff. Samples/sec: 39.59 | Avg Loss: 1.5083 | LR: 3.53e-04
2025-05-27 16:39:29 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E41 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
