# The base configuration for all experiments.
# This should be a snapshot of the config that got you the 90.24% result.
base_config:
  run_name_prefix: "HVT-XL-FineTune"
  seed: 42
  device: "cuda"
  cudnn_benchmark: true
  PACKAGE_ROOT_PATH: "phase4_finetuning"
  PROJECT_ROOT_PATH: "."
  
  data:
    root_dir: "/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection"
    original_dataset_name: "Original Dataset"
    augmented_dataset_name: "Augmented Dataset"
    img_size: [448, 448]
    train_split_ratio: 0.9
    num_workers: 0
    use_weighted_sampler: true
  
  model:
    ssl_pretrained_path: "phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_best_probe.pth" # SSL Model
    resume_finetune_path: null
    hvt_params:
      patch_size: 14
      embed_dim_rgb: 192
      spectral_channels: 0
      depths: [3, 6, 24, 3]
      num_heads: [6, 12, 24, 48]
      mlp_ratio: 4.0
      qkv_bias: true
      model_drop_rate: 0.1
      drop_path_rate: 0.2
      norm_layer_name: "LayerNorm"
      use_dfca: false
      use_gradient_checkpointing: true
      enable_consistency_loss_heads: false # Disabled for simplicity, can be ablated

  training:
    epochs: 100
    batch_size: 16
    accumulation_steps: 2
    freeze_backbone_epochs: 10
    clip_grad_norm: 1.0
    amp_enabled: true
    optimizer: { lr_backbone_unfrozen: 2.0e-05, lr_head_unfrozen: 0.0002 }
    scheduler: { name: "onecyclelr", pct_start: 0.1, div_factor: 10, final_div_factor: 1000 }
    loss: { type: "CombinedLoss", use_class_weights: true, label_smoothing: 0.1, focal_alpha: 0.5, focal_gamma: 2.0, weights: { ce: 0.5, focal: 0.5 } }

  augmentations:
    strategy: "cotton_disease"
    severity: "strong"
    mixup_alpha: 0.2
    cutmix_prob: 0.2

  evaluation:
    use_ema: true
    ema_decay: 0.9999
    tta_enabled: true
    early_stopping: { metric: "f1_macro", patience: 20, min_delta: 0.001 }
  
  torch_compile: { enable: true, mode: "reduce-overhead" }

# --- List of all experiments to run ---
experiments:
  - name: "01_full_system"
    # No changes, runs the base config
  - name: "02_ablation_no_ssl"
    changes:
      model:
        ssl_pretrained_path: null
  - name: "03_ablation_no_advanced_augs"
    changes:
      augmentations: { strategy: "minimal", mixup_alpha: 0.0, cutmix_prob: 0.0 }
  - name: "04_ablation_no_ema_tta"
    changes:
      evaluation: { use_ema: false, tta_enabled: false }
  - name: "05_ablation_no_freeze"
    changes:
      training: { freeze_backbone_epochs: 0 }
  - name: "06_ablation_simple_loss"
    changes:
      training: { loss: { type: "CrossEntropyLoss", use_class_weights: false, label_smoothing: 0.0 } }
  - name: "07_sota_comparison_resnet101"
    changes:
      # We will handle this model swap in the runner script
      model_override:
        type: "timm"
        name: "resnet101"
      model:
        ssl_pretrained_path: null # Baselines are not SSL pre-trained by us
  - name: "08_sota_comparison_vit_base"
    changes:
      model_override:
        type: "timm"
        name: "vit_base_patch16_224.augreg_in21k_ft_in1k" # A strong ViT baseline
      model:
        ssl_pretrained_path: null