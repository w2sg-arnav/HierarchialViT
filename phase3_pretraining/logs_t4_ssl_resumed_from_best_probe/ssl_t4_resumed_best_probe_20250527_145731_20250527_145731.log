2025-05-27 14:57:33 - root - INFO - [setup_logging:57] - Logging configured. File: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/logs_t4_ssl_resumed_from_best_probe/ssl_t4_resumed_best_probe_20250527_145731_20250527_145731.log (Level: DEBUG), Console Level: INFO
2025-05-27 14:57:35 - phase2_model.models - INFO - [<module>:17] - Models (InceptionV3Baseline, DFCA, DiseaseAwareHVT, factory) imported successfully into phase2_model.models package.
2025-05-27 14:57:35 - phase2_model - INFO - [<module>:25] - Models re-exported successfully by phase2_model/__init__.py.
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:58] - ======== Starting Phase 3: HVT Self-Supervised Pre-training (Run ID: 20250527_145731) ========
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:59] - Full run configuration snapshot: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining', 'log_dir_name': 'logs_t4_ssl_resumed_from_best_probe', 'log_file_pretrain': 'ssl_t4_resumed_best_probe.log', 'checkpoint_dir_name': 'pretrain_checkpoints_hvt_xl', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'resume_checkpoint_path': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth', 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': 'Augmented Dataset', 'train_split_ratio': 0.95, 'num_classes': 7, 'num_workers': 4, 'prefetch_factor': 2, 'hvt_params_for_backbone': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'pretrain_img_size': (448, 448), 'pretrain_epochs': 80, 'pretrain_batch_size': 32, 'accumulation_steps': 2, 'pretrain_lr': 0.0005, 'pretrain_optimizer': 'AdamW', 'pretrain_scheduler': 'WarmupCosine', 'warmup_epochs': 10, 'eta_min_lr': 1e-06, 'pretrain_weight_decay': 0.05, 'temperature': 0.1, 'projection_dim': 256, 'projection_hidden_dim': 4096, 'simclr_s': 1.0, 'simclr_p_grayscale': 0.2, 'simclr_p_gaussian_blur': 0.5, 'simclr_rrc_scale_min': 0.08, 'evaluate_every_n_epochs': 10, 'linear_probe_epochs': 10, 'linear_probe_lr': 0.1, 'probe_optimizer': 'SGD', 'probe_momentum': 0.9, 'probe_weight_decay': 0.0, 'probe_batch_size': 64, 'save_every_n_epochs': 20, 'model_arch_name_for_ckpt': 'hvt_xl_simclr_t4_resumed', 'clip_grad_norm': 1.0}
2025-05-27 14:57:35 - __main__ - INFO - [apply_pytorch_optimizations:49] - torch.backends.cudnn.benchmark = True
2025-05-27 14:57:35 - __main__ - INFO - [apply_pytorch_optimizations:52] - torch.set_float32_matmul_precision('high')
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:64] - Global random seed: 42. Device: cuda.
2025-05-27 14:57:35 - __main__ - INFO - [main_pretrain_script:65] - CUDA Device: Tesla T4
2025-05-27 14:57:35 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-27 14:57:35 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 14:57:35 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 14:57:36 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 14:57:36 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-27 14:57:36 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-27 14:57:36 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 14:57:37 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 14:57:38 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 14:57:38 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-27 14:57:38 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='val', img_size=(448, 448), use_spectral=False
2025-05-27 14:57:38 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 14:57:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 14:57:40 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 14:57:40 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'val' size: 457 samples.
2025-05-27 14:57:40 - __main__ - INFO - [main_pretrain_script:80] - Dataset sizes: Pretrain=8680, ProbeTrain=8680, ProbeVal=457
2025-05-27 14:57:40 - __main__ - INFO - [main_pretrain_script:91] - Pretrain DataLoader: 271 batches, BS=32, Workers=4
2025-05-27 14:57:40 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:36] - Initializing HVTForPretraining wrapper for img_size: (448, 448)
2025-05-27 14:57:40 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:44] - Instantiating HVTBackbone using parameters defined in Phase 3 config (hvt_params_for_backbone).
2025-05-27 14:57:40 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-27 14:57:44 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-27 14:57:48 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-27 14:57:48 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:70] - Projection head input dimension set to: 1536
2025-05-27 14:57:49 - phase3_pretraining.models.projection_head - INFO - [__init__:29] - ProjectionHead initialized: In=1536, Hidden=4096, Out=256, BatchNorm=True
2025-05-27 14:57:49 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:79] - HVTForPretraining wrapper initialized successfully.
2025-05-27 14:57:49 - phase3_pretraining.utils.augmentations - INFO - [__init__:28] - SimCLRAugmentation: img_size=(448, 448), s=1.0, p_gray=0.2, p_blur=0.5, rrc_min_scale=0.08
2025-05-27 14:57:49 - phase3_pretraining.utils.losses - INFO - [__init__:13] - InfoNCELoss initialized with temperature: 0.1
2025-05-27 14:57:49 - phase3_pretraining.pretrain.trainer - INFO - [__init__:42] - Initializing Pretrainer...
2025-05-27 14:57:49 - phase3_pretraining.pretrain.trainer - INFO - [__init__:59] - Optimizer: AdamW, BaseLR: 0.0005, WD: 0.05
2025-05-27 14:57:49 - phase3_pretraining.pretrain.trainer - INFO - [__init__:70] - Pretrainer init complete. AMP: True, Accum: 2, ClipGrad: 1.0
2025-05-27 14:57:49 - __main__ - INFO - [main_pretrain_script:103] - Attempting to resume training from checkpoint: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:110] - Loaded backbone and projection head state dicts.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:115] - Optimizer state loaded.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:117] - GradScaler state loaded.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:134] - Scheduler state_dict found. Pretrainer will use 'numerically_last_completed_epoch' for re-initialization.
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:135] - Resuming from completed SSL epoch 30. Will start training at epoch 31. Best probe: 41.79%
2025-05-27 14:58:46 - __main__ - INFO - [main_pretrain_script:152] - Starting/Resuming SimCLR pre-training from epoch 31 up to 80 total epochs.
2025-05-27 14:58:46 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:85] - Resuming: Scheduler will be initialized with last_step/epoch equivalent to completion of epoch 30 (0-indexed value: 8129).
2025-05-27 14:58:46 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:109] - Scheduler: WarmupCosine (WarmupSteps=2710, TotalTrainSteps=21680). Initialized with last_epoch (step): 8130
2025-05-27 15:06:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E31: Applying leftover grads from accum (1 steps).
2025-05-27 15:06:09 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 31 Training Summary: AvgLoss=1.7080, OptSteps=136, FinalLR=4.01e-04
2025-05-27 15:06:09 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 31/80 | Duration: 442.74s | Eff. Samples/sec: 39.17 | Avg Loss: 1.7080 | LR: 4.01e-04
2025-05-27 15:06:09 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E31 END: Alloc 4325.1MB, MaxAlloc 9430.9MB
2025-05-27 15:13:28 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E32: Applying leftover grads from accum (1 steps).
2025-05-27 15:13:29 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 32 Training Summary: AvgLoss=1.6430, OptSteps=136, FinalLR=3.97e-04
2025-05-27 15:13:29 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 32/80 | Duration: 439.97s | Eff. Samples/sec: 39.42 | Avg Loss: 1.6430 | LR: 3.97e-04
2025-05-27 15:13:29 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E32 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:20:46 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E33: Applying leftover grads from accum (1 steps).
2025-05-27 15:20:46 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 33 Training Summary: AvgLoss=1.6986, OptSteps=136, FinalLR=3.92e-04
2025-05-27 15:20:46 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 33/80 | Duration: 437.20s | Eff. Samples/sec: 39.67 | Avg Loss: 1.6986 | LR: 3.92e-04
2025-05-27 15:20:46 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E33 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:28:04 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E34: Applying leftover grads from accum (1 steps).
2025-05-27 15:28:05 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 34 Training Summary: AvgLoss=1.6131, OptSteps=136, FinalLR=3.88e-04
2025-05-27 15:28:05 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 34/80 | Duration: 438.54s | Eff. Samples/sec: 39.55 | Avg Loss: 1.6131 | LR: 3.88e-04
2025-05-27 15:28:05 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E34 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:35:20 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E35: Applying leftover grads from accum (1 steps).
2025-05-27 15:35:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 35 Training Summary: AvgLoss=1.5219, OptSteps=136, FinalLR=3.83e-04
2025-05-27 15:35:20 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 35/80 | Duration: 435.53s | Eff. Samples/sec: 39.82 | Avg Loss: 1.5219 | LR: 3.83e-04
2025-05-27 15:35:20 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E35 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:42:35 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E36: Applying leftover grads from accum (1 steps).
2025-05-27 15:42:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 36 Training Summary: AvgLoss=1.6070, OptSteps=136, FinalLR=3.78e-04
2025-05-27 15:42:35 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 36/80 | Duration: 435.25s | Eff. Samples/sec: 39.85 | Avg Loss: 1.6070 | LR: 3.78e-04
2025-05-27 15:42:35 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E36 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:49:51 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E37: Applying leftover grads from accum (1 steps).
2025-05-27 15:49:51 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 37 Training Summary: AvgLoss=1.6002, OptSteps=136, FinalLR=3.73e-04
2025-05-27 15:49:51 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 37/80 | Duration: 435.76s | Eff. Samples/sec: 39.80 | Avg Loss: 1.6002 | LR: 3.73e-04
2025-05-27 15:49:51 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E37 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 15:57:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E38: Applying leftover grads from accum (1 steps).
2025-05-27 15:57:08 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 38 Training Summary: AvgLoss=1.6459, OptSteps=136, FinalLR=3.68e-04
2025-05-27 15:57:08 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 38/80 | Duration: 437.12s | Eff. Samples/sec: 39.68 | Avg Loss: 1.6459 | LR: 3.68e-04
2025-05-27 15:57:08 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E38 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 16:04:26 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E39: Applying leftover grads from accum (1 steps).
2025-05-27 16:04:26 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 39 Training Summary: AvgLoss=1.5386, OptSteps=136, FinalLR=3.63e-04
2025-05-27 16:04:26 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 39/80 | Duration: 437.82s | Eff. Samples/sec: 39.61 | Avg Loss: 1.5386 | LR: 3.63e-04
2025-05-27 16:04:26 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E39 END: Alloc 4325.1MB, MaxAlloc 9436.6MB
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E40: Applying leftover grads from accum (1 steps).
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 40 Training Summary: AvgLoss=1.4906, OptSteps=136, FinalLR=3.58e-04
2025-05-27 16:11:42 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 40/80 | Duration: 436.34s | Eff. Samples/sec: 39.75 | Avg Loss: 1.4906 | LR: 3.58e-04
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 40) ---
2025-05-27 16:11:42 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E40: Feature dim for linear classifier: 1536
2025-05-27 16:32:06 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:269] - Linear Probe (SSL E40) Validation Accuracy: 31.07% (142/457)
2025-05-27 16:32:06 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:193] - Saving checkpoint (reflecting completion of SSL epoch 40) to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_40.pth...
2025-05-27 16:32:11 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:207] - Checkpoint for SSL epoch 40 saved to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_40.pth
2025-05-27 16:32:11 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E40 END: Alloc 4325.1MB, MaxAlloc 9436.6MB
2025-05-27 16:39:29 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E41: Applying leftover grads from accum (1 steps).
2025-05-27 16:39:29 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 41 Training Summary: AvgLoss=1.5083, OptSteps=136, FinalLR=3.53e-04
2025-05-27 16:39:29 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 41/80 | Duration: 438.10s | Eff. Samples/sec: 39.59 | Avg Loss: 1.5083 | LR: 3.53e-04
2025-05-27 16:39:29 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E41 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 16:46:48 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E42: Applying leftover grads from accum (1 steps).
2025-05-27 16:46:49 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 42 Training Summary: AvgLoss=1.4427, OptSteps=136, FinalLR=3.48e-04
2025-05-27 16:46:49 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 42/80 | Duration: 439.78s | Eff. Samples/sec: 39.44 | Avg Loss: 1.4427 | LR: 3.48e-04
2025-05-27 16:46:49 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E42 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 16:54:05 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E43: Applying leftover grads from accum (1 steps).
2025-05-27 16:54:06 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 43 Training Summary: AvgLoss=1.3905, OptSteps=136, FinalLR=3.43e-04
2025-05-27 16:54:06 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 43/80 | Duration: 436.85s | Eff. Samples/sec: 39.70 | Avg Loss: 1.3905 | LR: 3.43e-04
2025-05-27 16:54:06 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E43 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:01:22 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E44: Applying leftover grads from accum (1 steps).
2025-05-27 17:01:22 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 44 Training Summary: AvgLoss=1.4111, OptSteps=136, FinalLR=3.38e-04
2025-05-27 17:01:22 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 44/80 | Duration: 436.31s | Eff. Samples/sec: 39.75 | Avg Loss: 1.4111 | LR: 3.38e-04
2025-05-27 17:01:22 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E44 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:08:38 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E45: Applying leftover grads from accum (1 steps).
2025-05-27 17:08:39 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 45 Training Summary: AvgLoss=1.3316, OptSteps=136, FinalLR=3.32e-04
2025-05-27 17:08:39 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 45/80 | Duration: 436.86s | Eff. Samples/sec: 39.70 | Avg Loss: 1.3316 | LR: 3.32e-04
2025-05-27 17:08:39 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E45 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:15:57 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E46: Applying leftover grads from accum (1 steps).
2025-05-27 17:15:57 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 46 Training Summary: AvgLoss=1.3247, OptSteps=136, FinalLR=3.27e-04
2025-05-27 17:15:57 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 46/80 | Duration: 438.55s | Eff. Samples/sec: 39.55 | Avg Loss: 1.3247 | LR: 3.27e-04
2025-05-27 17:15:57 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E46 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:23:14 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E47: Applying leftover grads from accum (1 steps).
2025-05-27 17:23:14 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 47 Training Summary: AvgLoss=1.2994, OptSteps=136, FinalLR=3.22e-04
2025-05-27 17:23:14 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 47/80 | Duration: 437.09s | Eff. Samples/sec: 39.68 | Avg Loss: 1.2994 | LR: 3.22e-04
2025-05-27 17:23:14 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E47 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:30:34 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E48: Applying leftover grads from accum (1 steps).
2025-05-27 17:30:34 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 48 Training Summary: AvgLoss=1.1724, OptSteps=136, FinalLR=3.16e-04
2025-05-27 17:30:34 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 48/80 | Duration: 439.64s | Eff. Samples/sec: 39.45 | Avg Loss: 1.1724 | LR: 3.16e-04
2025-05-27 17:30:34 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E48 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:37:50 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E49: Applying leftover grads from accum (1 steps).
2025-05-27 17:37:50 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 49 Training Summary: AvgLoss=1.2528, OptSteps=136, FinalLR=3.11e-04
2025-05-27 17:37:50 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 49/80 | Duration: 435.93s | Eff. Samples/sec: 39.79 | Avg Loss: 1.2528 | LR: 3.11e-04
2025-05-27 17:37:50 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E49 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 17:45:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E50: Applying leftover grads from accum (1 steps).
2025-05-27 17:45:09 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 50 Training Summary: AvgLoss=1.1995, OptSteps=136, FinalLR=3.05e-04
2025-05-27 17:45:09 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 50/80 | Duration: 438.60s | Eff. Samples/sec: 39.54 | Avg Loss: 1.1995 | LR: 3.05e-04
2025-05-27 17:45:09 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 50) ---
2025-05-27 17:45:09 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E50: Feature dim for linear classifier: 1536
2025-05-27 18:05:29 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:269] - Linear Probe (SSL E50) Validation Accuracy: 31.73% (145/457)
2025-05-27 18:05:29 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E50 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:12:47 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E51: Applying leftover grads from accum (1 steps).
2025-05-27 18:12:47 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 51 Training Summary: AvgLoss=1.1657, OptSteps=136, FinalLR=3.00e-04
2025-05-27 18:12:47 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 51/80 | Duration: 437.77s | Eff. Samples/sec: 39.62 | Avg Loss: 1.1657 | LR: 3.00e-04
2025-05-27 18:12:47 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E51 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:20:03 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E52: Applying leftover grads from accum (1 steps).
2025-05-27 18:20:03 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 52 Training Summary: AvgLoss=1.1419, OptSteps=136, FinalLR=2.94e-04
2025-05-27 18:20:03 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 52/80 | Duration: 436.46s | Eff. Samples/sec: 39.74 | Avg Loss: 1.1419 | LR: 2.94e-04
2025-05-27 18:20:03 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E52 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:27:20 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E53: Applying leftover grads from accum (1 steps).
2025-05-27 18:27:21 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 53 Training Summary: AvgLoss=1.1529, OptSteps=136, FinalLR=2.89e-04
2025-05-27 18:27:21 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 53/80 | Duration: 437.26s | Eff. Samples/sec: 39.67 | Avg Loss: 1.1529 | LR: 2.89e-04
2025-05-27 18:27:21 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E53 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:34:37 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E54: Applying leftover grads from accum (1 steps).
2025-05-27 18:34:38 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 54 Training Summary: AvgLoss=1.0833, OptSteps=136, FinalLR=2.83e-04
2025-05-27 18:34:38 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 54/80 | Duration: 437.05s | Eff. Samples/sec: 39.68 | Avg Loss: 1.0833 | LR: 2.83e-04
2025-05-27 18:34:38 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E54 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:41:54 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E55: Applying leftover grads from accum (1 steps).
2025-05-27 18:41:54 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 55 Training Summary: AvgLoss=1.0533, OptSteps=136, FinalLR=2.77e-04
2025-05-27 18:41:54 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 55/80 | Duration: 436.49s | Eff. Samples/sec: 39.74 | Avg Loss: 1.0533 | LR: 2.77e-04
2025-05-27 18:41:54 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E55 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:49:12 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E56: Applying leftover grads from accum (1 steps).
2025-05-27 18:49:13 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 56 Training Summary: AvgLoss=1.0084, OptSteps=136, FinalLR=2.72e-04
2025-05-27 18:49:13 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 56/80 | Duration: 438.51s | Eff. Samples/sec: 39.55 | Avg Loss: 1.0084 | LR: 2.72e-04
2025-05-27 18:49:13 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E56 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 18:56:28 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E57: Applying leftover grads from accum (1 steps).
2025-05-27 18:56:28 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 57 Training Summary: AvgLoss=1.0223, OptSteps=136, FinalLR=2.66e-04
2025-05-27 18:56:28 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 57/80 | Duration: 435.16s | Eff. Samples/sec: 39.86 | Avg Loss: 1.0223 | LR: 2.66e-04
2025-05-27 18:56:28 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E57 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 19:03:44 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E58: Applying leftover grads from accum (1 steps).
2025-05-27 19:03:44 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 58 Training Summary: AvgLoss=1.0295, OptSteps=136, FinalLR=2.61e-04
2025-05-27 19:03:44 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 58/80 | Duration: 436.24s | Eff. Samples/sec: 39.76 | Avg Loss: 1.0295 | LR: 2.61e-04
2025-05-27 19:03:44 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E58 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 19:11:00 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E59: Applying leftover grads from accum (1 steps).
2025-05-27 19:11:01 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 59 Training Summary: AvgLoss=1.0116, OptSteps=136, FinalLR=2.55e-04
2025-05-27 19:11:01 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 59/80 | Duration: 436.47s | Eff. Samples/sec: 39.74 | Avg Loss: 1.0116 | LR: 2.55e-04
2025-05-27 19:11:01 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E59 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 19:18:19 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E60: Applying leftover grads from accum (1 steps).
2025-05-27 19:18:19 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 60 Training Summary: AvgLoss=1.0162, OptSteps=136, FinalLR=2.49e-04
2025-05-27 19:18:19 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 60/80 | Duration: 438.35s | Eff. Samples/sec: 39.57 | Avg Loss: 1.0162 | LR: 2.49e-04
2025-05-27 19:18:19 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 60) ---
2025-05-27 19:18:19 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E60: Feature dim for linear classifier: 1536
2025-05-27 19:38:40 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:269] - Linear Probe (SSL E60) Validation Accuracy: 45.30% (207/457)
2025-05-27 19:38:40 - __main__ - INFO - [main_pretrain_script:175] - New best probe: 45.30% (SSL E60). Saving best model.
2025-05-27 19:38:40 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:193] - Saving checkpoint (reflecting completion of SSL epoch 60) to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_best_probe.pth...
2025-05-27 19:38:45 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:207] - Checkpoint for SSL epoch 60 saved to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_best_probe.pth
2025-05-27 19:38:45 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:193] - Saving checkpoint (reflecting completion of SSL epoch 60) to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_60.pth...
2025-05-27 19:38:53 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:207] - Checkpoint for SSL epoch 60 saved to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_60.pth
2025-05-27 19:38:53 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E60 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 19:46:12 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E61: Applying leftover grads from accum (1 steps).
2025-05-27 19:46:12 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 61 Training Summary: AvgLoss=1.0079, OptSteps=136, FinalLR=2.44e-04
2025-05-27 19:46:12 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 61/80 | Duration: 439.60s | Eff. Samples/sec: 39.45 | Avg Loss: 1.0079 | LR: 2.44e-04
2025-05-27 19:46:12 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E61 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 19:53:27 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E62: Applying leftover grads from accum (1 steps).
2025-05-27 19:53:27 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 62 Training Summary: AvgLoss=0.9633, OptSteps=136, FinalLR=2.38e-04
2025-05-27 19:53:27 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 62/80 | Duration: 434.71s | Eff. Samples/sec: 39.90 | Avg Loss: 0.9633 | LR: 2.38e-04
2025-05-27 19:53:27 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E62 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:00:47 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E63: Applying leftover grads from accum (1 steps).
2025-05-27 20:00:48 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 63 Training Summary: AvgLoss=0.9881, OptSteps=136, FinalLR=2.33e-04
2025-05-27 20:00:48 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 63/80 | Duration: 440.65s | Eff. Samples/sec: 39.36 | Avg Loss: 0.9881 | LR: 2.33e-04
2025-05-27 20:00:48 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E63 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:08:03 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E64: Applying leftover grads from accum (1 steps).
2025-05-27 20:08:03 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 64 Training Summary: AvgLoss=0.9128, OptSteps=136, FinalLR=2.27e-04
2025-05-27 20:08:03 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 64/80 | Duration: 435.35s | Eff. Samples/sec: 39.84 | Avg Loss: 0.9128 | LR: 2.27e-04
2025-05-27 20:08:03 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E64 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:15:20 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E65: Applying leftover grads from accum (1 steps).
2025-05-27 20:15:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 65 Training Summary: AvgLoss=0.9150, OptSteps=136, FinalLR=2.21e-04
2025-05-27 20:15:20 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 65/80 | Duration: 437.02s | Eff. Samples/sec: 39.69 | Avg Loss: 0.9150 | LR: 2.21e-04
2025-05-27 20:15:20 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E65 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:22:38 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E66: Applying leftover grads from accum (1 steps).
2025-05-27 20:22:38 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 66 Training Summary: AvgLoss=0.8691, OptSteps=136, FinalLR=2.16e-04
2025-05-27 20:22:38 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 66/80 | Duration: 438.43s | Eff. Samples/sec: 39.56 | Avg Loss: 0.8691 | LR: 2.16e-04
2025-05-27 20:22:38 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E66 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:29:56 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E67: Applying leftover grads from accum (1 steps).
2025-05-27 20:29:56 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 67 Training Summary: AvgLoss=0.8866, OptSteps=136, FinalLR=2.10e-04
2025-05-27 20:29:56 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 67/80 | Duration: 437.59s | Eff. Samples/sec: 39.64 | Avg Loss: 0.8866 | LR: 2.10e-04
2025-05-27 20:29:56 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E67 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:37:12 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E68: Applying leftover grads from accum (1 steps).
2025-05-27 20:37:12 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 68 Training Summary: AvgLoss=0.9052, OptSteps=136, FinalLR=2.05e-04
2025-05-27 20:37:12 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 68/80 | Duration: 436.36s | Eff. Samples/sec: 39.75 | Avg Loss: 0.9052 | LR: 2.05e-04
2025-05-27 20:37:12 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E68 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:44:31 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E69: Applying leftover grads from accum (1 steps).
2025-05-27 20:44:32 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 69 Training Summary: AvgLoss=0.8649, OptSteps=136, FinalLR=1.99e-04
2025-05-27 20:44:32 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 69/80 | Duration: 439.15s | Eff. Samples/sec: 39.49 | Avg Loss: 0.8649 | LR: 1.99e-04
2025-05-27 20:44:32 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E69 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 20:51:50 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E70: Applying leftover grads from accum (1 steps).
2025-05-27 20:51:50 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 70 Training Summary: AvgLoss=0.8345, OptSteps=136, FinalLR=1.94e-04
2025-05-27 20:51:50 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 70/80 | Duration: 438.47s | Eff. Samples/sec: 39.56 | Avg Loss: 0.8345 | LR: 1.94e-04
2025-05-27 20:51:50 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 70) ---
2025-05-27 20:51:50 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E70: Feature dim for linear classifier: 1536
2025-05-27 21:12:09 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:269] - Linear Probe (SSL E70) Validation Accuracy: 39.61% (181/457)
2025-05-27 21:12:09 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E70 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 21:19:27 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E71: Applying leftover grads from accum (1 steps).
2025-05-27 21:19:28 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 71 Training Summary: AvgLoss=0.8633, OptSteps=136, FinalLR=1.88e-04
2025-05-27 21:19:28 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 71/80 | Duration: 438.33s | Eff. Samples/sec: 39.57 | Avg Loss: 0.8633 | LR: 1.88e-04
2025-05-27 21:19:28 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E71 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 21:26:45 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E72: Applying leftover grads from accum (1 steps).
2025-05-27 21:26:46 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 72 Training Summary: AvgLoss=0.8003, OptSteps=136, FinalLR=1.83e-04
2025-05-27 21:26:46 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 72/80 | Duration: 437.91s | Eff. Samples/sec: 39.61 | Avg Loss: 0.8003 | LR: 1.83e-04
2025-05-27 21:26:46 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E72 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 21:33:59 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E73: Applying leftover grads from accum (1 steps).
2025-05-27 21:34:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 73 Training Summary: AvgLoss=0.8072, OptSteps=136, FinalLR=1.77e-04
2025-05-27 21:34:00 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 73/80 | Duration: 434.22s | Eff. Samples/sec: 39.94 | Avg Loss: 0.8072 | LR: 1.77e-04
2025-05-27 21:34:00 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E73 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 21:41:14 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E74: Applying leftover grads from accum (1 steps).
2025-05-27 21:41:15 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 74 Training Summary: AvgLoss=0.8025, OptSteps=136, FinalLR=1.72e-04
2025-05-27 21:41:15 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 74/80 | Duration: 434.70s | Eff. Samples/sec: 39.90 | Avg Loss: 0.8025 | LR: 1.72e-04
2025-05-27 21:41:15 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E74 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 21:48:29 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E75: Applying leftover grads from accum (1 steps).
2025-05-27 21:48:30 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 75 Training Summary: AvgLoss=0.7981, OptSteps=136, FinalLR=1.67e-04
2025-05-27 21:48:30 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 75/80 | Duration: 435.25s | Eff. Samples/sec: 39.85 | Avg Loss: 0.7981 | LR: 1.67e-04
2025-05-27 21:48:30 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E75 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 21:55:48 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E76: Applying leftover grads from accum (1 steps).
2025-05-27 21:55:48 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 76 Training Summary: AvgLoss=0.7979, OptSteps=136, FinalLR=1.61e-04
2025-05-27 21:55:48 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 76/80 | Duration: 438.25s | Eff. Samples/sec: 39.58 | Avg Loss: 0.7979 | LR: 1.61e-04
2025-05-27 21:55:48 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E76 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 22:03:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E77: Applying leftover grads from accum (1 steps).
2025-05-27 22:03:08 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 77 Training Summary: AvgLoss=0.7898, OptSteps=136, FinalLR=1.56e-04
2025-05-27 22:03:08 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 77/80 | Duration: 439.80s | Eff. Samples/sec: 39.44 | Avg Loss: 0.7898 | LR: 1.56e-04
2025-05-27 22:03:08 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E77 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 22:10:23 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E78: Applying leftover grads from accum (1 steps).
2025-05-27 22:10:23 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 78 Training Summary: AvgLoss=0.7489, OptSteps=136, FinalLR=1.51e-04
2025-05-27 22:10:23 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 78/80 | Duration: 435.30s | Eff. Samples/sec: 39.84 | Avg Loss: 0.7489 | LR: 1.51e-04
2025-05-27 22:10:23 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E78 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 22:17:38 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E79: Applying leftover grads from accum (1 steps).
2025-05-27 22:17:38 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 79 Training Summary: AvgLoss=0.7445, OptSteps=136, FinalLR=1.46e-04
2025-05-27 22:17:38 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 79/80 | Duration: 434.90s | Eff. Samples/sec: 39.88 | Avg Loss: 0.7445 | LR: 1.46e-04
2025-05-27 22:17:38 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E79 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 22:24:55 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E80: Applying leftover grads from accum (1 steps).
2025-05-27 22:24:56 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 80 Training Summary: AvgLoss=0.6796, OptSteps=136, FinalLR=1.41e-04
2025-05-27 22:24:56 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 80/80 | Duration: 437.72s | Eff. Samples/sec: 39.62 | Avg Loss: 0.6796 | LR: 1.41e-04
2025-05-27 22:24:56 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 80) ---
2025-05-27 22:24:56 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E80: Feature dim for linear classifier: 1536
2025-05-27 22:45:16 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:269] - Linear Probe (SSL E80) Validation Accuracy: 35.45% (162/457)
2025-05-27 22:45:16 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:193] - Saving checkpoint (reflecting completion of SSL epoch 80) to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_80.pth...
2025-05-27 22:45:21 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:207] - Checkpoint for SSL epoch 80 saved to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_epoch_80.pth
2025-05-27 22:45:22 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E80 END: Alloc 4325.1MB, MaxAlloc 9441.7MB
2025-05-27 22:45:22 - __main__ - INFO - [main_pretrain_script:192] - Pre-training finished/interrupted. Absolute epochs completed: 80.
