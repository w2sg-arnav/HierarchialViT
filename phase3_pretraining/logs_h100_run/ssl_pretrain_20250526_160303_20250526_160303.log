2025-05-26 16:03:03 - root - INFO - [setup_logging:57] - Logging configured. File: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/logs_h100_run/ssl_pretrain_20250526_160303_20250526_160303.log (Level: DEBUG), Console Level: INFO
2025-05-26 16:03:04 - phase2_model.models - INFO - [<module>:17] - Models (InceptionV3Baseline, DFCA, DiseaseAwareHVT, factory) imported successfully into phase2_model.models package.
2025-05-26 16:03:04 - phase2_model - INFO - [<module>:25] - Models re-exported successfully by phase2_model/__init__.py.
2025-05-26 16:03:04 - __main__ - INFO - [main_pretrain_script:60] - ======== Starting Phase 3: HVT Self-Supervised Pre-training (Run ID: 20250526_160303) ========
2025-05-26 16:03:04 - __main__ - INFO - [main_pretrain_script:61] - Full run configuration snapshot: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining', 'log_dir_name': 'logs_h100_run', 'log_file_pretrain': 'ssl_pretrain.log', 'checkpoint_dir_name': 'checkpoints_hvt_xl_h100_run', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'resume_checkpoint_path': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_epoch_20.pth', 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': 'Augmented Dataset', 'train_split_ratio': 0.95, 'num_classes': 7, 'num_workers': 8, 'prefetch_factor': 2, 'hvt_params_for_backbone': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'pretrain_img_size': (448, 448), 'pretrain_epochs': 80, 'pretrain_batch_size': 32, 'accumulation_steps': 2, 'pretrain_lr': 0.0005, 'pretrain_optimizer': 'AdamW', 'pretrain_scheduler': 'WarmupCosine', 'warmup_epochs': 10, 'eta_min_lr': 1e-06, 'pretrain_weight_decay': 0.05, 'pretrain_momentum': 0.9, 'temperature': 0.1, 'projection_dim': 256, 'projection_hidden_dim': 4096, 'simclr_s': 1.0, 'simclr_p_grayscale': 0.2, 'simclr_p_gaussian_blur': 0.5, 'simclr_rrc_scale_min': 0.08, 'evaluate_every_n_epochs': 10, 'linear_probe_epochs': 10, 'linear_probe_lr': 0.1, 'probe_optimizer': 'SGD', 'probe_momentum': 0.9, 'probe_weight_decay': 0.0, 'probe_batch_size': 64, 'save_every_n_epochs': 10, 'model_arch_name_for_ckpt': 'hvt_xl_simclr_h100_run', 'clip_grad_norm': 1.0}
2025-05-26 16:03:04 - __main__ - INFO - [apply_pytorch_optimizations:51] - torch.backends.cudnn.benchmark = True
2025-05-26 16:03:04 - __main__ - INFO - [apply_pytorch_optimizations:54] - torch.set_float32_matmul_precision('high')
2025-05-26 16:03:04 - __main__ - INFO - [main_pretrain_script:66] - Global random seed: 42. Device: cuda.
2025-05-26 16:03:04 - __main__ - INFO - [main_pretrain_script:67] - CUDA Device: Tesla T4
2025-05-26 16:03:04 - __main__ - INFO - [main_pretrain_script:70] - Target image size for pre-training: (448, 448)
2025-05-26 16:03:04 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-26 16:03:04 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-26 16:03:04 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-26 16:03:04 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-26 16:03:04 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-26 16:03:04 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-26 16:03:04 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-26 16:03:04 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-26 16:03:05 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-26 16:03:05 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-26 16:03:05 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='val', img_size=(448, 448), use_spectral=False
2025-05-26 16:03:05 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-26 16:03:05 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-26 16:03:05 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-26 16:03:05 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'val' size: 457 samples.
2025-05-26 16:03:05 - __main__ - INFO - [main_pretrain_script:85] - Dataset sizes: Pretrain=8680, ProbeTrain=8680, ProbeVal=457
2025-05-26 16:03:05 - __main__ - INFO - [main_pretrain_script:96] - Pretrain DataLoader: 271 batches, BS=32, Workers=8
2025-05-26 16:03:05 - __main__ - INFO - [main_pretrain_script:98] - Initializing HVTForPretraining model wrapper...
2025-05-26 16:03:05 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:36] - Initializing HVTForPretraining wrapper for img_size: (448, 448)
2025-05-26 16:03:05 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:44] - Instantiating HVTBackbone using parameters defined in Phase 3 config (hvt_params_for_backbone).
2025-05-26 16:03:05 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-26 16:03:08 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-26 16:03:11 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-26 16:03:11 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:70] - Projection head input dimension set to: 1536
2025-05-26 16:03:11 - phase3_pretraining.models.projection_head - INFO - [__init__:29] - ProjectionHead initialized: In=1536, Hidden=4096, Out=256, BatchNorm=True
2025-05-26 16:03:11 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:79] - HVTForPretraining wrapper initialized successfully.
2025-05-26 16:03:11 - phase3_pretraining.utils.augmentations - INFO - [__init__:27] - SimCLRAugmentation: img_size=(448, 448), s=1.0, p_gray=0.2, p_blur=0.5, rrc_min_scale=0.08
2025-05-26 16:03:11 - phase3_pretraining.utils.losses - INFO - [__init__:13] - InfoNCELoss initialized with temperature: 0.1
2025-05-26 16:03:12 - phase3_pretraining.pretrain.trainer - INFO - [__init__:41] - Initializing Pretrainer...
2025-05-26 16:03:12 - phase3_pretraining.pretrain.trainer - INFO - [__init__:58] - Optimizer: AdamW, BaseLR: 0.0005, WD: 0.05
2025-05-26 16:03:12 - phase3_pretraining.pretrain.trainer - INFO - [__init__:69] - Pretrainer init complete. AMP: True, Accum: 2, ClipGrad: 1.0
2025-05-26 16:03:12 - __main__ - INFO - [main_pretrain_script:114] - Attempting to resume training from checkpoint: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_epoch_20.pth
2025-05-26 16:03:16 - __main__ - INFO - [main_pretrain_script:122] - Loaded backbone and projection head state dicts from checkpoint.
2025-05-26 16:03:16 - __main__ - INFO - [main_pretrain_script:130] - Optimizer state loaded.
2025-05-26 16:03:16 - __main__ - INFO - [main_pretrain_script:140] - GradScaler state loaded.
2025-05-26 16:03:16 - __main__ - INFO - [main_pretrain_script:148] - Resuming training from epoch 21. Best probe accuracy so far: 25.16%
2025-05-26 16:03:16 - __main__ - INFO - [main_pretrain_script:155] - Deferring scheduler state load: Pretrainer will initialize it with correct last_epoch.
2025-05-26 16:03:16 - __main__ - INFO - [main_pretrain_script:178] - Starting/Resuming SimCLR pre-training from epoch 21 up to 80 epochs.
2025-05-26 16:03:16 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:86] - Resuming: Scheduler will be initialized with last_step/epoch around: 5419
2025-05-26 16:03:16 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:106] - Scheduler: WarmupCosine (WarmupSteps=2710, TotalTrainSteps=21680). Last epoch for init: 5420
2025-05-26 16:10:32 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E21: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:10:32 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 21 Training Summary: AvgLoss=2.2436, OptSteps=136, FinalLR=4.73e-04
2025-05-26 16:10:32 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 21/80 | Duration: 436.78s | Samples/sec: 19.85 | Avg Loss: 2.2436 | LR: 4.73e-04
2025-05-26 16:10:33 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E21: Alloc 4325.1MB, MaxAlloc 9433.7MB
2025-05-26 16:17:57 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E22: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:17:58 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 22 Training Summary: AvgLoss=2.0935, OptSteps=136, FinalLR=4.70e-04
2025-05-26 16:17:58 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 22/80 | Duration: 445.04s | Samples/sec: 19.49 | Avg Loss: 2.0935 | LR: 4.70e-04
2025-05-26 16:17:58 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E22: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 16:25:19 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E23: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:25:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 23 Training Summary: AvgLoss=2.1128, OptSteps=136, FinalLR=4.67e-04
2025-05-26 16:25:20 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 23/80 | Duration: 442.22s | Samples/sec: 19.61 | Avg Loss: 2.1128 | LR: 4.67e-04
2025-05-26 16:25:20 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E23: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 16:32:42 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E24: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:32:43 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 24 Training Summary: AvgLoss=2.0178, OptSteps=136, FinalLR=4.65e-04
2025-05-26 16:32:43 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 24/80 | Duration: 443.12s | Samples/sec: 19.57 | Avg Loss: 2.0178 | LR: 4.65e-04
2025-05-26 16:32:43 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E24: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 16:40:03 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E25: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:40:03 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 25 Training Summary: AvgLoss=1.9023, OptSteps=136, FinalLR=4.62e-04
2025-05-26 16:40:03 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 25/80 | Duration: 440.21s | Samples/sec: 19.70 | Avg Loss: 1.9023 | LR: 4.62e-04
2025-05-26 16:40:03 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E25: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 16:47:22 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E26: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:47:23 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 26 Training Summary: AvgLoss=1.9417, OptSteps=136, FinalLR=4.59e-04
2025-05-26 16:47:23 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 26/80 | Duration: 439.66s | Samples/sec: 19.72 | Avg Loss: 1.9417 | LR: 4.59e-04
2025-05-26 16:47:23 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E26: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 16:54:43 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E27: Applying leftover gradients from accumulation (1 steps).
2025-05-26 16:54:44 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 27 Training Summary: AvgLoss=1.9321, OptSteps=136, FinalLR=4.55e-04
2025-05-26 16:54:44 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 27/80 | Duration: 440.84s | Samples/sec: 19.67 | Avg Loss: 1.9321 | LR: 4.55e-04
2025-05-26 16:54:44 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E27: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 17:02:05 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E28: Applying leftover gradients from accumulation (1 steps).
2025-05-26 17:02:06 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 28 Training Summary: AvgLoss=1.9371, OptSteps=136, FinalLR=4.52e-04
2025-05-26 17:02:06 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 28/80 | Duration: 442.08s | Samples/sec: 19.62 | Avg Loss: 1.9371 | LR: 4.52e-04
2025-05-26 17:02:06 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E28: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 17:09:28 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E29: Applying leftover gradients from accumulation (1 steps).
2025-05-26 17:09:28 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 29 Training Summary: AvgLoss=1.8709, OptSteps=136, FinalLR=4.49e-04
2025-05-26 17:09:28 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 29/80 | Duration: 442.79s | Samples/sec: 19.58 | Avg Loss: 1.8709 | LR: 4.49e-04
2025-05-26 17:09:28 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E29: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 17:16:49 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E30: Applying leftover gradients from accumulation (1 steps).
2025-05-26 17:16:50 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 30 Training Summary: AvgLoss=1.7841, OptSteps=136, FinalLR=4.45e-04
2025-05-26 17:16:50 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 30/80 | Duration: 441.31s | Samples/sec: 19.65 | Avg Loss: 1.7841 | LR: 4.45e-04
2025-05-26 17:16:50 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:228] - --- Starting Linear Probe (after SSL Epoch 30) ---
2025-05-26 17:16:50 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:243] - Probe E30: Feature dim for linear classifier: 1536
2025-05-26 17:38:10 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:280] - Linear Probe (SSL E30) Validation Accuracy: 17.29% (79/457)
2025-05-26 17:38:10 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_30.pth (reflecting completion of epoch 30)...
2025-05-26 17:38:17 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 30 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_30.pth
2025-05-26 17:38:17 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E30: Alloc 4325.1MB, MaxAlloc 9435.2MB
2025-05-26 17:45:39 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E31: Applying leftover gradients from accumulation (1 steps).
2025-05-26 17:45:39 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 31 Training Summary: AvgLoss=1.7881, OptSteps=136, FinalLR=4.42e-04
2025-05-26 17:45:39 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 31/80 | Duration: 442.14s | Samples/sec: 19.61 | Avg Loss: 1.7881 | LR: 4.42e-04
2025-05-26 17:45:39 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E31: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 17:53:03 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E32: Applying leftover gradients from accumulation (1 steps).
2025-05-26 17:53:04 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 32 Training Summary: AvgLoss=1.7345, OptSteps=136, FinalLR=4.38e-04
2025-05-26 17:53:04 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 32/80 | Duration: 444.55s | Samples/sec: 19.51 | Avg Loss: 1.7345 | LR: 4.38e-04
2025-05-26 17:53:04 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E32: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:00:25 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E33: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:00:25 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 33 Training Summary: AvgLoss=1.6961, OptSteps=136, FinalLR=4.34e-04
2025-05-26 18:00:25 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 33/80 | Duration: 441.60s | Samples/sec: 19.64 | Avg Loss: 1.6961 | LR: 4.34e-04
2025-05-26 18:00:25 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E33: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:07:46 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E34: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:07:46 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 34 Training Summary: AvgLoss=1.6923, OptSteps=136, FinalLR=4.30e-04
2025-05-26 18:07:46 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 34/80 | Duration: 440.76s | Samples/sec: 19.68 | Avg Loss: 1.6923 | LR: 4.30e-04
2025-05-26 18:07:46 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E34: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:15:07 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E35: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:15:07 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 35 Training Summary: AvgLoss=1.6518, OptSteps=136, FinalLR=4.27e-04
2025-05-26 18:15:07 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 35/80 | Duration: 440.96s | Samples/sec: 19.67 | Avg Loss: 1.6518 | LR: 4.27e-04
2025-05-26 18:15:07 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E35: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:22:29 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E36: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:22:30 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 36 Training Summary: AvgLoss=1.6423, OptSteps=136, FinalLR=4.23e-04
2025-05-26 18:22:30 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 36/80 | Duration: 442.49s | Samples/sec: 19.60 | Avg Loss: 1.6423 | LR: 4.23e-04
2025-05-26 18:22:30 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E36: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:29:50 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E37: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:29:51 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 37 Training Summary: AvgLoss=1.6140, OptSteps=136, FinalLR=4.18e-04
2025-05-26 18:29:51 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 37/80 | Duration: 441.22s | Samples/sec: 19.65 | Avg Loss: 1.6140 | LR: 4.18e-04
2025-05-26 18:29:51 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E37: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:37:14 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E38: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:37:15 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 38 Training Summary: AvgLoss=1.4890, OptSteps=136, FinalLR=4.14e-04
2025-05-26 18:37:15 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 38/80 | Duration: 443.76s | Samples/sec: 19.54 | Avg Loss: 1.4890 | LR: 4.14e-04
2025-05-26 18:37:15 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E38: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:44:35 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E39: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:44:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 39 Training Summary: AvgLoss=1.5696, OptSteps=136, FinalLR=4.10e-04
2025-05-26 18:44:35 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 39/80 | Duration: 440.33s | Samples/sec: 19.69 | Avg Loss: 1.5696 | LR: 4.10e-04
2025-05-26 18:44:35 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E39: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 18:51:58 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E40: Applying leftover gradients from accumulation (1 steps).
2025-05-26 18:51:58 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 40 Training Summary: AvgLoss=1.5403, OptSteps=136, FinalLR=4.06e-04
2025-05-26 18:51:58 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 40/80 | Duration: 443.11s | Samples/sec: 19.57 | Avg Loss: 1.5403 | LR: 4.06e-04
2025-05-26 18:51:58 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:228] - --- Starting Linear Probe (after SSL Epoch 40) ---
2025-05-26 18:51:58 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:243] - Probe E40: Feature dim for linear classifier: 1536
2025-05-26 19:13:10 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:280] - Linear Probe (SSL E40) Validation Accuracy: 41.14% (188/457)
2025-05-26 19:13:10 - __main__ - INFO - [main_pretrain_script:203] - New best probe: 41.14% (SSL E40). Saving best model.
2025-05-26 19:13:10 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_best_probe.pth (reflecting completion of epoch 40)...
2025-05-26 19:13:17 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 40 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_best_probe.pth
2025-05-26 19:13:17 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_40.pth (reflecting completion of epoch 40)...
2025-05-26 19:13:25 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 40 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_40.pth
2025-05-26 19:13:25 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E40: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 19:20:47 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E41: Applying leftover gradients from accumulation (1 steps).
2025-05-26 19:20:48 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 41 Training Summary: AvgLoss=1.5373, OptSteps=136, FinalLR=4.01e-04
2025-05-26 19:20:48 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 41/80 | Duration: 442.60s | Samples/sec: 19.59 | Avg Loss: 1.5373 | LR: 4.01e-04
2025-05-26 19:20:48 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E41: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 19:28:09 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E42: Applying leftover gradients from accumulation (1 steps).
2025-05-26 19:28:09 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 42 Training Summary: AvgLoss=1.5120, OptSteps=136, FinalLR=3.97e-04
2025-05-26 19:28:09 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 42/80 | Duration: 441.79s | Samples/sec: 19.63 | Avg Loss: 1.5120 | LR: 3.97e-04
2025-05-26 19:28:09 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E42: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 19:35:31 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E43: Applying leftover gradients from accumulation (1 steps).
2025-05-26 19:35:31 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 43 Training Summary: AvgLoss=1.4688, OptSteps=136, FinalLR=3.92e-04
2025-05-26 19:35:31 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 43/80 | Duration: 441.80s | Samples/sec: 19.63 | Avg Loss: 1.4688 | LR: 3.92e-04
2025-05-26 19:35:31 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E43: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 19:42:52 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E44: Applying leftover gradients from accumulation (1 steps).
2025-05-26 19:42:53 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 44 Training Summary: AvgLoss=1.4125, OptSteps=136, FinalLR=3.87e-04
2025-05-26 19:42:53 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 44/80 | Duration: 441.41s | Samples/sec: 19.65 | Avg Loss: 1.4125 | LR: 3.87e-04
2025-05-26 19:42:53 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E44: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 19:50:13 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E45: Applying leftover gradients from accumulation (1 steps).
2025-05-26 19:50:14 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 45 Training Summary: AvgLoss=1.3428, OptSteps=136, FinalLR=3.83e-04
2025-05-26 19:50:14 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 45/80 | Duration: 441.13s | Samples/sec: 19.66 | Avg Loss: 1.3428 | LR: 3.83e-04
2025-05-26 19:50:14 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E45: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 19:57:37 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E46: Applying leftover gradients from accumulation (1 steps).
2025-05-26 19:57:38 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 46 Training Summary: AvgLoss=1.3009, OptSteps=136, FinalLR=3.78e-04
2025-05-26 19:57:38 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 46/80 | Duration: 443.80s | Samples/sec: 19.54 | Avg Loss: 1.3009 | LR: 3.78e-04
2025-05-26 19:57:38 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E46: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 20:04:57 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E47: Applying leftover gradients from accumulation (1 steps).
2025-05-26 20:04:58 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 47 Training Summary: AvgLoss=1.2926, OptSteps=136, FinalLR=3.73e-04
2025-05-26 20:04:58 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 47/80 | Duration: 440.06s | Samples/sec: 19.71 | Avg Loss: 1.2926 | LR: 3.73e-04
2025-05-26 20:04:58 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E47: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 20:12:19 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E48: Applying leftover gradients from accumulation (1 steps).
2025-05-26 20:12:19 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 48 Training Summary: AvgLoss=1.2904, OptSteps=136, FinalLR=3.68e-04
2025-05-26 20:12:19 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 48/80 | Duration: 441.57s | Samples/sec: 19.64 | Avg Loss: 1.2904 | LR: 3.68e-04
2025-05-26 20:12:19 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E48: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 20:19:41 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E49: Applying leftover gradients from accumulation (1 steps).
2025-05-26 20:19:41 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 49 Training Summary: AvgLoss=1.2922, OptSteps=136, FinalLR=3.63e-04
2025-05-26 20:19:41 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 49/80 | Duration: 442.07s | Samples/sec: 19.62 | Avg Loss: 1.2922 | LR: 3.63e-04
2025-05-26 20:19:41 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E49: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 20:27:04 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E50: Applying leftover gradients from accumulation (1 steps).
2025-05-26 20:27:05 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 50 Training Summary: AvgLoss=1.2830, OptSteps=136, FinalLR=3.58e-04
2025-05-26 20:27:05 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 50/80 | Duration: 443.39s | Samples/sec: 19.56 | Avg Loss: 1.2830 | LR: 3.58e-04
2025-05-26 20:27:05 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:228] - --- Starting Linear Probe (after SSL Epoch 50) ---
2025-05-26 20:27:05 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:243] - Probe E50: Feature dim for linear classifier: 1536
2025-05-26 20:48:30 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:280] - Linear Probe (SSL E50) Validation Accuracy: 46.83% (214/457)
2025-05-26 20:48:30 - __main__ - INFO - [main_pretrain_script:203] - New best probe: 46.83% (SSL E50). Saving best model.
2025-05-26 20:48:30 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_best_probe.pth (reflecting completion of epoch 50)...
2025-05-26 20:48:38 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 50 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_best_probe.pth
2025-05-26 20:48:38 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_50.pth (reflecting completion of epoch 50)...
2025-05-26 20:48:49 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 50 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_50.pth
2025-05-26 20:48:49 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E50: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 20:56:14 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E51: Applying leftover gradients from accumulation (1 steps).
2025-05-26 20:56:15 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 51 Training Summary: AvgLoss=1.2298, OptSteps=136, FinalLR=3.53e-04
2025-05-26 20:56:15 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 51/80 | Duration: 445.20s | Samples/sec: 19.48 | Avg Loss: 1.2298 | LR: 3.53e-04
2025-05-26 20:56:15 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E51: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:03:34 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E52: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:03:34 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 52 Training Summary: AvgLoss=1.1828, OptSteps=136, FinalLR=3.48e-04
2025-05-26 21:03:34 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 52/80 | Duration: 439.88s | Samples/sec: 19.71 | Avg Loss: 1.1828 | LR: 3.48e-04
2025-05-26 21:03:34 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E52: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:11:00 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E53: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:11:01 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 53 Training Summary: AvgLoss=1.2010, OptSteps=136, FinalLR=3.42e-04
2025-05-26 21:11:01 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 53/80 | Duration: 446.24s | Samples/sec: 19.43 | Avg Loss: 1.2010 | LR: 3.42e-04
2025-05-26 21:11:01 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E53: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:18:21 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E54: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:18:21 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 54 Training Summary: AvgLoss=1.1514, OptSteps=136, FinalLR=3.37e-04
2025-05-26 21:18:21 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 54/80 | Duration: 440.67s | Samples/sec: 19.68 | Avg Loss: 1.1514 | LR: 3.37e-04
2025-05-26 21:18:21 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E54: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:25:43 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E55: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:25:43 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 55 Training Summary: AvgLoss=1.1222, OptSteps=136, FinalLR=3.32e-04
2025-05-26 21:25:43 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 55/80 | Duration: 442.14s | Samples/sec: 19.61 | Avg Loss: 1.1222 | LR: 3.32e-04
2025-05-26 21:25:43 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E55: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:33:07 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E56: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:33:07 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 56 Training Summary: AvgLoss=1.0615, OptSteps=136, FinalLR=3.27e-04
2025-05-26 21:33:07 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 56/80 | Duration: 443.72s | Samples/sec: 19.54 | Avg Loss: 1.0615 | LR: 3.27e-04
2025-05-26 21:33:07 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E56: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:40:30 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E57: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:40:30 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 57 Training Summary: AvgLoss=1.0836, OptSteps=136, FinalLR=3.21e-04
2025-05-26 21:40:30 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 57/80 | Duration: 443.07s | Samples/sec: 19.57 | Avg Loss: 1.0836 | LR: 3.21e-04
2025-05-26 21:40:30 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E57: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:47:51 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E58: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:47:52 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 58 Training Summary: AvgLoss=1.0772, OptSteps=136, FinalLR=3.16e-04
2025-05-26 21:47:52 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 58/80 | Duration: 441.41s | Samples/sec: 19.65 | Avg Loss: 1.0772 | LR: 3.16e-04
2025-05-26 21:47:52 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E58: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 21:55:15 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E59: Applying leftover gradients from accumulation (1 steps).
2025-05-26 21:55:16 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 59 Training Summary: AvgLoss=1.0761, OptSteps=136, FinalLR=3.10e-04
2025-05-26 21:55:16 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 59/80 | Duration: 444.20s | Samples/sec: 19.52 | Avg Loss: 1.0761 | LR: 3.10e-04
2025-05-26 21:55:16 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E59: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 22:02:39 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E60: Applying leftover gradients from accumulation (1 steps).
2025-05-26 22:02:39 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 60 Training Summary: AvgLoss=1.0165, OptSteps=136, FinalLR=3.05e-04
2025-05-26 22:02:39 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 60/80 | Duration: 443.43s | Samples/sec: 19.56 | Avg Loss: 1.0165 | LR: 3.05e-04
2025-05-26 22:02:39 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:228] - --- Starting Linear Probe (after SSL Epoch 60) ---
2025-05-26 22:02:39 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:243] - Probe E60: Feature dim for linear classifier: 1536
2025-05-26 22:24:03 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:280] - Linear Probe (SSL E60) Validation Accuracy: 44.20% (202/457)
2025-05-26 22:24:03 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_60.pth (reflecting completion of epoch 60)...
2025-05-26 22:24:10 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 60 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_60.pth
2025-05-26 22:24:10 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E60: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 22:31:33 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E61: Applying leftover gradients from accumulation (1 steps).
2025-05-26 22:31:33 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 61 Training Summary: AvgLoss=1.0545, OptSteps=136, FinalLR=2.99e-04
2025-05-26 22:31:33 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 61/80 | Duration: 443.58s | Samples/sec: 19.55 | Avg Loss: 1.0545 | LR: 2.99e-04
2025-05-26 22:31:33 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E61: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 22:38:56 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E62: Applying leftover gradients from accumulation (1 steps).
2025-05-26 22:38:56 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 62 Training Summary: AvgLoss=0.9800, OptSteps=136, FinalLR=2.94e-04
2025-05-26 22:38:56 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 62/80 | Duration: 442.99s | Samples/sec: 19.58 | Avg Loss: 0.9800 | LR: 2.94e-04
2025-05-26 22:38:56 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E62: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 22:46:16 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E63: Applying leftover gradients from accumulation (1 steps).
2025-05-26 22:46:16 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 63 Training Summary: AvgLoss=0.9771, OptSteps=136, FinalLR=2.88e-04
2025-05-26 22:46:16 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 63/80 | Duration: 439.69s | Samples/sec: 19.72 | Avg Loss: 0.9771 | LR: 2.88e-04
2025-05-26 22:46:16 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E63: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 22:53:35 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E64: Applying leftover gradients from accumulation (1 steps).
2025-05-26 22:53:36 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 64 Training Summary: AvgLoss=0.9716, OptSteps=136, FinalLR=2.83e-04
2025-05-26 22:53:36 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 64/80 | Duration: 439.91s | Samples/sec: 19.71 | Avg Loss: 0.9716 | LR: 2.83e-04
2025-05-26 22:53:36 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E64: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 23:00:56 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E65: Applying leftover gradients from accumulation (1 steps).
2025-05-26 23:00:56 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 65 Training Summary: AvgLoss=0.9539, OptSteps=136, FinalLR=2.77e-04
2025-05-26 23:00:56 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 65/80 | Duration: 440.43s | Samples/sec: 19.69 | Avg Loss: 0.9539 | LR: 2.77e-04
2025-05-26 23:00:56 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E65: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 23:08:20 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E66: Applying leftover gradients from accumulation (1 steps).
2025-05-26 23:08:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 66 Training Summary: AvgLoss=0.9472, OptSteps=136, FinalLR=2.71e-04
2025-05-26 23:08:20 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 66/80 | Duration: 443.69s | Samples/sec: 19.55 | Avg Loss: 0.9472 | LR: 2.71e-04
2025-05-26 23:08:20 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E66: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 23:15:44 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E67: Applying leftover gradients from accumulation (1 steps).
2025-05-26 23:15:45 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 67 Training Summary: AvgLoss=0.9364, OptSteps=136, FinalLR=2.66e-04
2025-05-26 23:15:45 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 67/80 | Duration: 444.73s | Samples/sec: 19.50 | Avg Loss: 0.9364 | LR: 2.66e-04
2025-05-26 23:15:45 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E67: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 23:23:05 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E68: Applying leftover gradients from accumulation (1 steps).
2025-05-26 23:23:05 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 68 Training Summary: AvgLoss=0.8823, OptSteps=136, FinalLR=2.60e-04
2025-05-26 23:23:05 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 68/80 | Duration: 440.19s | Samples/sec: 19.70 | Avg Loss: 0.8823 | LR: 2.60e-04
2025-05-26 23:23:05 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E68: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 23:30:24 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E69: Applying leftover gradients from accumulation (1 steps).
2025-05-26 23:30:25 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 69 Training Summary: AvgLoss=0.8874, OptSteps=136, FinalLR=2.55e-04
2025-05-26 23:30:25 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 69/80 | Duration: 439.81s | Samples/sec: 19.72 | Avg Loss: 0.8874 | LR: 2.55e-04
2025-05-26 23:30:25 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E69: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-26 23:37:47 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E70: Applying leftover gradients from accumulation (1 steps).
2025-05-26 23:37:48 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 70 Training Summary: AvgLoss=0.8082, OptSteps=136, FinalLR=2.49e-04
2025-05-26 23:37:48 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 70/80 | Duration: 443.08s | Samples/sec: 19.57 | Avg Loss: 0.8082 | LR: 2.49e-04
2025-05-26 23:37:48 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:228] - --- Starting Linear Probe (after SSL Epoch 70) ---
2025-05-26 23:37:48 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:243] - Probe E70: Feature dim for linear classifier: 1536
2025-05-26 23:59:14 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:280] - Linear Probe (SSL E70) Validation Accuracy: 39.39% (180/457)
2025-05-26 23:59:14 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_70.pth (reflecting completion of epoch 70)...
2025-05-26 23:59:20 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 70 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_70.pth
2025-05-26 23:59:20 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E70: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:06:43 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E71: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:06:43 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 71 Training Summary: AvgLoss=0.8429, OptSteps=136, FinalLR=2.43e-04
2025-05-27 00:06:43 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 71/80 | Duration: 443.50s | Samples/sec: 19.55 | Avg Loss: 0.8429 | LR: 2.43e-04
2025-05-27 00:06:43 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E71: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:14:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E72: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:14:08 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 72 Training Summary: AvgLoss=0.8152, OptSteps=136, FinalLR=2.38e-04
2025-05-27 00:14:08 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 72/80 | Duration: 444.86s | Samples/sec: 19.49 | Avg Loss: 0.8152 | LR: 2.38e-04
2025-05-27 00:14:08 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E72: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:21:27 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E73: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:21:28 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 73 Training Summary: AvgLoss=0.8685, OptSteps=136, FinalLR=2.32e-04
2025-05-27 00:21:28 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 73/80 | Duration: 439.82s | Samples/sec: 19.72 | Avg Loss: 0.8685 | LR: 2.32e-04
2025-05-27 00:21:28 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E73: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:28:49 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E74: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:28:49 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 74 Training Summary: AvgLoss=0.8255, OptSteps=136, FinalLR=2.26e-04
2025-05-27 00:28:49 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 74/80 | Duration: 441.23s | Samples/sec: 19.65 | Avg Loss: 0.8255 | LR: 2.26e-04
2025-05-27 00:28:49 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E74: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:36:08 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E75: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:36:08 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 75 Training Summary: AvgLoss=0.8179, OptSteps=136, FinalLR=2.21e-04
2025-05-27 00:36:08 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 75/80 | Duration: 439.48s | Samples/sec: 19.73 | Avg Loss: 0.8179 | LR: 2.21e-04
2025-05-27 00:36:08 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E75: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:43:30 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E76: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:43:30 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 76 Training Summary: AvgLoss=0.8580, OptSteps=136, FinalLR=2.15e-04
2025-05-27 00:43:30 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 76/80 | Duration: 441.93s | Samples/sec: 19.62 | Avg Loss: 0.8580 | LR: 2.15e-04
2025-05-27 00:43:30 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E76: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:50:51 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E77: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:50:52 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 77 Training Summary: AvgLoss=0.8310, OptSteps=136, FinalLR=2.10e-04
2025-05-27 00:50:52 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 77/80 | Duration: 441.24s | Samples/sec: 19.65 | Avg Loss: 0.8310 | LR: 2.10e-04
2025-05-27 00:50:52 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E77: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 00:58:14 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E78: Applying leftover gradients from accumulation (1 steps).
2025-05-27 00:58:14 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 78 Training Summary: AvgLoss=0.7759, OptSteps=136, FinalLR=2.04e-04
2025-05-27 00:58:14 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 78/80 | Duration: 442.83s | Samples/sec: 19.58 | Avg Loss: 0.7759 | LR: 2.04e-04
2025-05-27 00:58:14 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E78: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 01:05:35 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E79: Applying leftover gradients from accumulation (1 steps).
2025-05-27 01:05:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 79 Training Summary: AvgLoss=0.7419, OptSteps=136, FinalLR=1.99e-04
2025-05-27 01:05:35 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 79/80 | Duration: 440.57s | Samples/sec: 19.68 | Avg Loss: 0.7419 | LR: 1.99e-04
2025-05-27 01:05:35 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E79: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 01:12:57 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:168] - E80: Applying leftover gradients from accumulation (1 steps).
2025-05-27 01:12:57 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:188] - Epoch 80 Training Summary: AvgLoss=0.8312, OptSteps=136, FinalLR=1.93e-04
2025-05-27 01:12:57 - __main__ - INFO - [main_pretrain_script:196] - SSL Epoch 80/80 | Duration: 442.18s | Samples/sec: 19.61 | Avg Loss: 0.8312 | LR: 1.93e-04
2025-05-27 01:12:57 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:228] - --- Starting Linear Probe (after SSL Epoch 80) ---
2025-05-27 01:12:57 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:243] - Probe E80: Feature dim for linear classifier: 1536
2025-05-27 01:34:22 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:280] - Linear Probe (SSL E80) Validation Accuracy: 36.98% (169/457)
2025-05-27 01:34:22 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_80.pth (reflecting completion of epoch 80)...
2025-05-27 01:34:28 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 80 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_epoch_80.pth
2025-05-27 01:34:28 - __main__ - DEBUG - [main_pretrain_script:212] - CUDA Mem E80: Alloc 4325.1MB, MaxAlloc 9440.7MB
2025-05-27 01:34:28 - __main__ - INFO - [main_pretrain_script:218] - Pre-training finished/interrupted. Total epochs completed considering resume: 80.
2025-05-27 01:34:28 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:202] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_final_epoch80.pth (reflecting completion of epoch 80)...
2025-05-27 01:34:36 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:216] - Checkpoint for epoch 80 saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/checkpoints_hvt_xl_h100_run/hvt_xl_simclr_h100_run_final_epoch80.pth
2025-05-27 01:34:36 - __main__ - INFO - [main_pretrain_script:223] - Final model checkpoint saved. Best probe accuracy: 46.83%
