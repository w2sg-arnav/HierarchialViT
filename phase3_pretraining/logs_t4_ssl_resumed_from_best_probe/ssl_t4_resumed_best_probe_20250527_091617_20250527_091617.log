2025-05-27 09:16:17 - root - INFO - [setup_logging:57] - Logging configured. File: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/logs_t4_ssl_resumed_from_best_probe/ssl_t4_resumed_best_probe_20250527_091617_20250527_091617.log (Level: DEBUG), Console Level: INFO
2025-05-27 09:16:19 - phase2_model.models - INFO - [<module>:17] - Models (InceptionV3Baseline, DFCA, DiseaseAwareHVT, factory) imported successfully into phase2_model.models package.
2025-05-27 09:16:19 - phase2_model - INFO - [<module>:25] - Models re-exported successfully by phase2_model/__init__.py.
2025-05-27 09:16:19 - __main__ - INFO - [main_pretrain_script:58] - ======== Starting Phase 3: HVT Self-Supervised Pre-training (Run ID: 20250527_091617) ========
2025-05-27 09:16:19 - __main__ - INFO - [main_pretrain_script:59] - Full run configuration snapshot: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining', 'log_dir_name': 'logs_t4_ssl_resumed_from_best_probe', 'log_file_pretrain': 'ssl_t4_resumed_best_probe.log', 'checkpoint_dir_name': 'pretrain_checkpoints_hvt_xl', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'resume_checkpoint_path': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth', 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': 'Augmented Dataset', 'train_split_ratio': 0.95, 'num_classes': 7, 'num_workers': 4, 'prefetch_factor': 2, 'hvt_params_for_backbone': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'pretrain_img_size': (448, 448), 'pretrain_epochs': 80, 'pretrain_batch_size': 32, 'accumulation_steps': 2, 'pretrain_lr': 0.0005, 'pretrain_optimizer': 'AdamW', 'pretrain_scheduler': 'WarmupCosine', 'warmup_epochs': 10, 'eta_min_lr': 1e-06, 'pretrain_weight_decay': 0.05, 'temperature': 0.1, 'projection_dim': 256, 'projection_hidden_dim': 4096, 'simclr_s': 1.0, 'simclr_p_grayscale': 0.2, 'simclr_p_gaussian_blur': 0.5, 'simclr_rrc_scale_min': 0.08, 'evaluate_every_n_epochs': 10, 'linear_probe_epochs': 10, 'linear_probe_lr': 0.1, 'probe_optimizer': 'SGD', 'probe_momentum': 0.9, 'probe_weight_decay': 0.0, 'probe_batch_size': 64, 'save_every_n_epochs': 20, 'model_arch_name_for_ckpt': 'hvt_xl_simclr_t4_resumed', 'clip_grad_norm': 1.0}
2025-05-27 09:16:19 - __main__ - INFO - [apply_pytorch_optimizations:49] - torch.backends.cudnn.benchmark = True
2025-05-27 09:16:19 - __main__ - INFO - [apply_pytorch_optimizations:52] - torch.set_float32_matmul_precision('high')
2025-05-27 09:16:19 - __main__ - INFO - [main_pretrain_script:64] - Global random seed: 42. Device: cuda.
2025-05-27 09:16:19 - __main__ - INFO - [main_pretrain_script:65] - CUDA Device: Tesla T4
2025-05-27 09:16:19 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-27 09:16:19 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 09:16:20 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 09:16:21 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 09:16:21 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-27 09:16:21 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-27 09:16:21 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 09:16:22 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 09:16:23 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 09:16:23 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-27 09:16:23 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='val', img_size=(448, 448), use_spectral=False
2025-05-27 09:16:23 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-27 09:16:23 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-27 09:16:25 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-27 09:16:25 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'val' size: 457 samples.
2025-05-27 09:16:25 - __main__ - INFO - [main_pretrain_script:80] - Dataset sizes: Pretrain=8680, ProbeTrain=8680, ProbeVal=457
2025-05-27 09:16:25 - __main__ - INFO - [main_pretrain_script:91] - Pretrain DataLoader: 271 batches, BS=32, Workers=4
2025-05-27 09:16:25 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:36] - Initializing HVTForPretraining wrapper for img_size: (448, 448)
2025-05-27 09:16:25 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:44] - Instantiating HVTBackbone using parameters defined in Phase 3 config (hvt_params_for_backbone).
2025-05-27 09:16:25 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-27 09:16:29 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-27 09:16:32 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-27 09:16:32 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:70] - Projection head input dimension set to: 1536
2025-05-27 09:16:32 - phase3_pretraining.models.projection_head - INFO - [__init__:29] - ProjectionHead initialized: In=1536, Hidden=4096, Out=256, BatchNorm=True
2025-05-27 09:16:32 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:79] - HVTForPretraining wrapper initialized successfully.
2025-05-27 09:16:33 - phase3_pretraining.utils.augmentations - INFO - [__init__:28] - SimCLRAugmentation: img_size=(448, 448), s=1.0, p_gray=0.2, p_blur=0.5, rrc_min_scale=0.08
2025-05-27 09:16:33 - phase3_pretraining.utils.losses - INFO - [__init__:13] - InfoNCELoss initialized with temperature: 0.1
2025-05-27 09:16:33 - phase3_pretraining.pretrain.trainer - INFO - [__init__:42] - Initializing Pretrainer...
2025-05-27 09:16:33 - phase3_pretraining.pretrain.trainer - INFO - [__init__:59] - Optimizer: AdamW, BaseLR: 0.0005, WD: 0.05
2025-05-27 09:16:33 - phase3_pretraining.pretrain.trainer - INFO - [__init__:70] - Pretrainer init complete. AMP: True, Accum: 2, ClipGrad: 1.0
2025-05-27 09:16:33 - __main__ - INFO - [main_pretrain_script:103] - Attempting to resume training from checkpoint: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth
2025-05-27 09:17:25 - __main__ - INFO - [main_pretrain_script:110] - Loaded backbone and projection head state dicts.
2025-05-27 09:17:25 - __main__ - INFO - [main_pretrain_script:115] - Optimizer state loaded.
2025-05-27 09:17:25 - __main__ - INFO - [main_pretrain_script:117] - GradScaler state loaded.
2025-05-27 09:17:25 - __main__ - INFO - [main_pretrain_script:134] - Scheduler state_dict found. Pretrainer will use 'numerically_last_completed_epoch' for re-initialization.
2025-05-27 09:17:25 - __main__ - INFO - [main_pretrain_script:135] - Resuming from completed SSL epoch 30. Will start training at epoch 31. Best probe: 41.79%
2025-05-27 09:17:25 - __main__ - INFO - [main_pretrain_script:152] - Starting/Resuming SimCLR pre-training from epoch 31 up to 80 total epochs.
2025-05-27 09:17:25 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:85] - Resuming: Scheduler will be initialized with last_step/epoch equivalent to completion of epoch 30 (0-indexed value: 8129).
2025-05-27 09:17:25 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:109] - Scheduler: WarmupCosine (WarmupSteps=2710, TotalTrainSteps=21680). Initialized with last_epoch (step): 8130
2025-05-27 09:24:57 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E31: Applying leftover grads from accum (1 steps).
2025-05-27 09:24:57 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 31 Training Summary: AvgLoss=1.7144, OptSteps=136, FinalLR=4.01e-04
2025-05-27 09:24:57 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 31/80 | Duration: 452.07s | Eff. Samples/sec: 38.37 | Avg Loss: 1.7144 | LR: 4.01e-04
2025-05-27 09:24:57 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E31 END: Alloc 4325.1MB, MaxAlloc 9430.9MB
2025-05-27 09:32:21 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E32: Applying leftover grads from accum (1 steps).
2025-05-27 09:32:21 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 32 Training Summary: AvgLoss=1.6395, OptSteps=136, FinalLR=3.97e-04
2025-05-27 09:32:21 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 32/80 | Duration: 444.09s | Eff. Samples/sec: 39.05 | Avg Loss: 1.6395 | LR: 3.97e-04
2025-05-27 09:32:21 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E32 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 09:39:41 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E33: Applying leftover grads from accum (1 steps).
2025-05-27 09:39:41 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 33 Training Summary: AvgLoss=1.6877, OptSteps=136, FinalLR=3.92e-04
2025-05-27 09:39:41 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 33/80 | Duration: 439.98s | Eff. Samples/sec: 39.42 | Avg Loss: 1.6877 | LR: 3.92e-04
2025-05-27 09:39:41 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E33 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 09:47:03 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E34: Applying leftover grads from accum (1 steps).
2025-05-27 09:47:04 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 34 Training Summary: AvgLoss=1.6358, OptSteps=136, FinalLR=3.88e-04
2025-05-27 09:47:04 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 34/80 | Duration: 442.33s | Eff. Samples/sec: 39.21 | Avg Loss: 1.6358 | LR: 3.88e-04
2025-05-27 09:47:04 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E34 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 09:54:23 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E35: Applying leftover grads from accum (1 steps).
2025-05-27 09:54:24 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 35 Training Summary: AvgLoss=1.5296, OptSteps=136, FinalLR=3.83e-04
2025-05-27 09:54:24 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 35/80 | Duration: 440.23s | Eff. Samples/sec: 39.40 | Avg Loss: 1.5296 | LR: 3.83e-04
2025-05-27 09:54:24 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E35 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 10:01:43 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E36: Applying leftover grads from accum (1 steps).
2025-05-27 10:01:43 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 36 Training Summary: AvgLoss=1.6212, OptSteps=136, FinalLR=3.78e-04
2025-05-27 10:01:43 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 36/80 | Duration: 439.26s | Eff. Samples/sec: 39.48 | Avg Loss: 1.6212 | LR: 3.78e-04
2025-05-27 10:01:43 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E36 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 10:09:03 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E37: Applying leftover grads from accum (1 steps).
2025-05-27 10:09:04 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 37 Training Summary: AvgLoss=1.6015, OptSteps=136, FinalLR=3.73e-04
2025-05-27 10:09:04 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 37/80 | Duration: 440.45s | Eff. Samples/sec: 39.38 | Avg Loss: 1.6015 | LR: 3.73e-04
2025-05-27 10:09:04 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E37 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 10:16:25 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E38: Applying leftover grads from accum (1 steps).
2025-05-27 10:16:25 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 38 Training Summary: AvgLoss=1.6398, OptSteps=136, FinalLR=3.68e-04
2025-05-27 10:16:25 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 38/80 | Duration: 441.56s | Eff. Samples/sec: 39.28 | Avg Loss: 1.6398 | LR: 3.68e-04
2025-05-27 10:16:25 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E38 END: Alloc 4325.1MB, MaxAlloc 9434.6MB
2025-05-27 10:23:47 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E39: Applying leftover grads from accum (1 steps).
2025-05-27 10:23:47 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 39 Training Summary: AvgLoss=1.5491, OptSteps=136, FinalLR=3.63e-04
2025-05-27 10:23:47 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 39/80 | Duration: 441.88s | Eff. Samples/sec: 39.25 | Avg Loss: 1.5491 | LR: 3.63e-04
2025-05-27 10:23:47 - __main__ - DEBUG - [main_pretrain_script:184] - CUDA Mem E39 END: Alloc 4325.1MB, MaxAlloc 9436.6MB
2025-05-27 10:31:07 - phase3_pretraining.pretrain.trainer - WARNING - [train_one_epoch:159] - E40: Applying leftover grads from accum (1 steps).
2025-05-27 10:31:07 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:176] - Epoch 40 Training Summary: AvgLoss=1.4978, OptSteps=136, FinalLR=3.58e-04
2025-05-27 10:31:07 - __main__ - INFO - [main_pretrain_script:168] - SSL Epoch 40/80 | Duration: 440.49s | Eff. Samples/sec: 39.37 | Avg Loss: 1.4978 | LR: 3.58e-04
2025-05-27 10:31:07 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:218] - --- Starting Linear Probe (after SSL Epoch 40) ---
2025-05-27 10:31:07 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:231] - Probe E40: Feature dim for linear classifier: 1536
