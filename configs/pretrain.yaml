# Pre-training configuration for HViT self-supervised learning
# Uses SimCLR-based contrastive learning

data:
  root_dir: "/path/to/sarcld2024"
  img_size: [256, 256]
  num_classes: 7
  num_workers: 4

model:
  patch_size: 16
  embed_dim_rgb: 96
  embed_dim_spectral: 96
  spectral_channels: 0
  depths: [2, 2, 6, 2]
  num_heads: [3, 6, 12, 24]
  mlp_ratio: 4.0
  qkv_bias: true
  model_drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.1
  norm_layer_name: "LayerNorm"
  use_dfca: false
  use_gradient_checkpointing: true
  ssl_enable_mae: false
  ssl_enable_contrastive: true
  enable_consistency_loss_heads: false

training:
  epochs: 100
  batch_size: 64
  learning_rate: 0.0003
  weight_decay: 0.0001
  warmup_epochs: 10

# SimCLR parameters
temperature: 0.07
projection_dim: 128
simclr_s: 1.0
simclr_p_grayscale: 0.2
simclr_p_gaussian_blur: 0.5

# Checkpointing
probe_every: 10
save_every: 25

logging:
  level: "INFO"
  log_dir: "outputs/pretrain/logs"
  reconstruction_metrics:
    - mse
    - psnr
    - ssim

logging:
  tensorboard: true
  wandb:
    enabled: true
    project: "hierarchialvit"
    group: "pretrain_xl"
  save_frequency: 10
  log_frequency: 50

distributed:
  enabled: true
  backend: "nccl"
  world_size: 8
  find_unused_parameters: true
