2025-05-25 09:54:37 - root - INFO - [setup_logging:57] - Logging configured. File: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/logs/phase3_simclr_hvt_xl_20250525_095437.log (Level: DEBUG), Console Level: INFO
2025-05-25 09:54:38 - phase2_model.models - INFO - [<module>:17] - Models (InceptionV3Baseline, DFCA, DiseaseAwareHVT, factory) imported successfully into phase2_model.models package.
2025-05-25 09:54:38 - phase2_model - INFO - [<module>:25] - Models re-exported successfully by phase2_model/__init__.py.
2025-05-25 09:54:38 - __main__ - INFO - [main_pretrain_script:67] - ======== Starting Phase 3: HVT Self-Supervised Pre-training ========
2025-05-25 09:54:38 - __main__ - INFO - [main_pretrain_script:68] - Full run configuration: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining', 'log_dir_name': 'logs', 'log_file_pretrain': 'phase3_simclr_hvt_xl.log', 'checkpoint_dir_name': 'pretrain_checkpoints_hvt_xl', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': 'Augmented Dataset', 'train_split_ratio': 0.95, 'num_classes': 7, 'num_workers': 4, 'prefetch_factor': 2, 'hvt_params_for_backbone': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'pretrain_img_size': (448, 448), 'pretrain_epochs': 80, 'pretrain_batch_size': 32, 'accumulation_steps': 2, 'pretrain_lr': 0.0005, 'pretrain_optimizer': 'AdamW', 'pretrain_scheduler': 'WarmupCosine', 'warmup_epochs': 10, 'eta_min_lr': 1e-06, 'pretrain_weight_decay': 0.05, 'pretrain_momentum': 0.9, 'temperature': 0.1, 'projection_dim': 256, 'projection_hidden_dim': 4096, 'simclr_s': 1.0, 'simclr_p_grayscale': 0.2, 'simclr_p_gaussian_blur': 0.5, 'simclr_rrc_scale_min': 0.08, 'evaluate_every_n_epochs': 10, 'linear_probe_epochs': 20, 'linear_probe_lr': 0.1, 'probe_optimizer': 'SGD', 'probe_momentum': 0.9, 'probe_weight_decay': 0.0, 'probe_batch_size': 256, 'save_every_n_epochs': 20, 'model_arch_name_for_ckpt': 'hvt_xl_simclr', 'clip_grad_norm': 1.0}
2025-05-25 09:54:38 - __main__ - INFO - [apply_pytorch_optimizations:51] - torch.backends.cudnn.benchmark = True
2025-05-25 09:54:38 - __main__ - INFO - [apply_pytorch_optimizations:57] - torch.set_float32_matmul_precision('high')
2025-05-25 09:54:38 - __main__ - INFO - [main_pretrain_script:76] - Global random seed set to: 42
2025-05-25 09:54:38 - __main__ - INFO - [main_pretrain_script:77] - Using device: cuda
2025-05-25 09:54:39 - __main__ - INFO - [main_pretrain_script:79] - CUDA Device: Tesla T4, PyTorch CUDA Version: 12.1
2025-05-25 09:54:39 - __main__ - INFO - [main_pretrain_script:83] - Target image size for pre-training: (448, 448)
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-25 09:54:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-25 09:54:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-25 09:54:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-25 09:54:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='val', img_size=(448, 448), use_spectral=False
2025-05-25 09:54:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-25 09:54:39 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-25 09:54:39 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'val' size: 457 samples.
2025-05-25 09:54:39 - __main__ - INFO - [main_pretrain_script:105] - Pretrain dataset size: 8680
2025-05-25 09:54:39 - __main__ - INFO - [main_pretrain_script:106] - Probe train dataset: 8680, Probe val dataset: 457
2025-05-25 09:54:39 - __main__ - INFO - [main_pretrain_script:118] - Pretrain DataLoader: 271 batches of size 32. Num workers: 4
2025-05-25 09:54:39 - __main__ - INFO - [main_pretrain_script:121] - Initializing HVTForPretraining model wrapper...
2025-05-25 09:54:39 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:36] - Initializing HVTForPretraining wrapper for img_size: (448, 448)
2025-05-25 09:54:39 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:44] - Instantiating HVTBackbone using parameters defined in Phase 3 config (hvt_params_for_backbone).
2025-05-25 09:54:39 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-25 09:54:42 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-25 09:54:44 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-25 09:54:44 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:70] - Projection head input dimension set to: 1536
2025-05-25 09:54:44 - phase3_pretraining.models.projection_head - INFO - [__init__:29] - ProjectionHead initialized: In=1536, Hidden=4096, Out=256, BatchNorm=True
2025-05-25 09:54:44 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:79] - HVTForPretraining wrapper initialized successfully.
2025-05-25 09:54:45 - phase3_pretraining.utils.augmentations - INFO - [__init__:27] - SimCLRAugmentation: img_size=(448, 448), s=1.0, p_gray=0.2, p_blur=0.5, rrc_min_scale=0.08
2025-05-25 09:54:45 - phase3_pretraining.utils.losses - INFO - [__init__:13] - InfoNCELoss initialized with temperature: 0.1
2025-05-25 09:54:45 - __main__ - INFO - [main_pretrain_script:137] - Initializing Pretrainer instance...
2025-05-25 09:54:45 - phase3_pretraining.pretrain.trainer - INFO - [__init__:35] - Initializing Pretrainer...
2025-05-25 09:54:45 - phase3_pretraining.pretrain.trainer - INFO - [__init__:52] - Optimizer: AdamW, LR: 0.0005, Weight Decay: 0.05
2025-05-25 09:54:45 - phase3_pretraining.pretrain.trainer - INFO - [__init__:65] - Pretrainer initialized. AMP enabled: True, Accum steps: 2, Clip Grad: 1.0
2025-05-25 09:54:45 - __main__ - INFO - [main_pretrain_script:145] - Starting SimCLR pre-training for 80 epochs.
2025-05-25 09:54:45 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:83] - Scheduler: WarmupCosine (WarmupSteps=2710, TotalTrainSteps=21680).
2025-05-25 10:02:01 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E1: Applying remaining gradients from accumulation.
2025-05-25 10:02:01 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 1 Training Summary: AvgLoss=3.4983, OptSteps=136, FinalLR=2.51e-05
2025-05-25 10:02:01 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 1/80 | Duration: 436.05s | Samples/sec: 19.89 | Avg Loss: 3.4983 | LR: 2.51e-05
2025-05-25 10:02:01 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E1: Alloc 3244.2MB, MaxAlloc 8348.5MB
2025-05-25 10:09:23 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E2: Applying remaining gradients from accumulation.
2025-05-25 10:09:23 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 2 Training Summary: AvgLoss=3.2037, OptSteps=136, FinalLR=5.02e-05
2025-05-25 10:09:23 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 2/80 | Duration: 442.01s | Samples/sec: 19.62 | Avg Loss: 3.2037 | LR: 5.02e-05
2025-05-25 10:09:23 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E2: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 10:16:40 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E3: Applying remaining gradients from accumulation.
2025-05-25 10:16:40 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 3 Training Summary: AvgLoss=3.2138, OptSteps=136, FinalLR=7.53e-05
2025-05-25 10:16:40 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 3/80 | Duration: 437.49s | Samples/sec: 19.82 | Avg Loss: 3.2138 | LR: 7.53e-05
2025-05-25 10:16:40 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E3: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 10:24:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E4: Applying remaining gradients from accumulation.
2025-05-25 10:24:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 4 Training Summary: AvgLoss=3.0888, OptSteps=136, FinalLR=1.00e-04
2025-05-25 10:24:00 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 4/80 | Duration: 439.98s | Samples/sec: 19.71 | Avg Loss: 3.0888 | LR: 1.00e-04
2025-05-25 10:24:00 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E4: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 10:31:18 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E5: Applying remaining gradients from accumulation.
2025-05-25 10:31:18 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 5 Training Summary: AvgLoss=3.0345, OptSteps=136, FinalLR=1.25e-04
2025-05-25 10:31:18 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 5/80 | Duration: 437.54s | Samples/sec: 19.82 | Avg Loss: 3.0345 | LR: 1.25e-04
2025-05-25 10:31:18 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E5: Alloc 3244.2MB, MaxAlloc 8354.0MB
2025-05-25 10:38:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E6: Applying remaining gradients from accumulation.
2025-05-25 10:38:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 6 Training Summary: AvgLoss=2.9490, OptSteps=136, FinalLR=1.51e-04
2025-05-25 10:38:35 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 6/80 | Duration: 437.37s | Samples/sec: 19.83 | Avg Loss: 2.9490 | LR: 1.51e-04
2025-05-25 10:38:35 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E6: Alloc 3244.2MB, MaxAlloc 8354.0MB
2025-05-25 10:45:53 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E7: Applying remaining gradients from accumulation.
2025-05-25 10:45:54 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 7 Training Summary: AvgLoss=2.9737, OptSteps=136, FinalLR=1.76e-04
2025-05-25 10:45:54 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 7/80 | Duration: 438.21s | Samples/sec: 19.79 | Avg Loss: 2.9737 | LR: 1.76e-04
2025-05-25 10:45:54 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E7: Alloc 3244.2MB, MaxAlloc 8354.0MB
2025-05-25 10:53:14 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E8: Applying remaining gradients from accumulation.
2025-05-25 10:53:14 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 8 Training Summary: AvgLoss=2.9101, OptSteps=136, FinalLR=2.01e-04
2025-05-25 10:53:14 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 8/80 | Duration: 440.35s | Samples/sec: 19.69 | Avg Loss: 2.9101 | LR: 2.01e-04
2025-05-25 10:53:14 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E8: Alloc 3244.2MB, MaxAlloc 8354.0MB
2025-05-25 11:00:34 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E9: Applying remaining gradients from accumulation.
2025-05-25 11:00:35 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 9 Training Summary: AvgLoss=2.8041, OptSteps=136, FinalLR=2.26e-04
2025-05-25 11:00:35 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 9/80 | Duration: 440.65s | Samples/sec: 19.68 | Avg Loss: 2.8041 | LR: 2.26e-04
2025-05-25 11:00:35 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E9: Alloc 3244.2MB, MaxAlloc 8354.0MB
2025-05-25 11:07:53 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E10: Applying remaining gradients from accumulation.
2025-05-25 11:07:54 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 10 Training Summary: AvgLoss=2.7066, OptSteps=136, FinalLR=2.51e-04
2025-05-25 11:07:54 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 10/80 | Duration: 439.20s | Samples/sec: 19.74 | Avg Loss: 2.7066 | LR: 2.51e-04
2025-05-25 11:07:54 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:168] - --- Starting Linear Probe (after SSL Epoch 10) ---
2025-05-25 11:07:54 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:173] - Probe E10: Feature dim for linear classifier: 1536
2025-05-25 11:08:07 - __main__ - ERROR - [main_pretrain_script:178] - Fatal error during pre-training loop at epoch 10: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.58 GiB of which 1.77 GiB is free. Process 189440 has 12.80 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/cvpr25/phase3_pretraining/run_ssl_pretraining.py", line 163, in main_pretrain_script
    current_probe_acc = pretrainer.evaluate_linear_probe(current_ssl_epoch=epoch)
  File "/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain/trainer.py", line 187, in evaluate_linear_probe
    with torch.no_grad(): features = self.model(rgb_img=rgb_imgs, mode='probe_extract')
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/cvpr25/phase3_pretraining/models/hvt_wrapper.py", line 89, in forward
    x_rgb_encoded, _, _, _ = self.backbone.forward_features_encoded(
  File "/teamspace/studios/this_studio/cvpr25/phase2_model/models/hvt.py", line 445, in forward_features_encoded
    x_rgb_encoded, rgb_orig_patch_grid = self._forward_stream(
  File "/teamspace/studios/this_studio/cvpr25/phase2_model/models/hvt.py", line 430, in _forward_stream
    x_patches = stage_module(x_patches)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/cvpr25/phase2_model/models/hvt.py", line 164, in forward
    x = blk(x)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/cvpr25/phase2_model/models/hvt.py", line 99, in forward
    x = x + self.drop_path(self.attn(self.norm1(x)))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/cvpr25/phase2_model/models/hvt.py", line 66, in forward
    attn = (q @ k.transpose(-2, -1)) * self.scale
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.58 GiB of which 1.77 GiB is free. Process 189440 has 12.80 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-25 11:08:07 - __main__ - INFO - [main_pretrain_script:180] - Pre-training finished or was interrupted. Last completed epoch: 9.
2025-05-25 11:08:08 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:215] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_final_epoch9.pth...
2025-05-25 11:08:17 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:228] - Checkpoint saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_final_epoch9.pth
2025-05-25 11:08:17 - __main__ - INFO - [main_pretrain_script:184] - Final pre-trained model checkpoint saved. Best probe accuracy during run: -1.00%
