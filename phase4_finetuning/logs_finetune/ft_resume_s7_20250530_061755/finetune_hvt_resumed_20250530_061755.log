2025-05-30 06:17:55 - root - INFO - [setup_logging:75] - Logging configured. Log file: /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_resume_s7_20250530_061755/finetune_hvt_resumed_20250530_061755.log. Logger 'root' Effective Level: DEBUG
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:138] - ======== Starting Phase 4 Fine-tuning (Run: ft_resume_s7_20250530_061755) ========
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:139] - Configuration: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase4_finetuning', 'run_name_suffix': 'ft_resume_s7', 'log_dir': 'logs_finetune', 'log_file_base': 'finetune_hvt_resumed', 'checkpoint_save_dir_name': 'checkpoints', 'best_model_filename_base': 'best_finetuned_hvt_resumed', 'final_model_filename_base': 'final_finetuned_hvt_resumed', 'resume_finetune_checkpoint_path': '/teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune_optimized_strategy_v1_stable/checkpoints_optimized_strategy_v1_stable/best_finetuned_hvt_optimized_strategy_v1_stable.pth', 'load_optimizer_scheduler_on_resume': True, 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': None, 'img_size': (448, 448), 'num_classes': 7, 'train_split_ratio': 0.8, 'normalize_data': True, 'use_weighted_sampler': True, 'num_workers': 4, 'prefetch_factor': 2, 'model_architecture_name': 'DiseaseAwareHVT_ResumedFinetune', 'hvt_params_for_model_init': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 0, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.1, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False}, 'epochs': 100, 'batch_size': 16, 'accumulation_steps': 2, 'amp_enabled': True, 'clip_grad_norm': 1.0, 'log_interval': 10, 'optimizer': 'AdamW', 'weight_decay': 0.1, 'optimizer_params': {'betas': (0.9, 0.999), 'eps': 1e-08}, 'freeze_backbone_epochs': 10, 'lr_head_frozen_phase': 5e-05, 'lr_backbone_unfrozen_phase': 1e-05, 'lr_head_unfrozen_phase': 1e-05, 'scheduler': 'WarmupCosine', 'warmup_epochs': 5, 'eta_min_lr': 1e-07, 'loss_label_smoothing': 0.1, 'augmentations_enabled': True, 'use_enhanced_augmentations': True, 'evaluate_every_n_epochs': 1, 'early_stopping_patience': 30, 'metric_to_monitor_early_stopping': 'f1_macro', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True}
2025-05-30 06:17:55 - __main__ - INFO - [set_global_seed:39] - Global random seed set to: 42
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:148] - Device: cuda. GPU: Tesla T4
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:153] - Enabled cudnn.benchmark.
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:156] - Set matmul_precision to high.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:54] - [DATASET INIT - train] Root: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection, ImgSize: (448, 448), Normalize: True
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:74] - [DATASET INIT - train] Scanning: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-30 06:17:55 - phase4_finetuning.dataset - DEBUG - [__init__:67] - [DATASET INIT - train] Dataset name 'None' is None/empty, skipping.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:105] - [DATASET INIT - train] Total valid image paths collected: 2137 from ~2137 items considered.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:117] - [DATASET INIT - train] Dataset split size: 1709 samples.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:128] - [DATASET INIT - train] Base RGB Transforms: Compose(
      ToImage()
      ToDtype(scale=True)
      Resize(size=[448, 448], interpolation=InterpolationMode.BICUBIC, antialias=True)
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)
)
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:54] - [DATASET INIT - val] Root: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection, ImgSize: (448, 448), Normalize: True
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:74] - [DATASET INIT - val] Scanning: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-30 06:17:55 - phase4_finetuning.dataset - DEBUG - [__init__:67] - [DATASET INIT - val] Dataset name 'None' is None/empty, skipping.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:105] - [DATASET INIT - val] Total valid image paths collected: 2137 from ~2137 items considered.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:117] - [DATASET INIT - val] Dataset split size: 428 samples.
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [__init__:128] - [DATASET INIT - val] Base RGB Transforms: Compose(
      ToImage()
      ToDtype(scale=True)
      Resize(size=[448, 448], interpolation=InterpolationMode.BICUBIC, antialias=True)
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)
)
2025-05-30 06:17:55 - phase4_finetuning.dataset - INFO - [get_class_weights:196] - Computed class weights (inv_freq for loss) for split 'train': [1.191 0.684 1.179 1.115 1.387 0.533 2.806]
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:180] - Using WeightedRandomSampler.
2025-05-30 06:17:55 - __main__ - INFO - [main_execution_logic:189] - Dataloaders: Train batches=106, Val batches=27
2025-05-30 06:17:55 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-30 06:17:58 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-30 06:18:00 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-30 06:18:00 - __main__ - INFO - [main_execution_logic:198] - Initialized DiseaseAwareHVT model with 7 classes.
2025-05-30 06:18:00 - __main__ - INFO - [main_execution_logic:214] - Optimizer initialized: AdamW.
2025-05-30 06:18:00 - __main__ - INFO - [resume_from_checkpoint:62] - Resuming from checkpoint: /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune_optimized_strategy_v1_stable/checkpoints_optimized_strategy_v1_stable/best_finetuned_hvt_optimized_strategy_v1_stable.pth
2025-05-30 06:18:02 - __main__ - DEBUG - [resume_from_checkpoint:65] - Checkpoint keys: ['rgb_pos_embed', 'rgb_patch_embed.proj.weight', 'rgb_patch_embed.proj.bias', 'rgb_patch_embed.norm.weight', 'rgb_patch_embed.norm.bias', 'rgb_stages.0.blocks.0.norm1.weight', 'rgb_stages.0.blocks.0.norm1.bias', 'rgb_stages.0.blocks.0.attn.qkv.weight', 'rgb_stages.0.blocks.0.attn.qkv.bias', 'rgb_stages.0.blocks.0.attn.proj.weight', 'rgb_stages.0.blocks.0.attn.proj.bias', 'rgb_stages.0.blocks.0.norm2.weight', 'rgb_stages.0.blocks.0.norm2.bias', 'rgb_stages.0.blocks.0.mlp.fc1.weight', 'rgb_stages.0.blocks.0.mlp.fc1.bias', 'rgb_stages.0.blocks.0.mlp.fc2.weight', 'rgb_stages.0.blocks.0.mlp.fc2.bias', 'rgb_stages.0.blocks.1.norm1.weight', 'rgb_stages.0.blocks.1.norm1.bias', 'rgb_stages.0.blocks.1.attn.qkv.weight', 'rgb_stages.0.blocks.1.attn.qkv.bias', 'rgb_stages.0.blocks.1.attn.proj.weight', 'rgb_stages.0.blocks.1.attn.proj.bias', 'rgb_stages.0.blocks.1.norm2.weight', 'rgb_stages.0.blocks.1.norm2.bias', 'rgb_stages.0.blocks.1.mlp.fc1.weight', 'rgb_stages.0.blocks.1.mlp.fc1.bias', 'rgb_stages.0.blocks.1.mlp.fc2.weight', 'rgb_stages.0.blocks.1.mlp.fc2.bias', 'rgb_stages.0.blocks.2.norm1.weight', 'rgb_stages.0.blocks.2.norm1.bias', 'rgb_stages.0.blocks.2.attn.qkv.weight', 'rgb_stages.0.blocks.2.attn.qkv.bias', 'rgb_stages.0.blocks.2.attn.proj.weight', 'rgb_stages.0.blocks.2.attn.proj.bias', 'rgb_stages.0.blocks.2.norm2.weight', 'rgb_stages.0.blocks.2.norm2.bias', 'rgb_stages.0.blocks.2.mlp.fc1.weight', 'rgb_stages.0.blocks.2.mlp.fc1.bias', 'rgb_stages.0.blocks.2.mlp.fc2.weight', 'rgb_stages.0.blocks.2.mlp.fc2.bias', 'rgb_stages.0.downsample_layer.reduction.weight', 'rgb_stages.0.downsample_layer.norm.weight', 'rgb_stages.0.downsample_layer.norm.bias', 'rgb_stages.1.blocks.0.norm1.weight', 'rgb_stages.1.blocks.0.norm1.bias', 'rgb_stages.1.blocks.0.attn.qkv.weight', 'rgb_stages.1.blocks.0.attn.qkv.bias', 'rgb_stages.1.blocks.0.attn.proj.weight', 'rgb_stages.1.blocks.0.attn.proj.bias', 'rgb_stages.1.blocks.0.norm2.weight', 'rgb_stages.1.blocks.0.norm2.bias', 'rgb_stages.1.blocks.0.mlp.fc1.weight', 'rgb_stages.1.blocks.0.mlp.fc1.bias', 'rgb_stages.1.blocks.0.mlp.fc2.weight', 'rgb_stages.1.blocks.0.mlp.fc2.bias', 'rgb_stages.1.blocks.1.norm1.weight', 'rgb_stages.1.blocks.1.norm1.bias', 'rgb_stages.1.blocks.1.attn.qkv.weight', 'rgb_stages.1.blocks.1.attn.qkv.bias', 'rgb_stages.1.blocks.1.attn.proj.weight', 'rgb_stages.1.blocks.1.attn.proj.bias', 'rgb_stages.1.blocks.1.norm2.weight', 'rgb_stages.1.blocks.1.norm2.bias', 'rgb_stages.1.blocks.1.mlp.fc1.weight', 'rgb_stages.1.blocks.1.mlp.fc1.bias', 'rgb_stages.1.blocks.1.mlp.fc2.weight', 'rgb_stages.1.blocks.1.mlp.fc2.bias', 'rgb_stages.1.blocks.2.norm1.weight', 'rgb_stages.1.blocks.2.norm1.bias', 'rgb_stages.1.blocks.2.attn.qkv.weight', 'rgb_stages.1.blocks.2.attn.qkv.bias', 'rgb_stages.1.blocks.2.attn.proj.weight', 'rgb_stages.1.blocks.2.attn.proj.bias', 'rgb_stages.1.blocks.2.norm2.weight', 'rgb_stages.1.blocks.2.norm2.bias', 'rgb_stages.1.blocks.2.mlp.fc1.weight', 'rgb_stages.1.blocks.2.mlp.fc1.bias', 'rgb_stages.1.blocks.2.mlp.fc2.weight', 'rgb_stages.1.blocks.2.mlp.fc2.bias', 'rgb_stages.1.blocks.3.norm1.weight', 'rgb_stages.1.blocks.3.norm1.bias', 'rgb_stages.1.blocks.3.attn.qkv.weight', 'rgb_stages.1.blocks.3.attn.qkv.bias', 'rgb_stages.1.blocks.3.attn.proj.weight', 'rgb_stages.1.blocks.3.attn.proj.bias', 'rgb_stages.1.blocks.3.norm2.weight', 'rgb_stages.1.blocks.3.norm2.bias', 'rgb_stages.1.blocks.3.mlp.fc1.weight', 'rgb_stages.1.blocks.3.mlp.fc1.bias', 'rgb_stages.1.blocks.3.mlp.fc2.weight', 'rgb_stages.1.blocks.3.mlp.fc2.bias', 'rgb_stages.1.blocks.4.norm1.weight', 'rgb_stages.1.blocks.4.norm1.bias', 'rgb_stages.1.blocks.4.attn.qkv.weight', 'rgb_stages.1.blocks.4.attn.qkv.bias', 'rgb_stages.1.blocks.4.attn.proj.weight', 'rgb_stages.1.blocks.4.attn.proj.bias', 'rgb_stages.1.blocks.4.norm2.weight', 'rgb_stages.1.blocks.4.norm2.bias', 'rgb_stages.1.blocks.4.mlp.fc1.weight', 'rgb_stages.1.blocks.4.mlp.fc1.bias', 'rgb_stages.1.blocks.4.mlp.fc2.weight', 'rgb_stages.1.blocks.4.mlp.fc2.bias', 'rgb_stages.1.blocks.5.norm1.weight', 'rgb_stages.1.blocks.5.norm1.bias', 'rgb_stages.1.blocks.5.attn.qkv.weight', 'rgb_stages.1.blocks.5.attn.qkv.bias', 'rgb_stages.1.blocks.5.attn.proj.weight', 'rgb_stages.1.blocks.5.attn.proj.bias', 'rgb_stages.1.blocks.5.norm2.weight', 'rgb_stages.1.blocks.5.norm2.bias', 'rgb_stages.1.blocks.5.mlp.fc1.weight', 'rgb_stages.1.blocks.5.mlp.fc1.bias', 'rgb_stages.1.blocks.5.mlp.fc2.weight', 'rgb_stages.1.blocks.5.mlp.fc2.bias', 'rgb_stages.1.downsample_layer.reduction.weight', 'rgb_stages.1.downsample_layer.norm.weight', 'rgb_stages.1.downsample_layer.norm.bias', 'rgb_stages.2.blocks.0.norm1.weight', 'rgb_stages.2.blocks.0.norm1.bias', 'rgb_stages.2.blocks.0.attn.qkv.weight', 'rgb_stages.2.blocks.0.attn.qkv.bias', 'rgb_stages.2.blocks.0.attn.proj.weight', 'rgb_stages.2.blocks.0.attn.proj.bias', 'rgb_stages.2.blocks.0.norm2.weight', 'rgb_stages.2.blocks.0.norm2.bias', 'rgb_stages.2.blocks.0.mlp.fc1.weight', 'rgb_stages.2.blocks.0.mlp.fc1.bias', 'rgb_stages.2.blocks.0.mlp.fc2.weight', 'rgb_stages.2.blocks.0.mlp.fc2.bias', 'rgb_stages.2.blocks.1.norm1.weight', 'rgb_stages.2.blocks.1.norm1.bias', 'rgb_stages.2.blocks.1.attn.qkv.weight', 'rgb_stages.2.blocks.1.attn.qkv.bias', 'rgb_stages.2.blocks.1.attn.proj.weight', 'rgb_stages.2.blocks.1.attn.proj.bias', 'rgb_stages.2.blocks.1.norm2.weight', 'rgb_stages.2.blocks.1.norm2.bias', 'rgb_stages.2.blocks.1.mlp.fc1.weight', 'rgb_stages.2.blocks.1.mlp.fc1.bias', 'rgb_stages.2.blocks.1.mlp.fc2.weight', 'rgb_stages.2.blocks.1.mlp.fc2.bias', 'rgb_stages.2.blocks.2.norm1.weight', 'rgb_stages.2.blocks.2.norm1.bias', 'rgb_stages.2.blocks.2.attn.qkv.weight', 'rgb_stages.2.blocks.2.attn.qkv.bias', 'rgb_stages.2.blocks.2.attn.proj.weight', 'rgb_stages.2.blocks.2.attn.proj.bias', 'rgb_stages.2.blocks.2.norm2.weight', 'rgb_stages.2.blocks.2.norm2.bias', 'rgb_stages.2.blocks.2.mlp.fc1.weight', 'rgb_stages.2.blocks.2.mlp.fc1.bias', 'rgb_stages.2.blocks.2.mlp.fc2.weight', 'rgb_stages.2.blocks.2.mlp.fc2.bias', 'rgb_stages.2.blocks.3.norm1.weight', 'rgb_stages.2.blocks.3.norm1.bias', 'rgb_stages.2.blocks.3.attn.qkv.weight', 'rgb_stages.2.blocks.3.attn.qkv.bias', 'rgb_stages.2.blocks.3.attn.proj.weight', 'rgb_stages.2.blocks.3.attn.proj.bias', 'rgb_stages.2.blocks.3.norm2.weight', 'rgb_stages.2.blocks.3.norm2.bias', 'rgb_stages.2.blocks.3.mlp.fc1.weight', 'rgb_stages.2.blocks.3.mlp.fc1.bias', 'rgb_stages.2.blocks.3.mlp.fc2.weight', 'rgb_stages.2.blocks.3.mlp.fc2.bias', 'rgb_stages.2.blocks.4.norm1.weight', 'rgb_stages.2.blocks.4.norm1.bias', 'rgb_stages.2.blocks.4.attn.qkv.weight', 'rgb_stages.2.blocks.4.attn.qkv.bias', 'rgb_stages.2.blocks.4.attn.proj.weight', 'rgb_stages.2.blocks.4.attn.proj.bias', 'rgb_stages.2.blocks.4.norm2.weight', 'rgb_stages.2.blocks.4.norm2.bias', 'rgb_stages.2.blocks.4.mlp.fc1.weight', 'rgb_stages.2.blocks.4.mlp.fc1.bias', 'rgb_stages.2.blocks.4.mlp.fc2.weight', 'rgb_stages.2.blocks.4.mlp.fc2.bias', 'rgb_stages.2.blocks.5.norm1.weight', 'rgb_stages.2.blocks.5.norm1.bias', 'rgb_stages.2.blocks.5.attn.qkv.weight', 'rgb_stages.2.blocks.5.attn.qkv.bias', 'rgb_stages.2.blocks.5.attn.proj.weight', 'rgb_stages.2.blocks.5.attn.proj.bias', 'rgb_stages.2.blocks.5.norm2.weight', 'rgb_stages.2.blocks.5.norm2.bias', 'rgb_stages.2.blocks.5.mlp.fc1.weight', 'rgb_stages.2.blocks.5.mlp.fc1.bias', 'rgb_stages.2.blocks.5.mlp.fc2.weight', 'rgb_stages.2.blocks.5.mlp.fc2.bias', 'rgb_stages.2.blocks.6.norm1.weight', 'rgb_stages.2.blocks.6.norm1.bias', 'rgb_stages.2.blocks.6.attn.qkv.weight', 'rgb_stages.2.blocks.6.attn.qkv.bias', 'rgb_stages.2.blocks.6.attn.proj.weight', 'rgb_stages.2.blocks.6.attn.proj.bias', 'rgb_stages.2.blocks.6.norm2.weight', 'rgb_stages.2.blocks.6.norm2.bias', 'rgb_stages.2.blocks.6.mlp.fc1.weight', 'rgb_stages.2.blocks.6.mlp.fc1.bias', 'rgb_stages.2.blocks.6.mlp.fc2.weight', 'rgb_stages.2.blocks.6.mlp.fc2.bias', 'rgb_stages.2.blocks.7.norm1.weight', 'rgb_stages.2.blocks.7.norm1.bias', 'rgb_stages.2.blocks.7.attn.qkv.weight', 'rgb_stages.2.blocks.7.attn.qkv.bias', 'rgb_stages.2.blocks.7.attn.proj.weight', 'rgb_stages.2.blocks.7.attn.proj.bias', 'rgb_stages.2.blocks.7.norm2.weight', 'rgb_stages.2.blocks.7.norm2.bias', 'rgb_stages.2.blocks.7.mlp.fc1.weight', 'rgb_stages.2.blocks.7.mlp.fc1.bias', 'rgb_stages.2.blocks.7.mlp.fc2.weight', 'rgb_stages.2.blocks.7.mlp.fc2.bias', 'rgb_stages.2.blocks.8.norm1.weight', 'rgb_stages.2.blocks.8.norm1.bias', 'rgb_stages.2.blocks.8.attn.qkv.weight', 'rgb_stages.2.blocks.8.attn.qkv.bias', 'rgb_stages.2.blocks.8.attn.proj.weight', 'rgb_stages.2.blocks.8.attn.proj.bias', 'rgb_stages.2.blocks.8.norm2.weight', 'rgb_stages.2.blocks.8.norm2.bias', 'rgb_stages.2.blocks.8.mlp.fc1.weight', 'rgb_stages.2.blocks.8.mlp.fc1.bias', 'rgb_stages.2.blocks.8.mlp.fc2.weight', 'rgb_stages.2.blocks.8.mlp.fc2.bias', 'rgb_stages.2.blocks.9.norm1.weight', 'rgb_stages.2.blocks.9.norm1.bias', 'rgb_stages.2.blocks.9.attn.qkv.weight', 'rgb_stages.2.blocks.9.attn.qkv.bias', 'rgb_stages.2.blocks.9.attn.proj.weight', 'rgb_stages.2.blocks.9.attn.proj.bias', 'rgb_stages.2.blocks.9.norm2.weight', 'rgb_stages.2.blocks.9.norm2.bias', 'rgb_stages.2.blocks.9.mlp.fc1.weight', 'rgb_stages.2.blocks.9.mlp.fc1.bias', 'rgb_stages.2.blocks.9.mlp.fc2.weight', 'rgb_stages.2.blocks.9.mlp.fc2.bias', 'rgb_stages.2.blocks.10.norm1.weight', 'rgb_stages.2.blocks.10.norm1.bias', 'rgb_stages.2.blocks.10.attn.qkv.weight', 'rgb_stages.2.blocks.10.attn.qkv.bias', 'rgb_stages.2.blocks.10.attn.proj.weight', 'rgb_stages.2.blocks.10.attn.proj.bias', 'rgb_stages.2.blocks.10.norm2.weight', 'rgb_stages.2.blocks.10.norm2.bias', 'rgb_stages.2.blocks.10.mlp.fc1.weight', 'rgb_stages.2.blocks.10.mlp.fc1.bias', 'rgb_stages.2.blocks.10.mlp.fc2.weight', 'rgb_stages.2.blocks.10.mlp.fc2.bias', 'rgb_stages.2.blocks.11.norm1.weight', 'rgb_stages.2.blocks.11.norm1.bias', 'rgb_stages.2.blocks.11.attn.qkv.weight', 'rgb_stages.2.blocks.11.attn.qkv.bias', 'rgb_stages.2.blocks.11.attn.proj.weight', 'rgb_stages.2.blocks.11.attn.proj.bias', 'rgb_stages.2.blocks.11.norm2.weight', 'rgb_stages.2.blocks.11.norm2.bias', 'rgb_stages.2.blocks.11.mlp.fc1.weight', 'rgb_stages.2.blocks.11.mlp.fc1.bias', 'rgb_stages.2.blocks.11.mlp.fc2.weight', 'rgb_stages.2.blocks.11.mlp.fc2.bias', 'rgb_stages.2.blocks.12.norm1.weight', 'rgb_stages.2.blocks.12.norm1.bias', 'rgb_stages.2.blocks.12.attn.qkv.weight', 'rgb_stages.2.blocks.12.attn.qkv.bias', 'rgb_stages.2.blocks.12.attn.proj.weight', 'rgb_stages.2.blocks.12.attn.proj.bias', 'rgb_stages.2.blocks.12.norm2.weight', 'rgb_stages.2.blocks.12.norm2.bias', 'rgb_stages.2.blocks.12.mlp.fc1.weight', 'rgb_stages.2.blocks.12.mlp.fc1.bias', 'rgb_stages.2.blocks.12.mlp.fc2.weight', 'rgb_stages.2.blocks.12.mlp.fc2.bias', 'rgb_stages.2.blocks.13.norm1.weight', 'rgb_stages.2.blocks.13.norm1.bias', 'rgb_stages.2.blocks.13.attn.qkv.weight', 'rgb_stages.2.blocks.13.attn.qkv.bias', 'rgb_stages.2.blocks.13.attn.proj.weight', 'rgb_stages.2.blocks.13.attn.proj.bias', 'rgb_stages.2.blocks.13.norm2.weight', 'rgb_stages.2.blocks.13.norm2.bias', 'rgb_stages.2.blocks.13.mlp.fc1.weight', 'rgb_stages.2.blocks.13.mlp.fc1.bias', 'rgb_stages.2.blocks.13.mlp.fc2.weight', 'rgb_stages.2.blocks.13.mlp.fc2.bias', 'rgb_stages.2.blocks.14.norm1.weight', 'rgb_stages.2.blocks.14.norm1.bias', 'rgb_stages.2.blocks.14.attn.qkv.weight', 'rgb_stages.2.blocks.14.attn.qkv.bias', 'rgb_stages.2.blocks.14.attn.proj.weight', 'rgb_stages.2.blocks.14.attn.proj.bias', 'rgb_stages.2.blocks.14.norm2.weight', 'rgb_stages.2.blocks.14.norm2.bias', 'rgb_stages.2.blocks.14.mlp.fc1.weight', 'rgb_stages.2.blocks.14.mlp.fc1.bias', 'rgb_stages.2.blocks.14.mlp.fc2.weight', 'rgb_stages.2.blocks.14.mlp.fc2.bias', 'rgb_stages.2.blocks.15.norm1.weight', 'rgb_stages.2.blocks.15.norm1.bias', 'rgb_stages.2.blocks.15.attn.qkv.weight', 'rgb_stages.2.blocks.15.attn.qkv.bias', 'rgb_stages.2.blocks.15.attn.proj.weight', 'rgb_stages.2.blocks.15.attn.proj.bias', 'rgb_stages.2.blocks.15.norm2.weight', 'rgb_stages.2.blocks.15.norm2.bias', 'rgb_stages.2.blocks.15.mlp.fc1.weight', 'rgb_stages.2.blocks.15.mlp.fc1.bias', 'rgb_stages.2.blocks.15.mlp.fc2.weight', 'rgb_stages.2.blocks.15.mlp.fc2.bias', 'rgb_stages.2.blocks.16.norm1.weight', 'rgb_stages.2.blocks.16.norm1.bias', 'rgb_stages.2.blocks.16.attn.qkv.weight', 'rgb_stages.2.blocks.16.attn.qkv.bias', 'rgb_stages.2.blocks.16.attn.proj.weight', 'rgb_stages.2.blocks.16.attn.proj.bias', 'rgb_stages.2.blocks.16.norm2.weight', 'rgb_stages.2.blocks.16.norm2.bias', 'rgb_stages.2.blocks.16.mlp.fc1.weight', 'rgb_stages.2.blocks.16.mlp.fc1.bias', 'rgb_stages.2.blocks.16.mlp.fc2.weight', 'rgb_stages.2.blocks.16.mlp.fc2.bias', 'rgb_stages.2.blocks.17.norm1.weight', 'rgb_stages.2.blocks.17.norm1.bias', 'rgb_stages.2.blocks.17.attn.qkv.weight', 'rgb_stages.2.blocks.17.attn.qkv.bias', 'rgb_stages.2.blocks.17.attn.proj.weight', 'rgb_stages.2.blocks.17.attn.proj.bias', 'rgb_stages.2.blocks.17.norm2.weight', 'rgb_stages.2.blocks.17.norm2.bias', 'rgb_stages.2.blocks.17.mlp.fc1.weight', 'rgb_stages.2.blocks.17.mlp.fc1.bias', 'rgb_stages.2.blocks.17.mlp.fc2.weight', 'rgb_stages.2.blocks.17.mlp.fc2.bias', 'rgb_stages.2.blocks.18.norm1.weight', 'rgb_stages.2.blocks.18.norm1.bias', 'rgb_stages.2.blocks.18.attn.qkv.weight', 'rgb_stages.2.blocks.18.attn.qkv.bias', 'rgb_stages.2.blocks.18.attn.proj.weight', 'rgb_stages.2.blocks.18.attn.proj.bias', 'rgb_stages.2.blocks.18.norm2.weight', 'rgb_stages.2.blocks.18.norm2.bias', 'rgb_stages.2.blocks.18.mlp.fc1.weight', 'rgb_stages.2.blocks.18.mlp.fc1.bias', 'rgb_stages.2.blocks.18.mlp.fc2.weight', 'rgb_stages.2.blocks.18.mlp.fc2.bias', 'rgb_stages.2.blocks.19.norm1.weight', 'rgb_stages.2.blocks.19.norm1.bias', 'rgb_stages.2.blocks.19.attn.qkv.weight', 'rgb_stages.2.blocks.19.attn.qkv.bias', 'rgb_stages.2.blocks.19.attn.proj.weight', 'rgb_stages.2.blocks.19.attn.proj.bias', 'rgb_stages.2.blocks.19.norm2.weight', 'rgb_stages.2.blocks.19.norm2.bias', 'rgb_stages.2.blocks.19.mlp.fc1.weight', 'rgb_stages.2.blocks.19.mlp.fc1.bias', 'rgb_stages.2.blocks.19.mlp.fc2.weight', 'rgb_stages.2.blocks.19.mlp.fc2.bias', 'rgb_stages.2.blocks.20.norm1.weight', 'rgb_stages.2.blocks.20.norm1.bias', 'rgb_stages.2.blocks.20.attn.qkv.weight', 'rgb_stages.2.blocks.20.attn.qkv.bias', 'rgb_stages.2.blocks.20.attn.proj.weight', 'rgb_stages.2.blocks.20.attn.proj.bias', 'rgb_stages.2.blocks.20.norm2.weight', 'rgb_stages.2.blocks.20.norm2.bias', 'rgb_stages.2.blocks.20.mlp.fc1.weight', 'rgb_stages.2.blocks.20.mlp.fc1.bias', 'rgb_stages.2.blocks.20.mlp.fc2.weight', 'rgb_stages.2.blocks.20.mlp.fc2.bias', 'rgb_stages.2.blocks.21.norm1.weight', 'rgb_stages.2.blocks.21.norm1.bias', 'rgb_stages.2.blocks.21.attn.qkv.weight', 'rgb_stages.2.blocks.21.attn.qkv.bias', 'rgb_stages.2.blocks.21.attn.proj.weight', 'rgb_stages.2.blocks.21.attn.proj.bias', 'rgb_stages.2.blocks.21.norm2.weight', 'rgb_stages.2.blocks.21.norm2.bias', 'rgb_stages.2.blocks.21.mlp.fc1.weight', 'rgb_stages.2.blocks.21.mlp.fc1.bias', 'rgb_stages.2.blocks.21.mlp.fc2.weight', 'rgb_stages.2.blocks.21.mlp.fc2.bias', 'rgb_stages.2.blocks.22.norm1.weight', 'rgb_stages.2.blocks.22.norm1.bias', 'rgb_stages.2.blocks.22.attn.qkv.weight', 'rgb_stages.2.blocks.22.attn.qkv.bias', 'rgb_stages.2.blocks.22.attn.proj.weight', 'rgb_stages.2.blocks.22.attn.proj.bias', 'rgb_stages.2.blocks.22.norm2.weight', 'rgb_stages.2.blocks.22.norm2.bias', 'rgb_stages.2.blocks.22.mlp.fc1.weight', 'rgb_stages.2.blocks.22.mlp.fc1.bias', 'rgb_stages.2.blocks.22.mlp.fc2.weight', 'rgb_stages.2.blocks.22.mlp.fc2.bias', 'rgb_stages.2.blocks.23.norm1.weight', 'rgb_stages.2.blocks.23.norm1.bias', 'rgb_stages.2.blocks.23.attn.qkv.weight', 'rgb_stages.2.blocks.23.attn.qkv.bias', 'rgb_stages.2.blocks.23.attn.proj.weight', 'rgb_stages.2.blocks.23.attn.proj.bias', 'rgb_stages.2.blocks.23.norm2.weight', 'rgb_stages.2.blocks.23.norm2.bias', 'rgb_stages.2.blocks.23.mlp.fc1.weight', 'rgb_stages.2.blocks.23.mlp.fc1.bias', 'rgb_stages.2.blocks.23.mlp.fc2.weight', 'rgb_stages.2.blocks.23.mlp.fc2.bias', 'rgb_stages.2.downsample_layer.reduction.weight', 'rgb_stages.2.downsample_layer.norm.weight', 'rgb_stages.2.downsample_layer.norm.bias', 'rgb_stages.3.blocks.0.norm1.weight', 'rgb_stages.3.blocks.0.norm1.bias', 'rgb_stages.3.blocks.0.attn.qkv.weight', 'rgb_stages.3.blocks.0.attn.qkv.bias', 'rgb_stages.3.blocks.0.attn.proj.weight', 'rgb_stages.3.blocks.0.attn.proj.bias', 'rgb_stages.3.blocks.0.norm2.weight', 'rgb_stages.3.blocks.0.norm2.bias', 'rgb_stages.3.blocks.0.mlp.fc1.weight', 'rgb_stages.3.blocks.0.mlp.fc1.bias', 'rgb_stages.3.blocks.0.mlp.fc2.weight', 'rgb_stages.3.blocks.0.mlp.fc2.bias', 'rgb_stages.3.blocks.1.norm1.weight', 'rgb_stages.3.blocks.1.norm1.bias', 'rgb_stages.3.blocks.1.attn.qkv.weight', 'rgb_stages.3.blocks.1.attn.qkv.bias', 'rgb_stages.3.blocks.1.attn.proj.weight', 'rgb_stages.3.blocks.1.attn.proj.bias', 'rgb_stages.3.blocks.1.norm2.weight', 'rgb_stages.3.blocks.1.norm2.bias', 'rgb_stages.3.blocks.1.mlp.fc1.weight', 'rgb_stages.3.blocks.1.mlp.fc1.bias', 'rgb_stages.3.blocks.1.mlp.fc2.weight', 'rgb_stages.3.blocks.1.mlp.fc2.bias', 'rgb_stages.3.blocks.2.norm1.weight', 'rgb_stages.3.blocks.2.norm1.bias', 'rgb_stages.3.blocks.2.attn.qkv.weight', 'rgb_stages.3.blocks.2.attn.qkv.bias', 'rgb_stages.3.blocks.2.attn.proj.weight', 'rgb_stages.3.blocks.2.attn.proj.bias', 'rgb_stages.3.blocks.2.norm2.weight', 'rgb_stages.3.blocks.2.norm2.bias', 'rgb_stages.3.blocks.2.mlp.fc1.weight', 'rgb_stages.3.blocks.2.mlp.fc1.bias', 'rgb_stages.3.blocks.2.mlp.fc2.weight', 'rgb_stages.3.blocks.2.mlp.fc2.bias', 'norm_rgb_final_encoder.weight', 'norm_rgb_final_encoder.bias', 'classifier_head_norm.weight', 'classifier_head_norm.bias', 'classifier_head.weight', 'classifier_head.bias']
2025-05-30 06:18:02 - __main__ - INFO - [resume_from_checkpoint:70] - Checkpoint appears to be a raw model state dictionary.
2025-05-30 06:18:02 - __main__ - INFO - [resume_from_checkpoint:82] - Loaded model state from checkpoint.
2025-05-30 06:18:02 - __main__ - WARNING - [resume_from_checkpoint:91] - Optimizer state not loaded.
2025-05-30 06:18:02 - __main__ - WARNING - [resume_from_checkpoint:97] - GradScaler state not loaded.
2025-05-30 06:18:02 - __main__ - INFO - [resume_from_checkpoint:112] - Resuming from epoch 1. Best metric: 0.0000. Last scheduler step: -1
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:222] - Model moved to device: cuda. Total params: 273,615,751
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:235] - Scheduler initialized: WarmupCosine. Warmup steps: 265, Total steps: 5300
2025-05-30 06:18:02 - phase4_finetuning.utils.augmentations - WARNING - [__init__:226] - Legacy 'EnhancedFinetuneAugmentation' called. Redirecting to 'StableEnhancedFinetuneAugmentation' with 'moderate' severity.
2025-05-30 06:18:02 - phase4_finetuning.utils.augmentations - INFO - [__init__:145] - StableEnhancedFinetuneAugmentation initialized with 'moderate' severity for stability.
2025-05-30 06:18:02 - phase4_finetuning.utils.augmentations - DEBUG - [__init__:146] - Augmentation pipeline: Compose(
      RandomHorizontalFlip(p=0.5)
      RandomVerticalFlip(p=0.3)
      RandomRotation(degrees=[-15.0, 15.0], interpolation=InterpolationMode.BILINEAR, expand=False, fill=0.0)
      StableColorJitter()
      RandomAffine(degrees=[-7.0, 7.0], translate=(0.08, 0.08), scale=(0.9, 1.1), shear=[-8.0, 8.0], interpolation=InterpolationMode.BILINEAR, fill=0.0)
      RandomPerspective(p=0.2, distortion_scale=0.15, interpolation=InterpolationMode.BILINEAR, fill=0.0)
      RandomApply(    GaussianBlur(kernel_size=(3, 3), sigma=[0.1, 0.5]))
      StableRandomErasing()
)
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:241] - Using augmentation: EnhancedFinetuneAugmentation.
2025-05-30 06:18:02 - phase4_finetuning.finetune.trainer - INFO - [__init__:53] - Finetuner initialized: device=cuda, accum_steps=2, AMP=True
2025-05-30 06:18:02 - phase4_finetuning.finetune.trainer - INFO - [__init__:55] - Using training augmentations: EnhancedFinetuneAugmentation
2025-05-30 06:18:02 - phase4_finetuning.finetune.trainer - INFO - [__init__:57] - Test-Time Augmentation (TTA) enabled for validation.
2025-05-30 06:18:02 - phase4_finetuning.finetune.trainer - INFO - [__init__:61] - Gradient monitoring enabled. Log interval: 50 optimizer steps.
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:268] - Checkpoints will be saved in: /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_resume_s7_20250530_061755/checkpoints
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:274] - Starting fine-tuning from epoch 1 for 100 total epochs.
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:287] - Epoch 1: head LR set to 5.00e-05
2025-05-30 06:18:02 - __main__ - INFO - [main_execution_logic:287] - Epoch 1: backbone LR set to 0.00e+00
2025-05-30 06:18:06 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B1: Scheduler step after optimizer step.
2025-05-30 06:18:07 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B3: Scheduler step after optimizer step.
2025-05-30 06:18:08 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B5: Scheduler step after optimizer step.
2025-05-30 06:18:09 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B7: Scheduler step after optimizer step.
2025-05-30 06:18:10 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B9: Scheduler step after optimizer step.
2025-05-30 06:18:11 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B11: Scheduler step after optimizer step.
2025-05-30 06:18:12 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B13: Scheduler step after optimizer step.
2025-05-30 06:18:13 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B15: Scheduler step after optimizer step.
2025-05-30 06:18:14 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B17: Scheduler step after optimizer step.
2025-05-30 06:18:15 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B19: Scheduler step after optimizer step.
2025-05-30 06:18:16 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B21: Scheduler step after optimizer step.
2025-05-30 06:18:17 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B23: Scheduler step after optimizer step.
2025-05-30 06:18:18 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B25: Scheduler step after optimizer step.
2025-05-30 06:18:19 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B27: Scheduler step after optimizer step.
2025-05-30 06:18:20 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B29: Scheduler step after optimizer step.
2025-05-30 06:18:21 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B31: Scheduler step after optimizer step.
2025-05-30 06:18:22 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B33: Scheduler step after optimizer step.
2025-05-30 06:18:23 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B35: Scheduler step after optimizer step.
2025-05-30 06:18:24 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B37: Scheduler step after optimizer step.
2025-05-30 06:18:25 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B39: Scheduler step after optimizer step.
2025-05-30 06:18:25 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B41: Scheduler step after optimizer step.
2025-05-30 06:18:26 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B43: Scheduler step after optimizer step.
2025-05-30 06:18:27 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B45: Scheduler step after optimizer step.
2025-05-30 06:18:28 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B47: Scheduler step after optimizer step.
2025-05-30 06:18:29 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B49: Scheduler step after optimizer step.
2025-05-30 06:18:30 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B51: Scheduler step after optimizer step.
2025-05-30 06:18:31 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B53: Scheduler step after optimizer step.
2025-05-30 06:18:32 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B55: Scheduler step after optimizer step.
2025-05-30 06:18:33 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B57: Scheduler step after optimizer step.
2025-05-30 06:18:34 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B59: Scheduler step after optimizer step.
2025-05-30 06:18:35 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B61: Scheduler step after optimizer step.
2025-05-30 06:18:36 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B63: Scheduler step after optimizer step.
2025-05-30 06:18:37 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B65: Scheduler step after optimizer step.
2025-05-30 06:18:38 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B67: Scheduler step after optimizer step.
2025-05-30 06:18:39 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B69: Scheduler step after optimizer step.
2025-05-30 06:18:40 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B71: Scheduler step after optimizer step.
2025-05-30 06:18:41 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B73: Scheduler step after optimizer step.
2025-05-30 06:18:42 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B75: Scheduler step after optimizer step.
2025-05-30 06:18:43 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B77: Scheduler step after optimizer step.
2025-05-30 06:18:44 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B79: Scheduler step after optimizer step.
2025-05-30 06:18:45 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B81: Scheduler step after optimizer step.
2025-05-30 06:18:45 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B83: Scheduler step after optimizer step.
2025-05-30 06:18:46 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B85: Scheduler step after optimizer step.
2025-05-30 06:18:47 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B87: Scheduler step after optimizer step.
2025-05-30 06:18:48 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B89: Scheduler step after optimizer step.
2025-05-30 06:18:49 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B91: Scheduler step after optimizer step.
2025-05-30 06:18:50 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B93: Scheduler step after optimizer step.
2025-05-30 06:18:51 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B95: Scheduler step after optimizer step.
2025-05-30 06:18:52 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B97: Scheduler step after optimizer step.
2025-05-30 06:18:53 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:132] - E1 OptStep 50: Grad Norm: 7.6261
2025-05-30 06:18:53 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B99: Scheduler step after optimizer step.
2025-05-30 06:18:54 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B101: Scheduler step after optimizer step.
2025-05-30 06:18:55 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B103: Scheduler step after optimizer step.
2025-05-30 06:18:56 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E1 B105: Scheduler step after optimizer step.
2025-05-30 06:18:56 - phase4_finetuning.finetune.trainer - INFO - [train_one_epoch:163] - Epoch 1 training finished. Avg Loss: 0.9263, Final LR: 1.00e-05, Opt Steps: 53, NaN count: 0
2025-05-30 06:19:04 - phase4_finetuning.finetune.trainer - INFO - [validate_one_epoch:237] - Validation finished. Avg Loss: 0.8690, accuracy: 0.7967, f1_macro: 0.7973, f1_weighted: 0.7991, precision_macro: 0.7866, precision_weighted: 0.8269, recall_macro: 0.8300, recall_weighted: 0.7967, f1_Bacterial_Blight: 0.6422, f1_Curl_Virus: 0.7910, f1_Healthy_Leaf: 0.7965, f1_Herbicide_Growth_Damage: 0.9134, f1_Leaf_Hopper_Jassids: 0.7767, f1_Leaf_Redding: 0.8000, f1_Leaf_Variegation: 0.8615
2025-05-30 06:19:10 - phase4_finetuning.finetune.trainer - INFO - [save_model_checkpoint:252] - Model checkpoint saved to /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_resume_s7_20250530_061755/checkpoints/best_finetuned_hvt_resumed.pth with metadata (epoch=1, best_metric=0.7973328722013239)
2025-05-30 06:19:10 - __main__ - INFO - [main_execution_logic:309] - Epoch 1: New best f1_macro: 0.7973
2025-05-30 06:19:10 - __main__ - INFO - [main_execution_logic:319] - Epoch 1 completed in 67.31 seconds.
2025-05-30 06:19:10 - __main__ - INFO - [main_execution_logic:287] - Epoch 2: head LR set to 5.00e-05
2025-05-30 06:19:10 - __main__ - INFO - [main_execution_logic:287] - Epoch 2: backbone LR set to 0.00e+00
2025-05-30 06:19:11 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B1: Scheduler step after optimizer step.
2025-05-30 06:19:12 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B3: Scheduler step after optimizer step.
2025-05-30 06:19:13 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B5: Scheduler step after optimizer step.
2025-05-30 06:19:15 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B7: Scheduler step after optimizer step.
2025-05-30 06:19:16 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B9: Scheduler step after optimizer step.
2025-05-30 06:19:17 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B11: Scheduler step after optimizer step.
2025-05-30 06:19:18 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B13: Scheduler step after optimizer step.
2025-05-30 06:19:19 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B15: Scheduler step after optimizer step.
2025-05-30 06:19:20 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B17: Scheduler step after optimizer step.
2025-05-30 06:19:21 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B19: Scheduler step after optimizer step.
2025-05-30 06:19:22 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B21: Scheduler step after optimizer step.
2025-05-30 06:19:23 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B23: Scheduler step after optimizer step.
2025-05-30 06:19:24 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B25: Scheduler step after optimizer step.
2025-05-30 06:19:25 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B27: Scheduler step after optimizer step.
2025-05-30 06:19:26 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B29: Scheduler step after optimizer step.
2025-05-30 06:19:27 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B31: Scheduler step after optimizer step.
2025-05-30 06:19:27 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B33: Scheduler step after optimizer step.
2025-05-30 06:19:28 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B35: Scheduler step after optimizer step.
2025-05-30 06:19:29 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B37: Scheduler step after optimizer step.
2025-05-30 06:19:30 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B39: Scheduler step after optimizer step.
2025-05-30 06:19:31 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B41: Scheduler step after optimizer step.
2025-05-30 06:19:32 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B43: Scheduler step after optimizer step.
2025-05-30 06:19:33 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B45: Scheduler step after optimizer step.
2025-05-30 06:19:34 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B47: Scheduler step after optimizer step.
2025-05-30 06:19:35 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B49: Scheduler step after optimizer step.
2025-05-30 06:19:36 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B51: Scheduler step after optimizer step.
2025-05-30 06:19:37 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B53: Scheduler step after optimizer step.
2025-05-30 06:19:38 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B55: Scheduler step after optimizer step.
2025-05-30 06:19:39 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B57: Scheduler step after optimizer step.
2025-05-30 06:19:40 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B59: Scheduler step after optimizer step.
2025-05-30 06:19:41 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B61: Scheduler step after optimizer step.
2025-05-30 06:19:42 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B63: Scheduler step after optimizer step.
2025-05-30 06:19:43 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B65: Scheduler step after optimizer step.
2025-05-30 06:19:44 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B67: Scheduler step after optimizer step.
2025-05-30 06:19:45 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B69: Scheduler step after optimizer step.
2025-05-30 06:19:46 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B71: Scheduler step after optimizer step.
2025-05-30 06:19:47 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B73: Scheduler step after optimizer step.
2025-05-30 06:19:48 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B75: Scheduler step after optimizer step.
2025-05-30 06:19:49 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B77: Scheduler step after optimizer step.
2025-05-30 06:19:50 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B79: Scheduler step after optimizer step.
2025-05-30 06:19:51 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B81: Scheduler step after optimizer step.
2025-05-30 06:19:52 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B83: Scheduler step after optimizer step.
2025-05-30 06:19:53 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B85: Scheduler step after optimizer step.
2025-05-30 06:19:54 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B87: Scheduler step after optimizer step.
2025-05-30 06:19:55 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B89: Scheduler step after optimizer step.
2025-05-30 06:19:56 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B91: Scheduler step after optimizer step.
2025-05-30 06:19:57 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B93: Scheduler step after optimizer step.
2025-05-30 06:19:58 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B95: Scheduler step after optimizer step.
2025-05-30 06:19:58 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B97: Scheduler step after optimizer step.
2025-05-30 06:19:59 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:132] - E2 OptStep 50: Grad Norm: 5.2324
2025-05-30 06:19:59 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B99: Scheduler step after optimizer step.
2025-05-30 06:20:00 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B101: Scheduler step after optimizer step.
2025-05-30 06:20:01 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B103: Scheduler step after optimizer step.
2025-05-30 06:20:02 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E2 B105: Scheduler step after optimizer step.
2025-05-30 06:20:02 - phase4_finetuning.finetune.trainer - INFO - [train_one_epoch:163] - Epoch 2 training finished. Avg Loss: 0.9052, Final LR: 2.00e-05, Opt Steps: 53, NaN count: 0
2025-05-30 06:20:11 - phase4_finetuning.finetune.trainer - INFO - [validate_one_epoch:237] - Validation finished. Avg Loss: 0.8576, accuracy: 0.7944, f1_macro: 0.7941, f1_weighted: 0.7956, precision_macro: 0.7806, precision_weighted: 0.8197, recall_macro: 0.8274, recall_weighted: 0.7944, f1_Bacterial_Blight: 0.6538, f1_Curl_Virus: 0.7852, f1_Healthy_Leaf: 0.8182, f1_Herbicide_Growth_Damage: 0.8976, f1_Leaf_Hopper_Jassids: 0.7593, f1_Leaf_Redding: 0.7961, f1_Leaf_Variegation: 0.8485
2025-05-30 06:20:11 - __main__ - INFO - [main_execution_logic:313] - Epoch 2: f1_macro (0.7941) not improved. Patience: 1/30
2025-05-30 06:20:11 - __main__ - INFO - [main_execution_logic:319] - Epoch 2 completed in 60.86 seconds.
2025-05-30 06:20:11 - __main__ - INFO - [main_execution_logic:287] - Epoch 3: head LR set to 5.00e-05
2025-05-30 06:20:11 - __main__ - INFO - [main_execution_logic:287] - Epoch 3: backbone LR set to 0.00e+00
2025-05-30 06:20:12 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B1: Scheduler step after optimizer step.
2025-05-30 06:20:13 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B3: Scheduler step after optimizer step.
2025-05-30 06:20:14 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B5: Scheduler step after optimizer step.
2025-05-30 06:20:15 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B7: Scheduler step after optimizer step.
2025-05-30 06:20:16 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B9: Scheduler step after optimizer step.
2025-05-30 06:20:17 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B11: Scheduler step after optimizer step.
2025-05-30 06:20:18 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B13: Scheduler step after optimizer step.
2025-05-30 06:20:19 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B15: Scheduler step after optimizer step.
2025-05-30 06:20:20 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B17: Scheduler step after optimizer step.
2025-05-30 06:20:21 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B19: Scheduler step after optimizer step.
2025-05-30 06:20:22 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B21: Scheduler step after optimizer step.
2025-05-30 06:20:23 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B23: Scheduler step after optimizer step.
2025-05-30 06:20:24 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B25: Scheduler step after optimizer step.
2025-05-30 06:20:25 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B27: Scheduler step after optimizer step.
2025-05-30 06:20:26 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B29: Scheduler step after optimizer step.
2025-05-30 06:20:27 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B31: Scheduler step after optimizer step.
2025-05-30 06:20:27 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B33: Scheduler step after optimizer step.
2025-05-30 06:20:28 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B35: Scheduler step after optimizer step.
2025-05-30 06:20:29 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B37: Scheduler step after optimizer step.
2025-05-30 06:20:30 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B39: Scheduler step after optimizer step.
2025-05-30 06:20:31 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B41: Scheduler step after optimizer step.
2025-05-30 06:20:32 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B43: Scheduler step after optimizer step.
2025-05-30 06:20:33 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B45: Scheduler step after optimizer step.
2025-05-30 06:20:34 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B47: Scheduler step after optimizer step.
2025-05-30 06:20:35 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B49: Scheduler step after optimizer step.
2025-05-30 06:20:36 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B51: Scheduler step after optimizer step.
2025-05-30 06:20:37 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B53: Scheduler step after optimizer step.
2025-05-30 06:20:38 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B55: Scheduler step after optimizer step.
2025-05-30 06:20:39 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B57: Scheduler step after optimizer step.
2025-05-30 06:20:40 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B59: Scheduler step after optimizer step.
2025-05-30 06:20:41 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B61: Scheduler step after optimizer step.
2025-05-30 06:20:42 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B63: Scheduler step after optimizer step.
2025-05-30 06:20:43 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B65: Scheduler step after optimizer step.
2025-05-30 06:20:44 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B67: Scheduler step after optimizer step.
2025-05-30 06:20:45 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B69: Scheduler step after optimizer step.
2025-05-30 06:20:46 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B71: Scheduler step after optimizer step.
2025-05-30 06:20:47 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B73: Scheduler step after optimizer step.
2025-05-30 06:20:48 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B75: Scheduler step after optimizer step.
2025-05-30 06:20:48 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B77: Scheduler step after optimizer step.
2025-05-30 06:20:49 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B79: Scheduler step after optimizer step.
2025-05-30 06:20:50 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B81: Scheduler step after optimizer step.
2025-05-30 06:20:51 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B83: Scheduler step after optimizer step.
2025-05-30 06:20:52 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B85: Scheduler step after optimizer step.
2025-05-30 06:20:53 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B87: Scheduler step after optimizer step.
2025-05-30 06:20:54 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B89: Scheduler step after optimizer step.
2025-05-30 06:20:55 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B91: Scheduler step after optimizer step.
2025-05-30 06:20:56 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B93: Scheduler step after optimizer step.
2025-05-30 06:20:57 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B95: Scheduler step after optimizer step.
2025-05-30 06:20:58 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B97: Scheduler step after optimizer step.
2025-05-30 06:20:59 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:132] - E3 OptStep 50: Grad Norm: 6.7367
2025-05-30 06:20:59 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B99: Scheduler step after optimizer step.
2025-05-30 06:21:00 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B101: Scheduler step after optimizer step.
2025-05-30 06:21:01 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B103: Scheduler step after optimizer step.
2025-05-30 06:21:02 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E3 B105: Scheduler step after optimizer step.
2025-05-30 06:21:02 - phase4_finetuning.finetune.trainer - INFO - [train_one_epoch:163] - Epoch 3 training finished. Avg Loss: 0.9116, Final LR: 3.00e-05, Opt Steps: 53, NaN count: 0
2025-05-30 06:21:10 - phase4_finetuning.finetune.trainer - INFO - [validate_one_epoch:237] - Validation finished. Avg Loss: 0.9014, accuracy: 0.7453, f1_macro: 0.7529, f1_weighted: 0.7426, precision_macro: 0.7489, precision_weighted: 0.7938, recall_macro: 0.8001, recall_weighted: 0.7453, f1_Bacterial_Blight: 0.6415, f1_Curl_Virus: 0.6825, f1_Healthy_Leaf: 0.7603, f1_Herbicide_Growth_Damage: 0.8976, f1_Leaf_Hopper_Jassids: 0.7107, f1_Leaf_Redding: 0.7158, f1_Leaf_Variegation: 0.8615
2025-05-30 06:21:10 - __main__ - INFO - [main_execution_logic:313] - Epoch 3: f1_macro (0.7529) not improved. Patience: 2/30
2025-05-30 06:21:10 - __main__ - INFO - [main_execution_logic:319] - Epoch 3 completed in 59.55 seconds.
2025-05-30 06:21:10 - __main__ - INFO - [main_execution_logic:287] - Epoch 4: head LR set to 5.00e-05
2025-05-30 06:21:10 - __main__ - INFO - [main_execution_logic:287] - Epoch 4: backbone LR set to 0.00e+00
2025-05-30 06:21:12 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B1: Scheduler step after optimizer step.
2025-05-30 06:21:13 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B3: Scheduler step after optimizer step.
2025-05-30 06:21:14 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B5: Scheduler step after optimizer step.
2025-05-30 06:21:15 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B7: Scheduler step after optimizer step.
2025-05-30 06:21:16 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B9: Scheduler step after optimizer step.
2025-05-30 06:21:17 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B11: Scheduler step after optimizer step.
2025-05-30 06:21:18 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B13: Scheduler step after optimizer step.
2025-05-30 06:21:19 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B15: Scheduler step after optimizer step.
2025-05-30 06:21:20 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B17: Scheduler step after optimizer step.
2025-05-30 06:21:21 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B19: Scheduler step after optimizer step.
2025-05-30 06:21:22 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B21: Scheduler step after optimizer step.
2025-05-30 06:21:23 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B23: Scheduler step after optimizer step.
2025-05-30 06:21:24 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B25: Scheduler step after optimizer step.
2025-05-30 06:21:25 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B27: Scheduler step after optimizer step.
2025-05-30 06:21:26 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B29: Scheduler step after optimizer step.
2025-05-30 06:21:27 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B31: Scheduler step after optimizer step.
2025-05-30 06:21:28 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B33: Scheduler step after optimizer step.
2025-05-30 06:21:29 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B35: Scheduler step after optimizer step.
2025-05-30 06:21:30 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B37: Scheduler step after optimizer step.
2025-05-30 06:21:31 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B39: Scheduler step after optimizer step.
2025-05-30 06:21:32 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B41: Scheduler step after optimizer step.
2025-05-30 06:21:33 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B43: Scheduler step after optimizer step.
2025-05-30 06:21:34 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B45: Scheduler step after optimizer step.
2025-05-30 06:21:35 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B47: Scheduler step after optimizer step.
2025-05-30 06:21:36 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B49: Scheduler step after optimizer step.
2025-05-30 06:21:37 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B51: Scheduler step after optimizer step.
2025-05-30 06:21:38 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B53: Scheduler step after optimizer step.
2025-05-30 06:21:39 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B55: Scheduler step after optimizer step.
2025-05-30 06:21:40 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B57: Scheduler step after optimizer step.
2025-05-30 06:21:41 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B59: Scheduler step after optimizer step.
2025-05-30 06:21:42 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B61: Scheduler step after optimizer step.
2025-05-30 06:21:43 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B63: Scheduler step after optimizer step.
2025-05-30 06:21:44 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B65: Scheduler step after optimizer step.
2025-05-30 06:21:45 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B67: Scheduler step after optimizer step.
2025-05-30 06:21:46 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B69: Scheduler step after optimizer step.
2025-05-30 06:21:47 - phase4_finetuning.finetune.trainer - DEBUG - [train_one_epoch:143] - E4 B71: Scheduler step after optimizer step.
2025-05-30 06:21:53 - phase4_finetuning.finetune.trainer - INFO - [save_model_checkpoint:252] - Model checkpoint saved to /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_resume_s7_20250530_061755/checkpoints/final_finetuned_hvt_resumed.pth with metadata (epoch=4, best_metric=0.7973328722013239)
2025-05-30 06:21:53 - __main__ - INFO - [main_execution_logic:328] - Training completed. Final checkpoint saved: /teamspace/studios/this_studio/cvpr25/phase4_finetuning/logs_finetune/ft_resume_s7_20250530_061755/checkpoints/final_finetuned_hvt_resumed.pth
2025-05-30 06:21:53 - __main__ - INFO - [main_execution_logic:329] - Best f1_macro: 0.7973
