# ğŸ—ï¸ Model Architecture

This document provides a detailed technical description of the HierarchicalViT (HViT) architecture.

## Table of Contents

- [Overview](#overview)
- [Patch Embedding](#patch-embedding)
- [Hierarchical Stages](#hierarchical-stages)
- [Transformer Block](#transformer-block)
- [Patch Merging](#patch-merging)
- [Disease-Focused Cross-Attention (DFCA)](#disease-focused-cross-attention-dfca)
- [Classification Head](#classification-head)
- [Model Variants](#model-variants)

---

## Overview

HierarchicalViT is a hierarchical vision transformer that processes images through multiple stages with progressive spatial downsampling. The architecture is inspired by Swin Transformer but optimized for plant disease classification.

### Key Design Principles

1. **Hierarchical Feature Learning**: Multi-scale representations from fine to coarse
2. **Efficient Computation**: Linear complexity with respect to image size within each stage
3. **Strong Inductive Biases**: Locality preserved through windowed attention patterns
4. **Flexible Fusion**: Optional multi-modal fusion via DFCA module

### Architecture Diagram

```
Input Image (B, 3, H, W)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Patch Embedding (16Ã—16)                â”‚
â”‚   Conv2D(3 â†’ 96, kernel=16, stride=16) + Reshape   â”‚
â”‚              Output: (B, H/16 Ã— W/16, 96)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ + Positional Embedding (learnable)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Stage 1                           â”‚
â”‚   depth=2, dim=96, heads=3, mlp_ratio=4.0          â”‚
â”‚   Resolution: H/16 Ã— W/16                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ Patch Merging (spatial 2Ã— downsample)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Stage 2                           â”‚
â”‚   depth=2, dim=192, heads=6, mlp_ratio=4.0         â”‚
â”‚   Resolution: H/32 Ã— W/32                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ Patch Merging
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Stage 3                           â”‚
â”‚   depth=6, dim=384, heads=12, mlp_ratio=4.0        â”‚
â”‚   Resolution: H/64 Ã— W/64                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ Patch Merging
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Stage 4                           â”‚
â”‚   depth=2, dim=768, heads=24, mlp_ratio=4.0        â”‚
â”‚   Resolution: H/128 Ã— W/128                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ Global Average Pooling
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LayerNorm + Linear(768, num_classes)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   Output Logits (B, num_classes)
```

---

## Patch Embedding

The patch embedding layer converts input images into a sequence of tokens.

### Implementation

```python
class PatchEmbed(nn.Module):
    def __init__(
        self,
        img_size: Tuple[int, int] = (256, 256),
        patch_size: int = 16,
        in_chans: int = 3,
        embed_dim: int = 96
    ):
        # Convolutional projection
        self.proj = nn.Conv2d(
            in_chans, embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
```

### Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `patch_size` | 16 | Size of each patch (16Ã—16 pixels) |
| `in_chans` | 3 | Input channels (RGB) |
| `embed_dim` | 96 | Output embedding dimension |

### Computation

For a 256Ã—256 input image:
- Number of patches: `(256/16) Ã— (256/16) = 16 Ã— 16 = 256`
- Output shape: `(B, 256, 96)`

---

## Hierarchical Stages

Each stage consists of multiple transformer blocks operating at the same spatial resolution.

### Stage Configuration (HViT-Small)

| Stage | Depth | Channels | Heads | Head Dim | MLP Hidden |
|:-----:|:-----:|:--------:|:-----:|:--------:|:----------:|
| 1 | 2 | 96 | 3 | 32 | 384 |
| 2 | 2 | 192 | 6 | 32 | 768 |
| 3 | 6 | 384 | 12 | 32 | 1536 |
| 4 | 2 | 768 | 24 | 32 | 3072 |

### Resolution Progression

| Stage | Input Resolution | After Patch Merge |
|:-----:|:----------------:|:-----------------:|
| 1 | 16 Ã— 16 | 8 Ã— 8 |
| 2 | 8 Ã— 8 | 4 Ã— 4 |
| 3 | 4 Ã— 4 | 2 Ã— 2 |
| 4 | 2 Ã— 2 | - (final) |

---

## Transformer Block

Each transformer block follows the pre-norm design with Drop Path regularization.

### Block Structure

```
Input
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                              â”‚
  â–¼                              â”‚
LayerNorm                        â”‚
  â”‚                              â”‚
  â–¼                              â”‚
Multi-Head Self-Attention        â”‚
  â”‚                              â”‚
  â–¼                              â”‚
DropPath (stochastic depth)      â”‚
  â”‚                              â”‚
  â–¼                              â”‚
Add â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                              â”‚
  â–¼                              â”‚
LayerNorm                        â”‚
  â”‚                              â”‚
  â–¼                              â”‚
MLP (Linear â†’ GELU â†’ Linear)     â”‚
  â”‚                              â”‚
  â–¼                              â”‚
DropPath                         â”‚
  â”‚                              â”‚
  â–¼                              â”‚
Add â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
Output
```

### Multi-Head Self-Attention

```python
class Attention(nn.Module):
    def __init__(self, dim, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
```

### MLP Block

```python
class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, 
                 act_layer=nn.GELU, drop=0.):
        hidden_features = hidden_features or in_features * 4
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features or in_features)
        self.drop = nn.Dropout(drop)
```

---

## Patch Merging

Patch merging reduces spatial resolution by 2Ã— while doubling the channel dimension.

### Algorithm

1. **Reshape**: Arrange patches into 2Ã—2 groups
2. **Concatenate**: Stack 4 patches along channel dimension (C â†’ 4C)
3. **Project**: Linear layer reduces channels (4C â†’ 2C)
4. **Normalize**: LayerNorm on output

### Implementation

```python
class PatchMerging(nn.Module):
    def forward(self, x):
        B, H, W, C = x.shape
        
        # Pad if needed for even dimensions
        x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))
        
        # Split into 2x2 patches
        x0 = x[:, 0::2, 0::2, :]  # Top-left
        x1 = x[:, 1::2, 0::2, :]  # Bottom-left
        x2 = x[:, 0::2, 1::2, :]  # Top-right
        x3 = x[:, 1::2, 1::2, :]  # Bottom-right
        
        # Concatenate and project
        x = torch.cat([x0, x1, x2, x3], dim=-1)  # (B, H/2, W/2, 4C)
        x = self.norm(x)
        x = self.reduction(x)  # (B, H/2, W/2, 2C)
        
        return x
```

---

## Disease-Focused Cross-Attention (DFCA)

The DFCA module enables fusion between RGB and spectral features (optional).

### Architecture

```
RGB Features (B, N, C)      Spectral Features (B, N, C)
        â”‚                           â”‚
        â–¼                           â–¼
   Query (Wq)              Key (Wk), Value (Wv)
        â”‚                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        Cross-Attention
        Attention(Q_rgb, K_spec, V_spec)
                  â”‚
                  â–¼
            Projection
                  â”‚
                  â–¼
        LayerNorm + Residual
                  â”‚
                  â–¼
         Fused Features (B, N, C)
```

### Implementation

```python
class DiseaseFocusedCrossAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.norm = nn.LayerNorm(embed_dim)
```

### Usage

Enable DFCA in configuration:

```yaml
model:
  use_dfca: true
  spectral_channels: 4  # Additional spectral bands
```

---

## Classification Head

The classification head converts pooled features to class predictions.

### Architecture

```
Stage 4 Output (B, 4, 768)
         â”‚
         â–¼
Global Average Pooling
   (B, 768)
         â”‚
         â–¼
LayerNorm(768)
         â”‚
         â–¼
Linear(768, num_classes)
         â”‚
         â–¼
Logits (B, num_classes)
```

---

## Model Variants

### HViT-Tiny

| Parameter | Value |
|-----------|-------|
| Embed Dim | 64 |
| Depths | [2, 2, 4, 2] |
| Heads | [2, 4, 8, 16] |
| Parameters | ~12M |

### HViT-Small (Default)

| Parameter | Value |
|-----------|-------|
| Embed Dim | 96 |
| Depths | [2, 2, 6, 2] |
| Heads | [3, 6, 12, 24] |
| Parameters | ~28M |

### HViT-Base

| Parameter | Value |
|-----------|-------|
| Embed Dim | 128 |
| Depths | [2, 2, 18, 2] |
| Heads | [4, 8, 16, 32] |
| Parameters | ~88M |

---

## Creating Custom Models

```python
from hvit.models import create_disease_aware_hvt

# Custom configuration
model = create_disease_aware_hvt(
    current_img_size=(224, 224),
    num_classes=10,
    model_params_dict={
        "patch_size": 16,
        "embed_dim_rgb": 64,
        "depths": [2, 2, 4, 2],
        "num_heads": [2, 4, 8, 16],
        "mlp_ratio": 4.0,
        "drop_path_rate": 0.1,
        "use_dfca": False,
        "use_gradient_checkpointing": True,
    }
)
```

---

## References

- [Swin Transformer](https://arxiv.org/abs/2103.14030) - Liu et al., 2021
- [Vision Transformer](https://arxiv.org/abs/2010.11929) - Dosovitskiy et al., 2020
- [DeiT](https://arxiv.org/abs/2012.12877) - Touvron et al., 2021

For implementation details, see [`hvit/models/hvt.py`](../hvit/models/hvt.py).
