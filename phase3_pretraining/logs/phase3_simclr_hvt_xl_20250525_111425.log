2025-05-25 11:14:25 - root - INFO - [setup_logging:57] - Logging configured. File: /teamspace/studios/this_studio/cvpr25/phase3_pretraining/logs/phase3_simclr_hvt_xl_20250525_111425.log (Level: DEBUG), Console Level: INFO
2025-05-25 11:14:26 - phase2_model.models - INFO - [<module>:17] - Models (InceptionV3Baseline, DFCA, DiseaseAwareHVT, factory) imported successfully into phase2_model.models package.
2025-05-25 11:14:26 - phase2_model - INFO - [<module>:25] - Models re-exported successfully by phase2_model/__init__.py.
2025-05-25 11:14:26 - __main__ - INFO - [main_pretrain_script:67] - ======== Starting Phase 3: HVT Self-Supervised Pre-training ========
2025-05-25 11:14:26 - __main__ - INFO - [main_pretrain_script:68] - Full run configuration: {'seed': 42, 'device': 'cuda', 'PROJECT_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25', 'PACKAGE_ROOT_PATH': '/teamspace/studios/this_studio/cvpr25/phase3_pretraining', 'log_dir_name': 'logs', 'log_file_pretrain': 'phase3_simclr_hvt_xl.log', 'checkpoint_dir_name': 'pretrain_checkpoints_hvt_xl', 'enable_torch_compile': False, 'torch_compile_mode': 'reduce-overhead', 'matmul_precision': 'high', 'cudnn_benchmark': True, 'data_root': '/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection', 'original_dataset_name': 'Original Dataset', 'augmented_dataset_name': 'Augmented Dataset', 'train_split_ratio': 0.95, 'num_classes': 7, 'num_workers': 4, 'prefetch_factor': 2, 'hvt_params_for_backbone': {'patch_size': 14, 'embed_dim_rgb': 192, 'embed_dim_spectral': 192, 'spectral_channels': 0, 'depths': [3, 6, 24, 3], 'num_heads': [6, 12, 24, 48], 'mlp_ratio': 4.0, 'qkv_bias': True, 'model_drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'norm_layer_name': 'LayerNorm', 'use_dfca': False, 'dfca_embed_dim_match_rgb': True, 'dfca_num_heads': 32, 'dfca_drop_rate': 0.1, 'dfca_use_disease_mask': True, 'use_gradient_checkpointing': True, 'ssl_enable_mae': False, 'ssl_enable_contrastive': False, 'enable_consistency_loss_heads': False, 'ssl_mae_mask_ratio': 0.75, 'ssl_mae_decoder_dim': 64, 'ssl_mae_norm_pix_loss': True, 'ssl_contrastive_projector_dim': 128, 'ssl_contrastive_projector_depth': 2}, 'pretrain_img_size': (448, 448), 'pretrain_epochs': 80, 'pretrain_batch_size': 32, 'accumulation_steps': 2, 'pretrain_lr': 0.0005, 'pretrain_optimizer': 'AdamW', 'pretrain_scheduler': 'WarmupCosine', 'warmup_epochs': 10, 'eta_min_lr': 1e-06, 'pretrain_weight_decay': 0.05, 'pretrain_momentum': 0.9, 'temperature': 0.1, 'projection_dim': 256, 'projection_hidden_dim': 4096, 'simclr_s': 1.0, 'simclr_p_grayscale': 0.2, 'simclr_p_gaussian_blur': 0.5, 'simclr_rrc_scale_min': 0.08, 'evaluate_every_n_epochs': 10, 'linear_probe_epochs': 20, 'linear_probe_lr': 0.1, 'probe_optimizer': 'SGD', 'probe_momentum': 0.9, 'probe_weight_decay': 0.0, 'probe_batch_size': 64, 'save_every_n_epochs': 20, 'model_arch_name_for_ckpt': 'hvt_xl_simclr', 'clip_grad_norm': 1.0}
2025-05-25 11:14:26 - __main__ - INFO - [apply_pytorch_optimizations:51] - torch.backends.cudnn.benchmark = True
2025-05-25 11:14:26 - __main__ - INFO - [apply_pytorch_optimizations:57] - torch.set_float32_matmul_precision('high')
2025-05-25 11:14:26 - __main__ - INFO - [main_pretrain_script:76] - Global random seed set to: 42
2025-05-25 11:14:26 - __main__ - INFO - [main_pretrain_script:77] - Using device: cuda
2025-05-25 11:14:26 - __main__ - INFO - [main_pretrain_script:79] - CUDA Device: Tesla T4, PyTorch CUDA Version: 12.1
2025-05-25 11:14:26 - __main__ - INFO - [main_pretrain_script:83] - Target image size for pre-training: (448, 448)
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-25 11:14:26 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-25 11:14:26 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='train', img_size=(448, 448), use_spectral=False
2025-05-25 11:14:26 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-25 11:14:26 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'train' size: 8680 samples.
2025-05-25 11:14:26 - phase3_pretraining.dataset - INFO - [__init__:52] - Initializing SARCLD2024Dataset: split='val', img_size=(448, 448), use_spectral=False
2025-05-25 11:14:26 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Original Dataset
2025-05-25 11:14:26 - phase3_pretraining.dataset - DEBUG - [__init__:62] - Scanning sub-dataset: /teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection/Augmented Dataset
2025-05-25 11:14:27 - phase3_pretraining.dataset - INFO - [__init__:77] - Found 9137 total image entries.
2025-05-25 11:14:27 - phase3_pretraining.dataset - INFO - [__init__:90] - Dataset split 'val' size: 457 samples.
2025-05-25 11:14:27 - __main__ - INFO - [main_pretrain_script:105] - Pretrain dataset size: 8680
2025-05-25 11:14:27 - __main__ - INFO - [main_pretrain_script:106] - Probe train dataset: 8680, Probe val dataset: 457
2025-05-25 11:14:27 - __main__ - INFO - [main_pretrain_script:118] - Pretrain DataLoader: 271 batches of size 32. Num workers: 4
2025-05-25 11:14:27 - __main__ - INFO - [main_pretrain_script:121] - Initializing HVTForPretraining model wrapper...
2025-05-25 11:14:27 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:36] - Initializing HVTForPretraining wrapper for img_size: (448, 448)
2025-05-25 11:14:27 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:44] - Instantiating HVTBackbone using parameters defined in Phase 3 config (hvt_params_for_backbone).
2025-05-25 11:14:27 - phase2_model.models.hvt - INFO - [create_disease_aware_hvt:602] - Factory: Creating DiseaseAwareHVT for img_size: (448, 448), num_classes: 7
2025-05-25 11:14:29 - phase2_model.models.hvt - INFO - [__init__:325] - HVT: Running RGB stream only. No fusion.
2025-05-25 11:14:32 - phase2_model.models.hvt - INFO - [__init__:359] - DiseaseAwareHVT initialized for image size (448, 448) and 7 classes.
2025-05-25 11:14:32 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:70] - Projection head input dimension set to: 1536
2025-05-25 11:14:32 - phase3_pretraining.models.projection_head - INFO - [__init__:29] - ProjectionHead initialized: In=1536, Hidden=4096, Out=256, BatchNorm=True
2025-05-25 11:14:32 - phase3_pretraining.models.hvt_wrapper - INFO - [__init__:79] - HVTForPretraining wrapper initialized successfully.
2025-05-25 11:14:33 - phase3_pretraining.utils.augmentations - INFO - [__init__:27] - SimCLRAugmentation: img_size=(448, 448), s=1.0, p_gray=0.2, p_blur=0.5, rrc_min_scale=0.08
2025-05-25 11:14:33 - phase3_pretraining.utils.losses - INFO - [__init__:13] - InfoNCELoss initialized with temperature: 0.1
2025-05-25 11:14:33 - __main__ - INFO - [main_pretrain_script:137] - Initializing Pretrainer instance...
2025-05-25 11:14:33 - phase3_pretraining.pretrain.trainer - INFO - [__init__:35] - Initializing Pretrainer...
2025-05-25 11:14:33 - phase3_pretraining.pretrain.trainer - INFO - [__init__:52] - Optimizer: AdamW, LR: 0.0005, Weight Decay: 0.05
2025-05-25 11:14:33 - phase3_pretraining.pretrain.trainer - INFO - [__init__:65] - Pretrainer initialized. AMP enabled: True, Accum steps: 2, Clip Grad: 1.0
2025-05-25 11:14:33 - __main__ - INFO - [main_pretrain_script:145] - Starting SimCLR pre-training for 80 epochs.
2025-05-25 11:14:33 - phase3_pretraining.pretrain.trainer - INFO - [_initialize_scheduler_if_needed:83] - Scheduler: WarmupCosine (WarmupSteps=2710, TotalTrainSteps=21680).
2025-05-25 11:21:48 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E1: Applying remaining gradients from accumulation.
2025-05-25 11:21:48 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 1 Training Summary: AvgLoss=3.4947, OptSteps=136, FinalLR=2.51e-05
2025-05-25 11:21:48 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 1/80 | Duration: 435.29s | Samples/sec: 19.92 | Avg Loss: 3.4947 | LR: 2.51e-05
2025-05-25 11:21:48 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E1: Alloc 3244.2MB, MaxAlloc 8348.1MB
2025-05-25 11:29:09 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E2: Applying remaining gradients from accumulation.
2025-05-25 11:29:10 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 2 Training Summary: AvgLoss=3.1979, OptSteps=136, FinalLR=5.02e-05
2025-05-25 11:29:10 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 2/80 | Duration: 441.76s | Samples/sec: 19.63 | Avg Loss: 3.1979 | LR: 5.02e-05
2025-05-25 11:29:10 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E2: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 11:36:27 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E3: Applying remaining gradients from accumulation.
2025-05-25 11:36:27 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 3 Training Summary: AvgLoss=3.1180, OptSteps=136, FinalLR=7.53e-05
2025-05-25 11:36:27 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 3/80 | Duration: 437.49s | Samples/sec: 19.82 | Avg Loss: 3.1180 | LR: 7.53e-05
2025-05-25 11:36:27 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E3: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 11:43:47 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E4: Applying remaining gradients from accumulation.
2025-05-25 11:43:47 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 4 Training Summary: AvgLoss=3.1159, OptSteps=136, FinalLR=1.00e-04
2025-05-25 11:43:47 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 4/80 | Duration: 440.00s | Samples/sec: 19.71 | Avg Loss: 3.1159 | LR: 1.00e-04
2025-05-25 11:43:47 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E4: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 11:51:04 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E5: Applying remaining gradients from accumulation.
2025-05-25 11:51:04 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 5 Training Summary: AvgLoss=3.0716, OptSteps=136, FinalLR=1.25e-04
2025-05-25 11:51:04 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 5/80 | Duration: 436.72s | Samples/sec: 19.86 | Avg Loss: 3.0716 | LR: 1.25e-04
2025-05-25 11:51:04 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E5: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 11:58:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E6: Applying remaining gradients from accumulation.
2025-05-25 11:58:21 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 6 Training Summary: AvgLoss=2.9514, OptSteps=136, FinalLR=1.51e-04
2025-05-25 11:58:21 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 6/80 | Duration: 436.73s | Samples/sec: 19.86 | Avg Loss: 2.9514 | LR: 1.51e-04
2025-05-25 11:58:21 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E6: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 12:05:38 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E7: Applying remaining gradients from accumulation.
2025-05-25 12:05:39 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 7 Training Summary: AvgLoss=3.0021, OptSteps=136, FinalLR=1.76e-04
2025-05-25 12:05:39 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 7/80 | Duration: 437.90s | Samples/sec: 19.80 | Avg Loss: 3.0021 | LR: 1.76e-04
2025-05-25 12:05:39 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E7: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 12:12:58 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E8: Applying remaining gradients from accumulation.
2025-05-25 12:12:58 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 8 Training Summary: AvgLoss=2.9222, OptSteps=136, FinalLR=2.01e-04
2025-05-25 12:12:58 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 8/80 | Duration: 439.39s | Samples/sec: 19.74 | Avg Loss: 2.9222 | LR: 2.01e-04
2025-05-25 12:12:58 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E8: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 12:20:18 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E9: Applying remaining gradients from accumulation.
2025-05-25 12:20:18 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 9 Training Summary: AvgLoss=2.8518, OptSteps=136, FinalLR=2.26e-04
2025-05-25 12:20:18 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 9/80 | Duration: 439.98s | Samples/sec: 19.71 | Avg Loss: 2.8518 | LR: 2.26e-04
2025-05-25 12:20:18 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E9: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 12:27:37 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E10: Applying remaining gradients from accumulation.
2025-05-25 12:27:37 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 10 Training Summary: AvgLoss=2.7155, OptSteps=136, FinalLR=2.51e-04
2025-05-25 12:27:37 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 10/80 | Duration: 439.14s | Samples/sec: 19.75 | Avg Loss: 2.7155 | LR: 2.51e-04
2025-05-25 12:27:37 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:168] - --- Starting Linear Probe (after SSL Epoch 10) ---
2025-05-25 12:27:37 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:173] - Probe E10: Feature dim for linear classifier: 1536
2025-05-25 13:10:13 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:202] - Linear Probe (SSL E10) Validation Accuracy: 28.88% (132/457)
2025-05-25 13:10:13 - __main__ - INFO - [main_pretrain_script:166] - New best probe accuracy: 28.88% (SSL Epoch 10). Saving best model.
2025-05-25 13:10:13 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:215] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth...
2025-05-25 13:10:18 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:228] - Checkpoint saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth
2025-05-25 13:10:18 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E10: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 13:17:39 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E11: Applying remaining gradients from accumulation.
2025-05-25 13:17:39 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 11 Training Summary: AvgLoss=2.5990, OptSteps=136, FinalLR=2.76e-04
2025-05-25 13:17:39 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 11/80 | Duration: 440.62s | Samples/sec: 19.68 | Avg Loss: 2.5990 | LR: 2.76e-04
2025-05-25 13:17:39 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E11: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 13:25:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E12: Applying remaining gradients from accumulation.
2025-05-25 13:25:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 12 Training Summary: AvgLoss=2.6072, OptSteps=136, FinalLR=3.01e-04
2025-05-25 13:25:00 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 12/80 | Duration: 441.42s | Samples/sec: 19.65 | Avg Loss: 2.6072 | LR: 3.01e-04
2025-05-25 13:25:00 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E12: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 13:32:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E13: Applying remaining gradients from accumulation.
2025-05-25 13:32:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 13 Training Summary: AvgLoss=2.5220, OptSteps=136, FinalLR=3.26e-04
2025-05-25 13:32:20 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 13/80 | Duration: 439.46s | Samples/sec: 19.73 | Avg Loss: 2.5220 | LR: 3.26e-04
2025-05-25 13:32:20 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E13: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 13:39:37 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E14: Applying remaining gradients from accumulation.
2025-05-25 13:39:37 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 14 Training Summary: AvgLoss=2.3961, OptSteps=136, FinalLR=3.51e-04
2025-05-25 13:39:37 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 14/80 | Duration: 437.53s | Samples/sec: 19.82 | Avg Loss: 2.3961 | LR: 3.51e-04
2025-05-25 13:39:37 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E14: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 13:46:55 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E15: Applying remaining gradients from accumulation.
2025-05-25 13:46:56 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 15 Training Summary: AvgLoss=2.3455, OptSteps=136, FinalLR=3.76e-04
2025-05-25 13:46:56 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 15/80 | Duration: 438.03s | Samples/sec: 19.80 | Avg Loss: 2.3455 | LR: 3.76e-04
2025-05-25 13:46:56 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E15: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 13:54:16 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E16: Applying remaining gradients from accumulation.
2025-05-25 13:54:16 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 16 Training Summary: AvgLoss=2.3303, OptSteps=136, FinalLR=4.01e-04
2025-05-25 13:54:16 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 16/80 | Duration: 440.96s | Samples/sec: 19.67 | Avg Loss: 2.3303 | LR: 4.01e-04
2025-05-25 13:54:16 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E16: Alloc 3244.2MB, MaxAlloc 8350.4MB
2025-05-25 14:01:37 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E17: Applying remaining gradients from accumulation.
2025-05-25 14:01:37 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 17 Training Summary: AvgLoss=2.2777, OptSteps=136, FinalLR=4.27e-04
2025-05-25 14:01:37 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 17/80 | Duration: 440.95s | Samples/sec: 19.67 | Avg Loss: 2.2777 | LR: 4.27e-04
2025-05-25 14:01:37 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E17: Alloc 3244.2MB, MaxAlloc 8354.6MB
2025-05-25 14:09:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E18: Applying remaining gradients from accumulation.
2025-05-25 14:09:00 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 18 Training Summary: AvgLoss=2.1517, OptSteps=136, FinalLR=4.52e-04
2025-05-25 14:09:00 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 18/80 | Duration: 442.91s | Samples/sec: 19.58 | Avg Loss: 2.1517 | LR: 4.52e-04
2025-05-25 14:09:00 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E18: Alloc 3244.2MB, MaxAlloc 8354.6MB
2025-05-25 14:16:19 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E19: Applying remaining gradients from accumulation.
2025-05-25 14:16:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 19 Training Summary: AvgLoss=2.1516, OptSteps=136, FinalLR=4.77e-04
2025-05-25 14:16:20 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 19/80 | Duration: 439.21s | Samples/sec: 19.74 | Avg Loss: 2.1516 | LR: 4.77e-04
2025-05-25 14:16:20 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E19: Alloc 3244.2MB, MaxAlloc 8354.6MB
2025-05-25 14:23:41 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E20: Applying remaining gradients from accumulation.
2025-05-25 14:23:41 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 20 Training Summary: AvgLoss=2.1754, OptSteps=136, FinalLR=5.00e-04
2025-05-25 14:23:41 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 20/80 | Duration: 441.76s | Samples/sec: 19.63 | Avg Loss: 2.1754 | LR: 5.00e-04
2025-05-25 14:23:41 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:168] - --- Starting Linear Probe (after SSL Epoch 20) ---
2025-05-25 14:23:41 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:173] - Probe E20: Feature dim for linear classifier: 1536
2025-05-25 15:06:26 - phase3_pretraining.pretrain.trainer - INFO - [evaluate_linear_probe:202] - Linear Probe (SSL E20) Validation Accuracy: 40.48% (185/457)
2025-05-25 15:06:26 - __main__ - INFO - [main_pretrain_script:166] - New best probe accuracy: 40.48% (SSL Epoch 20). Saving best model.
2025-05-25 15:06:26 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:215] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth...
2025-05-25 15:06:37 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:228] - Checkpoint saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_best_probe.pth
2025-05-25 15:06:37 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:215] - Saving checkpoint to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_epoch_20.pth...
2025-05-25 15:06:42 - phase3_pretraining.pretrain.trainer - INFO - [save_checkpoint:228] - Checkpoint saved successfully to /teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_epoch_20.pth
2025-05-25 15:06:42 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E20: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:14:03 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E21: Applying remaining gradients from accumulation.
2025-05-25 15:14:04 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 21 Training Summary: AvgLoss=2.0845, OptSteps=136, FinalLR=5.00e-04
2025-05-25 15:14:04 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 21/80 | Duration: 441.35s | Samples/sec: 19.65 | Avg Loss: 2.0845 | LR: 5.00e-04
2025-05-25 15:14:04 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E21: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:21:22 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E22: Applying remaining gradients from accumulation.
2025-05-25 15:21:23 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 22 Training Summary: AvgLoss=2.0203, OptSteps=136, FinalLR=5.00e-04
2025-05-25 15:21:23 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 22/80 | Duration: 439.11s | Samples/sec: 19.75 | Avg Loss: 2.0203 | LR: 5.00e-04
2025-05-25 15:21:23 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E22: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:28:42 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E23: Applying remaining gradients from accumulation.
2025-05-25 15:28:43 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 23 Training Summary: AvgLoss=1.9939, OptSteps=136, FinalLR=4.99e-04
2025-05-25 15:28:43 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 23/80 | Duration: 440.01s | Samples/sec: 19.71 | Avg Loss: 1.9939 | LR: 4.99e-04
2025-05-25 15:28:43 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E23: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:36:02 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E24: Applying remaining gradients from accumulation.
2025-05-25 15:36:02 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 24 Training Summary: AvgLoss=1.9456, OptSteps=136, FinalLR=4.99e-04
2025-05-25 15:36:02 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 24/80 | Duration: 439.48s | Samples/sec: 19.73 | Avg Loss: 1.9456 | LR: 4.99e-04
2025-05-25 15:36:02 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E24: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:43:21 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E25: Applying remaining gradients from accumulation.
2025-05-25 15:43:22 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 25 Training Summary: AvgLoss=1.8813, OptSteps=136, FinalLR=4.98e-04
2025-05-25 15:43:22 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 25/80 | Duration: 439.41s | Samples/sec: 19.74 | Avg Loss: 1.8813 | LR: 4.98e-04
2025-05-25 15:43:22 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E25: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:50:42 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E26: Applying remaining gradients from accumulation.
2025-05-25 15:50:43 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 26 Training Summary: AvgLoss=1.8043, OptSteps=136, FinalLR=4.98e-04
2025-05-25 15:50:43 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 26/80 | Duration: 441.07s | Samples/sec: 19.66 | Avg Loss: 1.8043 | LR: 4.98e-04
2025-05-25 15:50:43 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E26: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 15:58:01 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E27: Applying remaining gradients from accumulation.
2025-05-25 15:58:02 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 27 Training Summary: AvgLoss=1.8423, OptSteps=136, FinalLR=4.97e-04
2025-05-25 15:58:02 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 27/80 | Duration: 438.91s | Samples/sec: 19.76 | Avg Loss: 1.8423 | LR: 4.97e-04
2025-05-25 15:58:02 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E27: Alloc 3244.2MB, MaxAlloc 8356.6MB
2025-05-25 16:05:20 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:141] - E28: Applying remaining gradients from accumulation.
2025-05-25 16:05:21 - phase3_pretraining.pretrain.trainer - INFO - [train_one_epoch:159] - Epoch 28 Training Summary: AvgLoss=1.7187, OptSteps=136, FinalLR=4.96e-04
2025-05-25 16:05:21 - __main__ - INFO - [main_pretrain_script:159] - SSL Epoch 28/80 | Duration: 439.14s | Samples/sec: 19.75 | Avg Loss: 1.7187 | LR: 4.96e-04
2025-05-25 16:05:21 - __main__ - DEBUG - [main_pretrain_script:175] - CUDA Mem E28: Alloc 3244.2MB, MaxAlloc 8356.6MB
