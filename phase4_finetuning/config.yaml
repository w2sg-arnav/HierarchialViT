# ===================================================================
# == Main Configuration for Phase 4: Fine-tuning (100 Epoch Run)
# ===================================================================

# --- Run Setup ---
run_name_prefix: "HVT-XL_FineTune_Run_100epochs"
seed: 42
device: "cuda"
cudnn_benchmark: true
PACKAGE_ROOT_PATH: "./phase4_finetuning" # Will be updated by main.py
PROJECT_ROOT_PATH: ".." # Will be updated by main.py

# --- Model Configuration ---
model:
  # Path to the SSL pre-trained backbone from Phase 3
  ssl_pretrained_path: "/teamspace/studios/this_studio/cvpr25/phase3_pretraining/pretrain_checkpoints_hvt_xl/hvt_xl_simclr_t4_resumed_best_probe.pth"
  # Path to resume a fine-tuning run (leave blank for new run)
  resume_finetune_path: ""
  # HVT Backbone Parameters (MUST MATCH the architecture from the SSL checkpoint)
  hvt_params:
    patch_size: 14
    embed_dim_rgb: 192
    embed_dim_spectral: 0
    spectral_channels: 0
    depths: [3, 6, 24, 3]
    num_heads: [6, 12, 24, 48]
    mlp_ratio: 4.0
    qkv_bias: true
    model_drop_rate: 0.0
    attn_drop_rate: 0.0
    drop_path_rate: 0.2
    norm_layer_name: "LayerNorm"
    use_dfca: false
    use_gradient_checkpointing: true
    ssl_enable_mae: false
    ssl_enable_contrastive: false
    enable_consistency_loss_heads: false

# --- Data Configuration ---
data:
  root_dir: "/teamspace/studios/this_studio/cvpr25/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection"
  original_dataset_name: "Original Dataset"
  augmented_dataset_name: "Augmented Dataset"
  img_size: [448, 448]
  num_classes: 7
  train_split_ratio: 0.95
  num_workers: 4
  prefetch_factor: 2
  use_weighted_sampler: true

# --- Augmentation Configuration ---
augmentations:
  enable: true
  strategy: "cotton_disease"
  severity: "moderate"
  mixup_alpha: 0.2
  cutmix_prob: 0.5

# --- Training Configuration ---
training:
  epochs: 100 # Increased for a longer run
  batch_size: 16
  accumulation_steps: 2
  amp_enabled: true
  clip_grad_norm: 1.0
  freeze_backbone_epochs: 5
  save_every_n_epochs: 10 # NEW: Save a checkpoint every 10 epochs

  # Optimizer settings
  optimizer:
    lr_backbone_unfrozen: 5.0e-05
    lr_head_unfrozen: 0.001

  # Scheduler settings (OneCycleLR)
  scheduler:
    name: "onecyclelr"
    pct_start: 0.1
    div_factor: 25.0
    final_div_factor: 10000.0

  # Loss function settings
  loss:
    type: "combined"
    use_class_weights: true
    label_smoothing: 0.1
    focal_alpha: 0.25
    focal_gamma: 2.0
    weights:
      ce: 0.5
      focal: 0.5

# --- Evaluation and Advanced Techniques ---
evaluation:
  use_ema: true
  ema_decay: 0.9999
  tta_enabled: true
  early_stopping:
    metric: "val_f1_macro"
    patience: 20 # Increased patience for a longer run
    min_delta: 0.0001

# --- Torch Compile (Optional Performance Boost) ---
torch_compile:
  enable: false
  mode: "reduce-overhead"